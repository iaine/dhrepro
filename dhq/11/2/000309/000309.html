<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><title>DHQ: Digital Humanities Quarterly: Automated Pattern Analysis in Gesture Research: Similarity
                    Measuring in 3D Motion Capture Models of Communicative Action</title><link rel="stylesheet" type="text/css" href="/dhq/common/css/dhq.css"/><link rel="stylesheet" type="text/css" media="screen" href="/dhq/common/css/dhq_screen.css"/><link rel="stylesheet" type="text/css" media="print" href="/dhq/common/css/dhq_print.css"/><link rel="alternate" type="application/atom+xml" href="/dhq/feed/news.xml"/><link rel="shortcut icon" href="/dhq/common/images/favicon.ico"/><script type="text/javascript" src="/dhq/common/js/javascriptLibrary.js">
                &lt;!-- Javascript functions --&gt;
            </script><script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-15812721-1']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type =
'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0];
s.parentNode.insertBefore(ga, s);
 })();

        </script></head><body><div id="top"><div id="backgroundpic"><script type="text/javascript" src="/dhq/common/js/pics.js"><!--displays banner image--></script></div><div id="banner"><div id="dhqlogo"><img src="/dhq/common/images/dhqlogo.png" alt="DHQ Logo"/></div><div id="longdhqlogo"><img src="/dhq/common/images/dhqlogolonger.png" alt="Digital Humanities Quarterly Logo"/></div></div><div id="topNavigation"><div id="topnavlinks"><span><a href="/dhq/" class="topnav">home</a></span><span><a href="/dhq/submissions/index.html" class="topnav">submissions</a></span><span><a href="/dhq/about/about.html" class="topnav">about dhq</a></span><span><a href="/dhq/people/people.html" class="topnav">dhq people</a></span><span id="rightmost"><a href="/dhq/contact/contact.html" class="topnav">contact</a></span></div><div id="search"><form action="/dhq/findIt" method="get" onsubmit="javascript:document.location.href=cleanSearch(this.queryString.value); return false;"><div><input type="text" name="queryString" size="18"/> <input type="submit" value="Search"/></div></form></div></div></div><div id="main"><div id="leftsidebar"><div id="leftsidenav"><span>Current Issue<br/></span><ul><li><a href="/dhq/vol/11/3/index.html">2017: 11.3</a></li></ul><span>Preview Issue<br/></span><ul><li><a href="/dhq/preview/index.html">2017: 11.4</a></li></ul><span>Previous Issues<br/></span><ul><li><a href="/dhq/vol/11/2/index.html">2017: 11.2</a></li><li><a href="/dhq/vol/11/1/index.html">2017: 11.1</a></li><li><a href="/dhq/vol/10/4/index.html">2016: 10.4</a></li><li><a href="/dhq/vol/10/3/index.html">2016: 10.3</a></li><li><a href="/dhq/vol/10/2/index.html">2016: 10.2</a></li><li><a href="/dhq/vol/10/1/index.html">2016: 10.1</a></li><li><a href="/dhq/vol/9/4/index.html">2015: 9.4</a></li><li><a href="/dhq/vol/9/3/index.html">2015: 9.3</a></li><li><a href="/dhq/vol/9/2/index.html">2015: 9.2</a></li><li><a href="/dhq/vol/9/1/index.html">2015: 9.1</a></li><li><a href="/dhq/vol/8/4/index.html">2014: 8.4</a></li><li><a href="/dhq/vol/8/3/index.html">2014: 8.3</a></li><li><a href="/dhq/vol/8/2/index.html">2014: 8.2</a></li><li><a href="/dhq/vol/8/1/index.html">2014: 8.1</a></li><li><a href="/dhq/vol/7/3/index.html">2013: 7.3</a></li><li><a href="/dhq/vol/7/2/index.html">2013: 7.2</a></li><li><a href="/dhq/vol/7/1/index.html">2013: 7.1</a></li><li><a href="/dhq/vol/6/3/index.html">2012: 6.3</a></li><li><a href="/dhq/vol/6/2/index.html">2012: 6.2</a></li><li><a href="/dhq/vol/6/1/index.html">2012: 6.1</a></li><li><a href="/dhq/vol/5/3/index.html">2011: 5.3</a></li><li><a href="/dhq/vol/5/2/index.html">2011: 5.2</a></li><li><a href="/dhq/vol/5/1/index.html">2011: 5.1</a></li><li><a href="/dhq/vol/4/2/index.html">2010: 4.2</a></li><li><a href="/dhq/vol/4/1/index.html">2010: 4.1</a></li><li><a href="/dhq/vol/3/4/index.html">2009: 3.4</a></li><li><a href="/dhq/vol/3/3/index.html">2009: 3.3</a></li><li><a href="/dhq/vol/3/2/index.html">2009: 3.2</a></li><li><a href="/dhq/vol/3/1/index.html">2009: 3.1</a></li><li><a href="/dhq/vol/2/1/index.html">2008: 2.1</a></li><li><a href="/dhq/vol/1/2/index.html">2007: 1.2</a></li><li><a href="/dhq/vol/1/1/index.html">2007: 1.1</a></li></ul><span>Indexes<br/></span><ul><li><a href="/dhq/index/title.html"> Title</a></li><li><a href="/dhq/index/author.html"> Author</a></li></ul></div><img src="/dhq/common/images/lbarrev.png" style="margin-left : 7px;" alt="sidenavbarimg"/><div id="leftsideID"><b>ISSN 1938-4122</b><br/></div><div class="leftsidecontent"><h3>Announcements</h3><ul><li><a href="/dhq/announcements/index.html#reviewers">Call for Reviewers</a></li><li><a href="/dhq/announcements/index.html#submissions">Call for Submissions</a></li></ul></div><div class="leftsidecontent"><script type="text/javascript">addthis_pub  = 'dhq';</script><a href="http://www.addthis.com/bookmark.php" onmouseover="return addthis_open(this, '', '[URL]', '[TITLE]')" onmouseout="addthis_close()" onclick="return addthis_sendto()"><img src="http://s9.addthis.com/button1-addthis.gif" width="125" height="16" alt="button1-addthis.gif"/></a><script type="text/javascript" src="http://s7.addthis.com/js/152/addthis_widget.js">&lt;!-- Javascript functions --&gt;</script></div></div><div id="mainContent"><div id="printSiteTitle">DHQ: Digital Humanities Quarterly</div><div xmlns:dhqBiblio="http://digitalhumanities.org/dhq/ns/biblio" class="DHQarticle"><div id="pubInfo">2017<br/>Volume 11 Number 2</div><div class="toolbar"><form id="taporware" action="get"><div><a href="/dhq/vol/11/2/index.html">2017 11.2</a>
                     | 
                    <a rel="external" href="/dhq/vol/11/2/000309.xml">XML</a>

| 
		   Discuss
			(<a href="/dhq/vol/11/2/000309/000309.html#disqus_thread" data-disqus-identifier="000309">
				Comments
			</a>)
                </div></form></div>
    <div class="DHQheader">
        
            
                
                <h1 class="articleTitle lang en">Automated Pattern Analysis in Gesture Research: Similarity
                    Measuring in 3D Motion Capture Models of Communicative Action</h1>
                <div class="author"><a rel="external" href="../bios.html#schüller_daniel">Daniel Schüller</a> &lt;<a href="mailto:schueller_at_humtec_dot_rwth-aachen_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('schueller_at_humtec_dot_rwth-aachen_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('schueller_at_humtec_dot_rwth-aachen_dot_de'); return false;">schueller_at_humtec_dot_rwth-aachen_dot_de</a>&gt;, Natural Media Lab, Human Technology Centre, RWTH Aachen
                        University</div>
                <div class="author"><a rel="external" href="../bios.html#beecks_christian">Christian Beecks</a> &lt;<a href="mailto:beecks_at_informatik_dot_rwth-aachen_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('beecks_at_informatik_dot_rwth-aachen_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('beecks_at_informatik_dot_rwth-aachen_dot_de'); return false;">beecks_at_informatik_dot_rwth-aachen_dot_de</a>&gt;, Data Management and Exploration Group, RWTH Aachen
                        University</div>
                <div class="author"><a rel="external" href="../bios.html#hassani_marwan">Marwan Hassani</a> &lt;<a href="mailto:m_dot_hassani_at_tue_dot_nl" onclick="javascript:window.location.href='mailto:'+deobfuscate('m_dot_hassani_at_tue_dot_nl'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('m_dot_hassani_at_tue_dot_nl'); return false;">m_dot_hassani_at_tue_dot_nl</a>&gt;, Data Management and Exploration Group, RWTH Aachen
                        University</div>
                <div class="author"><a rel="external" href="../bios.html#hinnell_jennifer">Jennifer Hinnell</a> &lt;<a href="mailto:hinnell_at_ualberta_dot_ca" onclick="javascript:window.location.href='mailto:'+deobfuscate('hinnell_at_ualberta_dot_ca'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('hinnell_at_ualberta_dot_ca'); return false;">hinnell_at_ualberta_dot_ca</a>&gt;, Department of Linguistics, University of
                        Alberta</div>
                <div class="author"><a rel="external" href="../bios.html#brenger_bela">Bela Brenger</a> &lt;<a href="mailto:brenger_at_rwth-aachen_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('brenger_at_rwth-aachen_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('brenger_at_rwth-aachen_dot_de'); return false;">brenger_at_rwth-aachen_dot_de</a>&gt;, Natural Media Lab, Human Technology Centre, RWTH Aachen
                        University</div>
                <div class="author"><a rel="external" href="../bios.html#seidl_thomas">Thomas Seidl</a> &lt;<a href="mailto:seidl_at_dbs_dot_ifi_dot_lmu_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('seidl_at_dbs_dot_ifi_dot_lmu_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('seidl_at_dbs_dot_ifi_dot_lmu_dot_de'); return false;">seidl_at_dbs_dot_ifi_dot_lmu_dot_de</a>&gt;, Data Management and Exploration Group, RWTH Aachen
                        University</div>
                <div class="author"><a rel="external" href="../bios.html#mittelberg_irene">Irene Mittelberg</a> &lt;<a href="mailto:mittelberg_at_humtec_dot_rwth-aachen_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('mittelberg_at_humtec_dot_rwth-aachen_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('mittelberg_at_humtec_dot_rwth-aachen_dot_de'); return false;">mittelberg_at_humtec_dot_rwth-aachen_dot_de</a>&gt;, Natural Media Lab, Human Technology Centre, RWTH Aachen
                        University</div>
            
            

            
        
        
        
        
    <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Automated%20Pattern%20Analysis%20in%20Gesture%20Research%3A%20Similarity%20Measuring%20in%203D%20Motion%20Capture%20Models%20of%20Communicative%20Action&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=2017-05-22&amp;rft.volume=011&amp;rft.issue=2&amp;rft.aulast=Schüller&amp;rft.aufirst=Daniel&amp;rft.au=Daniel%20Schüller&amp;rft.au=Christian%20Beecks&amp;rft.au=Marwan%20Hassani&amp;rft.au=Jennifer%20Hinnell&amp;rft.au=Bela%20Brenger&amp;rft.au=Thomas%20Seidl&amp;rft.au=Irene%20Mittelberg"> </span></div>

    <div id="DHQtext">
        
            <div id="abstract"><h2>Abstract</h2>
                
                <p>The question of how to model similarity between gestures plays an important role
                    in current studies in the domain of human communication. Most research into
                    recurrent patterns in co-verbal gestures – manual communicative movements
                    emerging spontaneously during conversation – is driven by qualitative analyses
                    relying on observational comparisons between gestures. Due to the fact that
                    these kinds of gestures are not bound to well-formedness conditions, however, we
                    propose a quantitative approach consisting of a distance-based similarity model
                    for gestures recorded and represented in motion capture data streams. To this
                    end, we model gestures by flexible feature representations, namely gesture
                    signatures, which are then compared via signature-based distance functions such
                    as the Earth Mover's Distance and the Signature Quadratic Form Distance.
                    Experiments on real conversational motion capture data evidence the
                    appropriateness of the proposed approaches in terms of their accuracy and
                    efficiency. Our contribution to gesture similarity research and gesture data
                    analysis allows for new quantitative methods of identifying patterns of gestural
                    movements in human face-to-face interaction, i.e., in complex multimodal data
                    sets. </p>
            </div>
            
        
        
            
            
            <div class="div div0">
                <h1 class="head">Introduction</h1>
                <div class="counter"><a href="#p1">1</a></div><div class="ptext" id="p1">

                    Given the central place of the <em class="term">embodied mind</em> in experientialist
                    approaches to language, co-verbal gestures have become a valuable data source in
                    cognitive, functional and anthropological linguistics (e.g. [<a class="ref" href="#sweetser2007">Sweetser 2007</a>]). While there exist various views on embodiment,
                    the core idea is that human higher cognitive abilities are shaped by the
                    morphology of our bodies and the way we interact with the material, spatial and
                    social environment (e.g. [<a class="ref" href="#gibbs2006">Gibbs 2006</a>]; [<a class="ref" href="#johnson1987">Johnson 1987</a>]). Drawing on these premises, some gesture scholars stress that gestures are
                    conditioned by the forms and affordances of their material habitat as well as
                    the speakers’ interactive and collaborative practices (e.g. [<a class="ref" href="#enfield2009">Enfield 2009</a>]; [<a class="ref" href="#streeck2011">Streeck 2011</a>]). Pioneering work done
                    by Kendon (e.g. [<a class="ref" href="#kendon1972">Kendon 1972</a>], [<a class="ref" href="#kendon2004">Kendon 2004</a>]),
                    McNeill (e.g. [<a class="ref" href="#mcneill1985">McNeill 1985</a>], [<a class="ref" href="#mcneill1992">McNeill 1992</a>], [<a class="ref" href="#mcneill2005">McNeill 2005</a>]) and Müller [<a class="ref" href="#mueller1998">Müller 1998</a>] has shown
                    that manual gestures are an integral part of utterance formation and
                    communicative interaction. The state-of-the-art of research in the growing
                    interdisciplinary field of <em class="term">gesture studies</em> has recently been
                    presented in the <cite class="title italic">International Handbook on Multimodality in Human
                        Interaction</cite> ([<a class="ref" href="#mueller2013">Müller 2013</a>], Vol. 1 2013, [<a class="ref" href="#mueller2014">Müller 2014</a>] Vol. 2 2014). One quintessence to be drawn from this
                    large body of work is that language, whether spoken or signed, is embodied,
                    dynamic, multimodal and intersubjective (see also [<a class="ref" href="#duncan2007">Duncan 2007</a>];
                        [<a class="ref" href="#gibbs2006">Gibbs 2006</a>]; [<a class="ref" href="#jaeger2004">Jäger 2004</a>]; [<a class="ref" href="#mittelberg2013">Mittelberg 2013</a>]; [<a class="ref" href="#mueller2008">Müller 2008</a>]). </div>
                <div class="counter"><a href="#p2">2</a></div><div class="ptext" id="p2"> Indeed, human communication typically involves multiple modalities such as
                    vocalizations, spoken or signed discourse, manual gestures, eye gaze, body
                    posture and facial expressions. In face-to-face communication, manual gestures
                    play an important role by conveying meaningful information and guiding the
                    interlocutors’ attention to objects and persons talked about. Gestures here are
                    understood as spontaneously emerging, dynamic configurations and movements of
                    the speakers’ hands and arms that contribute to the communicative content and
                    partake in the interactive organization of a spoken dialogue situation (e.g.
                        [<a class="ref" href="#bavelas1992">Bavelas 1992</a>]; [<a class="ref" href="#kendon2004">Kendon 2004</a>]; [<a class="ref" href="#mcneill1992">McNeill 1992</a>]; [<a class="ref" href="#mueller1998">Müller 1998</a>]; [<a class="ref" href="#mittelberg2016">Mittelberg 2016</a>]). The contribution of gestures to multimodal
                    interaction may consist in, e.g., deictic reference to locations, ideas, persons
                    or things (both abstract and concrete); they may also fulfill metalinguistic
                    functions, e.g., in referring to citations of other speakers or outlining the
                    structure of their argumentation. Gestures may also provide schematic, iconic
                    portrayals of actions, things or spatial constellations [<a class="ref" href="#mittelberg2014">Mitteberg 2014</a>]; [<a class="ref" href="#rieser2012">Rieser 2012</a>]. As we use the term
                    here, gestures are not to be confused with emblems (e.g. the victory sign),
                    which have a culturally defined form-meaning correlation in the same sense that
                    words may have a fixed meaning [<a class="ref" href="#mcneill1992">McNeill 1992</a>]. So gestures are
                    always to be investigated in view of the co-occurring speech with which they
                    jointly create (new) semiotic material and support speakers in organizing their
                    thoughts or drawing connections to their social and physical environment [<a class="ref" href="#streeck2011">Streeck 2011</a>]; [<a class="ref" href="#mittelberg2016">Mittelberg 2016</a>]. </div>
                <div class="counter"><a href="#p3">3</a></div><div class="ptext" id="p3"> Drawing on this large body of gesture research across various fields of the
                    humanities and social sciences, the interdisciplinary approach presented here
                    aims at identifying and visualizing patterns of gestural behavior with the help
                    of custom-tailored computational tools and methods. Although co-speech gestures
                    tend to be regarded as highly idiosyncratic in respect to their spontaneous
                    individual articulation by speakers in spoken dialogue situations, it is safe to
                    assume that there are recurring forms of dynamic hand configurations and
                    movement patterns which are performed by speakers sharing the same cultural
                    background. From this assumption follows the hypothesis that, on the one hand, a
                    general degree of similarity between gestural forms may be presumed – trivially
                    – due to the shared morphology of the human body (e.g. [<a class="ref" href="#tomasello1999">Tomasello 1999</a>]). On the other hand, the specific cultural context
                    plays an important role in both language acquisition and the adoption of
                    culture-specific behavioral patterns (see, e.g. [<a class="ref" href="#bourdieu1987">Bourdieu 1987</a>] on
                    Habitus and Hexis). Moreover, gesture has also been ascribed a constitutive role
                    regarding the human capacity for language both ontogenetically and
                    phylogenetically (e.g. [<a class="ref" href="#goldin-meadow2003">Goldin-Meadow 2003</a>]; [<a class="ref" href="#mcneill2012">McNeill 2012</a>]). Previous empirical gesture research indeed shows
                    that co-verbal gestures exhibit recurrent form features and movement patterns,
                    as well as recurring form-meaning pairings: see, for instance, Bressem [<a class="ref" href="#bressem2013">Bressem 2013</a>] on form features; Fricke [<a class="ref" href="#fricke2010">Fricke 2010</a>]
                    on "kinaesthemes"; Kendon [<a class="ref" href="#kendon2004">Kendon 2004</a>] on "locution clusters",
                    McNeill [<a class="ref" href="#mcneill1992">McNeill 1992</a>], [<a class="ref" href="#mcneill2000">McNeill 2000</a>] on
                    cross-linguistic path and manner imagery portraying motion events; McNeill [<a class="ref" href="#mcneill2005">McNeill 2005</a>] on "catchments"; Ladewig [<a class="ref" href="#ladewig2011">Ladewig 2011</a>] and Müller [<a class="ref" href="#mueller2010">Müller 2010</a>] on "recurring gestures", and Cienki
                        [<a class="ref" href="#cienki2005">Cienki 2005</a>] and Mittelberg [<a class="ref" href="#mittelberg2010">Mittelberg 2010</a>]
                    on image-schematic patterns. From this perspective, a question central to
                    gesture research concerns the factors that may motivate and lend a certain
                    systematicity to forms and functions of human communicative behaviors not
                    constituting an independent sign system in the Saussurian sense as spoken and
                    signed languages do. Manual gesture is a semiotically versatile medium, for it
                    may, depending on a given context, assume more or less language-like functions:
                    from accompanying spoken discourse in the form of pointing, accentuating beats,
                    schematic iconicity or managing social interaction to carrying the full load of
                    language (e.g. [<a class="ref" href="#goldin-meadow2007">Goldin-Meadow 2007</a>]). </div>
                <div class="counter"><a href="#p4">4</a></div><div class="ptext" id="p4"> In this paper, we will focus on certain kinds of co-verbal gestures, i.e.
                    specific image-schematic gestalts, e.g. spirals, circles, and straight paths
                        [<a class="ref" href="#cienki2013">Cienki 2013</a>], [<a class="ref" href="#mittelberg2010">Mittelberg 2010</a>]. Figure 1 shows
                    visualizations of several different gestural traces belonging to these movement
                    types. In what follows, we will present a novel method designed to identify,
                    search and cluster in an automatized fashion gestural movement patterns
                    throughout our data set, and potentially also any other motion capture data set,
                    e.g. to recognize and group those traces that are similar regarding their formal
                    features and the way in which they unfold in gesture space. </div>
                
                
                <div class="figure">
                    <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" alt=""/></a></div>
                <div class="caption-no-label"><div class="label">Figure 1. </div></div></div>
            </div>
            <div class="div div0">
                <h1 class="head">Research Objective</h1>
                <div class="counter"><a href="#p5">5</a></div><div class="ptext" id="p5"> Whereas the gesture research discussed above mostly relies on observational
                    methods and qualitative video analyses, our aim is to add to the catalogue of
                    methods for empirical linguistics and gesture studies by outlining a
                    computational, quantitative and comparative 3D-model driven approach in gesture
                    research. While there is a trend to combine qualitative with quantitative as
                    well as experimental methods in multimodal communication research [<a class="ref" href="#gonzalez-marquez2007">Gonzalez-Marquez 2007</a>]; [<a class="ref" href="#mcneill2001">McNeill 2001</a>]; [<a class="ref" href="#mueller2013">Müller 2013</a>], standardized and widely applicable tools and
                    methods still need to be developed. In order to derive statistically significant
                    patterns from aligned linguistic and behavioral data, some recent research
                    initiatives have started to compile and work with larger-scale corpora, e.g.
                    drawing on technological advances in data management and automated analyses
                    (e.g. the Little Red Hen Project, [<a class="ref" href="#steen2013">Steen 2013</a>]). While using audio
                    and video technology to record co-speech gestures remains the dominant way to
                    construct multimodal corpora, some research teams have begun to employ 3D motion
                    capture technology to overcome the limits of 2D video, e.g. Lu and Huenerfauth
                        [<a class="ref" href="#lu2010">Lu 2010</a>] for signed language, and Beskow et al. [<a class="ref" href="#beskow2011">Beskow 2011</a>] and Pfeiffer et al. [<a class="ref" href="#pfeiffer2013a">Pfeiffer 2013a</a>]
                    for spoken discourse (see also [<a class="ref" href="#pfeiffer2013">Pfeiffer 2013</a>] for an overview).
                    Our contribution is to obtain some kind of numerical instrument for graduating
                    gestural similarity for measuring gesture similarity in sets of recorded
                    behavioral data. While this instrument, in its current state, in no way
                    addresses the problem of the meaning of certain gestural forms, it is the first
                    step towards a model of measuring similarity between recurring dynamic gesture
                    form patterns. Our goal is to first establish a robust, flexible and automated
                    methodology which allows us to determine 
                    <ol class="list"><li class="item">whether there are shared or common reoccurring gestural movement
                            patterns in a given set of 3D recorded, behavioral communication
                            data,</li><li class="item">exactly which forms there are, and</li><li class="item">the extent to which they occur,</li></ol> and then to apply this methodology to the recorded 3D numerical MoCap data
                    of a group of participants. </div>
                <div class="counter"><a href="#p6">6</a></div><div class="ptext" id="p6"> Both the alignment of gestures with the co-occurring speech, and the semantic
                    comparison of the established (formally) sufficiently similar gesture-speech
                    constructions, still have to be done manually by human gesture researchers,
                    through semiotic analyses of the multimodal, speech and behavioral data corpora.
                    The primary aim of developing an automated indicator of gesture similarity is to
                    identify recurrent movement patterns of interest from the recorded 3D corpus
                    data computationally, and thus to enable human gesture researchers to handle
                    these data sets in a more efficient manner. In order to make gesture similarity
                    automatically accessible, we propose a distance-based similarity model for
                    gestures arising in three-dimensional motion capture data streams. In comparison
                    to two-dimensional video capture technology, working with numerical
                    three-dimensional motion capture technology has the advantage of measuring and
                    visualizing the temporal and spatial dynamics of otherwise invisible movement
                    traces with the highest possible accuracy. We aim at maintaining this accuracy
                    by aggregating movement traces, also called trajectories, into a <em class="term">gesture
                        signature</em>
                    [<a class="ref" href="#beecks2015">Beecks 2015</a>]. This gesture signature has the ability of weighting
                    trajectories according to their relevance. Based on this lossless feature
                    representation, we propose to measure similarity by means of distance-based
                    approaches [<a class="ref" href="#beecks2013">Beecks 2013</a>], [<a class="ref" href="#beecks2010">Beecks 2010</a>]. We
                    particularly investigate the <em class="term">Earth Mover's Distance</em>
                    [<a class="ref" href="#rubner2000">Rubner 2000</a>] and the <em class="term">Signature Quadratic Form
                        Distance</em>
                    [<a class="ref" href="#beecks2010">Beecks 2010</a>] for the comparison of two gesture signatures on real
                    conversational motion capture data. </div>
            </div>
            <div class="div div0">
                <h1 class="head">Properties of the 3D Data Model</h1>
                <div class="counter"><a href="#p7">7</a></div><div class="ptext" id="p7"> From a philosophy of science point of view, before being able to apply
                    computational algorithms to naturalistic real-world gestures, there must be a
                    translation from the real-world dialogue situations, involving people speaking
                    and gesturing, from which data are captured, to a computable set of data. For
                    this purpose, a marker-based <cite class="title italic">Vicon</cite> Motion Capture system was used
                    in this study. Participants wear a series of markers attached to predetermined
                    body parts of interest (fingers, wrists, elbows, neck, head, etc.). The
                        <cite class="title italic">Vicon</cite> system automatically generates a chart of numerical
                    4-tuples of Euclidean space-time coordinates for each marker attached to these
                    points on the participants’ bodies. The movement of the markers is tracked by 14
                        <cite class="title italic">Vicon</cite> infrared cameras, and the physical trajectories of the
                    markers are represented in a chart of space-time coordinates. These space-time
                    charts form the data sets that are investigated algorithmically, relieving the
                    gesture analyst of the difficult, and subjective, task of manually examining
                    highly ephemeral real-world dialogue situations. But what are the crucial
                    features that such a numerical representation must have in order to enable
                    researchers to not only investigate a model but also to finally derive
                    statements and theories about a modeled real-world situation? We address the
                    following research questions: Which logical features of the model are essential
                    if one wants to investigate the real world by investigating a model? And
                    secondly, what are the epistemic benefits of investigating models instead of
                    real-world situations? </div>
                <div class="counter"><a href="#p8">8</a></div><div class="ptext" id="p8"> The most important feature is that the model <em class="emph">represents</em> its
                    representandum. The representandum in question is a set of relevant features and
                    relations of a given part of reality, namely the change in space-time location
                    of certain body parts caused by the participants’ kinetic movement. The
                    representing model, on the other hand, is the virtual, computational 3D
                    recording of the real-world kinetic movement, mapped onto Euclidean space.
                    Representation itself is the relation which holds between the model and its
                    representandum. Representation, as we understand the term here, is a
                    non-reflexive and non-symmetric relation, which simply means that an
                        <em class="emph">a</em> does not necessarily represent <em class="emph">a</em>, and that if
                        <em class="emph">a</em> represents <em class="emph">b</em>, then <em class="emph">b</em> does not
                    necessarily represent <em class="emph">a</em>. We further assume that representation
                    depends on transitive relations, such as identity between some complex
                    relational features of the entities to be represented in a model and the
                    entities that represent/model them. In short, this means that if there are
                    entities <em class="emph">x, y, z,</em> there must be at least one mutual complex
                    relational feature <em class="emph">Fr(x,y,z),</em> which these entities have in common.
                    The transitive relation <em class="emph">R</em>, then, is the "identity" relation, in
                    that two entities have an identical relation to a third entity – a frame of
                    reference, or a <span class="foreign i">tertium comparationis</span>. </div>
                <div class="counter"><a href="#p9">9</a></div><div class="ptext" id="p9">
                    <span class="hi bold">Definition:</span> Transitivity </div>
                <div class="counter"><a href="#p10">10</a></div><div class="ptext" id="p10"> For a binary relation <em class="emph">R</em> over a set <em class="emph">A</em> consisting of
                        <em class="emph">x, y, z,</em> the following statement holds true: </div>
   
                    <div class="example"><div class="ptext">∀x,y,z ∈ A: xRy &amp; yRz → xRz</div><div class="caption-no-label"><div class="label">example 1. </div></div></div>
               
                <div class="counter"><a href="#p11">11</a></div><div class="ptext" id="p11"> The transitivity of <em class="emph">R</em> is so important here because, in terms of
                    modeling, it is the crucial feature that R must have. The different relata
                    involved are: movements of body parts <em class="emph">(x)</em>, movements of markers
                        <em class="emph">(y)</em>, and computational trajectories <em class="emph">(z)</em>. The
                    relation R which holds between these relata is the identity of their curve with
                    respect to space – either to a given virtual Euclidian space, or the physical
                    space (which functions as a reference frame). The identity of the movement curve
                    of body parts and the movement curve of markers simply stems from their physical
                    attachment/ conjunction in physical space. The identity of marker movements and
                    the computational trajectories is a result of metering the markers’ light
                    reflections, by means of the 14 Vicon infrared cameras, and numerically mapping
                    the outcome onto Euclidean space coordinates. But if one differentiates between
                    physical and Euclidean space, there is hardly any identity one could honestly
                    speak of in this case. In what sense could physical and virtual movement curves
                    ever be identical? Only in the sense that we <em class="emph">identify</em> physical and
                    Euclidean space by conventions of scientific modeling and metering practices;
                    the term <em class="term">curve</em> is itself an indication of how familiar that convention is.
                    Since Euclidean space is a conventionalized and well-accepted geometrical model
                    of physical space – we are well accustomed to talking about physical movement in
                    terms of, distances, trajectories, vectors, miles, kilometres and change in
                    space/time coordinates etc. – one can say that Euclidean space is our familiar
                    standard model for describing our perception of movement in physical space, both
                    in everyday conversation and scientific discussion. Thus, it is justified to
                    speak of the <em class="term">identity</em> of the spatial curves of trajectories in Euclidean space
                    and those of moving physical objects such as Motion Capture markers, only if we
                    accept this convention. Regarding representation, what does this imply? If
                    representation depends on the transitivity of relation R holding between the
                    entities <em class="emph">x, y, z,</em> and the identity of that relation depends on
                    conventions of metering and modeling, representation additionally depends on
                    following conventions. Given a 4-tuple of coordinates of a MoCap marker, the
                    movement of this marker is modeled by a vector which points from tuple-1 to
                    tuple-2 to tuple-n. The crucial feature of this kind of modeling is that the
                    movement of a single marker <em class="emph">a</em> is represented as a dynamic
                    space-time trajectory which aggregates the consecutively changing coordinates of
                        <em class="emph">a</em> over a given time frame. </div>
                <div class="counter"><a href="#p12">12</a></div><div class="ptext" id="p12"> Regarding the above-mentioned definition of transitivity, let our variables take
                    the following values: </div>
                <div class="counter"><a href="#p13">13</a></div><div class="ptext" id="p13">
                    
                        <span class="hi bold">x</span> = movement of body part from position a to b;<br/> 
                        <span class="hi bold">y</span> = movement of marker M from position a to b;<br/>
                        <span class="hi bold">z</span> = trajectory of marker M 
                </div>
                <div class="counter"><a href="#p14">14</a></div><div class="ptext" id="p14"> Given these values, we outline the transitivity relation as follows: </div>
                <div class="example"><div class="ptext">∀x,y,z ∈ A: xRy &amp; yRz → xRz: x [movement of body part from position a
                        to b] <span class="hi bold">R</span> y [movement of marker M from position a to b]
                        &amp; y [movement of marker M from position a to b] <span class="hi bold">R</span> z
                        [trajectory of marker M] → x [movement of body part from position a to b]
                            <span class="hi bold">R</span> z [trajectory of marker M].</div><div class="caption-no-label"><div class="label">example 2. </div></div></div>

                <div class="counter"><a href="#p15">15</a></div><div class="ptext" id="p15"> This means that if <em class="emph">x, y, z</em> obtain a transitive relation
                        <em class="emph">R</em> in the above sense, then <em class="emph">x, y,</em> and
                        <em class="emph">z</em> are to be regarded as homomorphous abstract
                        <em class="emph">concepts</em> that all denote the same event of spatiotemporal
                    movement, and <em class="emph">R</em> is an equivalence relation. So the
                        <em class="emph">extensional</em> equivalence of these concepts is a necessary and
                    sufficient condition that allows us to investigate reality by investigating the
                    model: If a language and its translation are equivalent, it should be equally
                    valid to investigate one or the other. However, since the translation of the
                    concept "<em class="emph">movement</em> of marker" into "<em class="emph">aggregated</em> marker
                        <em class="emph">coordinates</em>" fails to be an <em class="emph">intensionally</em> adequate
                    translation, i.e. the concepts do not <em class="emph">mean</em> the same (event vs.
                    aggregated states of affairs), it could at first seem odd to describe real
                    movement in terms of trajectories. But, since the concepts are at least
                    phenomenologically and extensionally equivalent, this basically remains a
                    question of the interpreter’s ontology (see [<a class="ref" href="#quine1980">Quine 1980</a>]) and how
                    the final research result is to be formulated. If we decide to treat "event" and
                    "aggregated states of affairs" as being synonymous, the problem completely
                    disappears. Otherwise, we have to re-translate the problematic concept into one
                    which suits our needs. In terms of epistemic benefits, one major advantage of
                    the proposed distance-based gesture-similarity model (see the following
                    section), i.e. the combination of gesture signatures with signature-based
                    distance functions, is its applicability to any type of gestural pattern and to
                    data sets of any size. In fact, distance-based similarity models can be utilized
                    in order to model similarity between gestural patterns whose movement types are
                    well known and between gestural patterns whose inherent structures are
                    completely unknown. In this way, they provide an unsupervised way of modeling
                    gesture similarity. This flexibility is attributable to the fact that the
                    proposed approaches are model independent, i.e. no complex gesture model has to
                    be learned in a comprehensive training phase prior to indexing and query
                    processing. Another advantage of the proposed distance-based gesture-similarity
                    model is the possibility of efficient query processing. Although calculating the
                    distance between two gesture signatures is a computationally expensive task,
                    which results in at least a quadratic computation time complexity with respect
                    to the number of relevant trajectories, many approaches such as the independent
                    minimization lower bound of the Earth Mover's Distance on feature signatures
                        [<a class="ref" href="#uysal2014">Uysal 2014</a>] and metric indexing [<a class="ref" href="#beecks2011">Beecks 2011</a>],
                    as well as the Ptolemaic indexing [<a class="ref" href="#hetland2013">Hetland 2013</a>] of the Signature
                    Quadratic Form Distance, are available for efficient query processing and, thus,
                    for assessing gesture similarity in a larger quantitative way. </div>
            </div>
            <div class="div div0">
                <h1 class="head">Modeling Gesture Similarity</h1>
                <div class="counter"><a href="#p16">16</a></div><div class="ptext" id="p16"> In this section, we present a distance-based similarity model for the comparison
                    of gestures within three-dimensional motion capture data streams. To this end,
                    we first introduce <em class="term">gesture signatures</em> as a formal model of gestures
                    arising in motion capture data streams. Since gesture signatures comprise
                    multiple three-dimensional trajectories, we continue with outlining distance
                    functions for trajectories before we investigate distance functions applicable
                    to gesture signatures. </div>
                <div class="div div1">
                    <h2 class="head">Gesture Signatures</h2>
                    <div class="counter"><a href="#p17">17</a></div><div class="ptext" id="p17"> Motion capture data streams can be thought of as sequences of points in a
                        three-dimensional Euclidean space. In the scope of this work, these points
                        arise from several reflective markers which are attached to the body and in
                        particular to the hands of a participant. The motion of the markers is
                        triangulated via multiple cameras and finally recorded every 10
                        milliseconds. In this way, each marker defines a finite trajectory of points
                        in a three-dimensional space. The formal definition of a trajectory is given
                        below. </div>
                    <div class="counter"><a href="#p18">18</a></div><div class="ptext" id="p18">
                        <span class="hi bold">Definition:</span> Trajectory </div>
                    <div class="counter"><a href="#p19">19</a></div><div class="ptext" id="p19"> Given a three-dimensional feature space R<span class="hi superscript">3</span>, a
                        trajectory <em class="emph">t</em>:{1,…,n}→ R<span class="hi superscript">3</span> is defined
                        for all 1≤i≤n as: </div>
                    <div class="example"><div class="ptext">
                        <em class="emph">t</em>(i)=(x<span class="hi subscript">i</span>,y<span class="hi subscript">i</span>,z<span class="hi subscript">i</span>) </div><div class="caption-no-label"><div class="label">example 3. </div></div></div>
                    <div class="counter"><a href="#p20">20</a></div><div class="ptext" id="p20"> A trajectory describes the motion of a single marker in a three-dimensional
                        space. It is worth noting that the time information is abstracted to
                        integral numbers in order to model trajectories arising from different time
                        intervals. Since a gesture typically arises from multiple markers within a
                        certain period of time, we aggregate several trajectories including their
                        individual relevance by means of a gesture signature. For this purpose, we
                        denote the set of all finite trajectories as trajectory space T=∪<span class="hi subscript">k∈N</span>{t| t:{1,…,k}→ R<span class="hi superscript">3</span>}
                        , which is time-invariant, and define a gesture signature as a function from
                        the trajectory space T into the real numbers R. The formal definition of a
                        gesture signature is given below. </div>
                    <div class="counter"><a href="#p21">21</a></div><div class="ptext" id="p21">
                        <span class="hi bold">Definition:</span> Gesture Signature </div>
                    <div class="counter"><a href="#p22">22</a></div><div class="ptext" id="p22"> Let T be a trajectory space. A <em class="term">gesture signature S</em>∈R<span class="hi superscript">T</span> is defined as: </div>
                    <div class="example"><div class="ptext"> S:T→ R subject to |S<span class="hi superscript">-1</span>(R{0})|&lt;∞ </div><div class="caption-no-label"><div class="label">example 4. </div></div></div>
                    <div class="counter"><a href="#p23">23</a></div><div class="ptext" id="p23">
                        A gesture signature formalizes a gesture by assigning a finite number of
                        trajectories non-zero weights reflecting their importance. Negative weights
                        are immaterial in practice but ensure the gesture space
                            S={<em class="emph">S</em>∈R<span class="hi superscript">T</span>∧|S<span class="hi superscript">-1</span>(R{0})|&lt;∞} forms a vector space. While a
                        weight of zero indicates insignificance of a trajectory, a positive weight
                        is utilized to indicate contribution to the corresponding gesture. In this
                        way, a gesture signature allows us to focus on the trajectories arising from
                        those markers which actually form a gesture. For example, if a gesture is
                        expressed by the participant's hands, only the corresponding hand markers
                        and thus trajectories have to be weighted positively. </div>
                    
                    <div class="counter"><a href="#p24">24</a></div><div class="ptext" id="p24"> A gesture signature defines a generic mathematical model but omits a
                        concrete functional implementation. In fact, given a subset of relevant
                        trajectories 𝒯<span class="hi superscript">+</span>⊂T, the most naive way of
                        defining a gesture signature <em class="emph">S</em> consists in assigning relevant
                        trajectories a weight of one and irrelevant trajectories a weight of zero,
                        i.e. by defining <em class="emph">S</em> for all <em class="emph">t</em>∈T as follows: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img1.png" rel="external"><img src="resources/images/math_img1.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 2. </div></div></div>
                    <div class="counter"><a href="#p25">25</a></div><div class="ptext" id="p25"> The isotropic behavior of this approach, however, completely ignores the
                        inherent characteristics of the relevant trajectories. We therefore weight
                        each relevant trajectory according to its inherent properties of
                            <em class="term">motion distance</em> and <em class="term">motion variance</em>. These
                        properties are defined below. </div>
                    <div class="counter"><a href="#p26">26</a></div><div class="ptext" id="p26">
                        <span class="hi bold">Definition:</span> Motion Distance and Motion Variance </div>
                    <div class="counter"><a href="#p27">27</a></div><div class="ptext" id="p27"> Let T be a trajectory space and <em class="emph">t</em>:{1,…,n}→ R<span class="hi superscript">3</span> be a trajectory. The <em class="emph">motion distance
                            m</em><span class="hi subscript">δ</span>:T→R of trajectory <em class="emph">t</em> is
                        defined as: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img2.png" rel="external"><img src="resources/images/math_img2.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 3. </div></div></div>
                    <div class="counter"><a href="#p28">28</a></div><div class="ptext" id="p28"> The motion variance m<span class="hi subscript">σ<span class="hi superscript">2</span></span>:T→R of trajectory <em class="emph">t</em> is defined with mean </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img3.png" rel="external"><img src="resources/images/math_img3.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 4. </div></div></div>
                    <div class="counter"><a href="#p29">29</a></div><div class="ptext" id="p29"> as: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img4.png" rel="external"><img src="resources/images/math_img4.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 5. </div></div></div>
                    <div class="counter"><a href="#p30">30</a></div><div class="ptext" id="p30"> The intuition behind motion distance and motion variance is to take into
                        account the overall movement and vividness of a trajectory. The higher these
                        qualities, the more information the trajectory may contain and vice versa.
                        Their utilization with respect to a set of relevant trajectories finally
                        leads to the definitions of a <em class="emph">motion distance gesture signature</em>
                        and a <em class="emph">motion variance gesture signature</em>, as shown below. </div>
                    <div class="counter"><a href="#p31">31</a></div><div class="ptext" id="p31">
                        <span class="hi bold">Definition:</span> Motion Distance Gesture Signature and
                        Motion Variance Gesture Signature </div>
                    <div class="counter"><a href="#p32">32</a></div><div class="ptext" id="p32"> Let T be a trajectory space and 𝒯<span class="hi superscript">+</span>⊂T be a
                        subset of relevant trajectories. A motion distance gesture signature S<span class="hi subscript">m<span class="hi subscript">δ</span></span>∈R<span class="hi superscript">T</span> is defined for all <em class="emph">t</em>∈T as: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img5.png" rel="external"><img src="resources/images/math_img5.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 6. </div></div></div>
                    <div class="counter"><a href="#p33">33</a></div><div class="ptext" id="p33"> A motion variance gesture signature S<span class="hi subscript">m<span class="hi subscript">σ<span class="hi superscript">2</span></span></span>∈R<span class="hi superscript">T</span> is defined for all <em class="emph">t</em>∈T as: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img6.png" rel="external"><img src="resources/images/math_img6.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 7. </div></div></div>
                    <div class="counter"><a href="#p34">34</a></div><div class="ptext" id="p34"> Motion distance and motion variance gesture signatures are able to reflect
                        the characteristics of the expressed gestures with respect to the
                        corresponding relevant trajectories by adapting the number and weighting of
                        relevant trajectories. As a consequence, the computation of a
                        (dis)similarity value between gesture signatures is frequently based on the
                        (dis)similarity values among the involved trajectories in the trajectory
                        space. We thus outline applicable trajectory distance functions in the
                        following section. </div>
                </div>
                <div class="div div1">
                    <h2 class="head">Trajectory Distance Functions</h2>
                    <div class="counter"><a href="#p35">35</a></div><div class="ptext" id="p35"> Due to the nature of trajectories whose inherent properties are rarely
                        expressible in a single figure, trajectories are frequently compared by
                        aligning their coincident similar points with each other. A prominent
                        example is the <em class="term">Dynamic Time Warping Distance</em>, which was first
                        introduced in the field of speech recognition by Itakura [<a class="ref" href="#itakura1975">Itakura 1975</a>] and Sakoe and Chiba [<a class="ref" href="#sakoe1978">Sakoe 1978</a>]
                        and later brought to the domain of pattern detection in databases by Berndt
                        and Clifford [<a class="ref" href="#berndt1994">Berndt 1994</a>]. The idea of this distance is to
                        locally replicate points of the trajectories in order to fit the
                        trajectories to each other. The point-wise distances finally yield the
                        Dynamic Time Warping Distance, whose formal definition is given below. </div>
                    <div class="counter"><a href="#p36">36</a></div><div class="ptext" id="p36">
                        <span class="hi bold">Definition:</span> Dynamic Time Warping Distance </div>
                    <div class="counter"><a href="#p37">37</a></div><div class="ptext" id="p37"> Let <em class="emph">t</em><span class="hi subscript">n</span>:{1,…,n}→ R<span class="hi superscript">3</span> and t<span class="hi subscript">m</span>:{1,…,m}→ R<span class="hi superscript">3</span> be two trajectories from T and
                            <em class="emph">δ</em>:R<span class="hi superscript">3</span>×R<span class="hi superscript">3</span>→R be a distance function. The <em class="emph">Dynamic Time Warping
                            Distance DTW</em><span class="hi subscript">δ</span>:T×T→R between
                            <em class="emph">t</em><span class="hi subscript">n</span> and <em class="emph">t</em><span class="hi subscript">m</span> is recursively defined as: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img7.png" rel="external"><img src="resources/images/math_img7.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 8. </div></div></div>
                    <div class="counter"><a href="#p38">38</a></div><div class="ptext" id="p38"> with </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img8.png" rel="external"><img src="resources/images/math_img8.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 9. </div></div></div>
                    <div class="counter"><a href="#p39">39</a></div><div class="ptext" id="p39"> As can be seen in the definition above, the Dynamic Time Warping Distance is
                        defined recursively by minimizing the distances <em class="emph">δ</em> between
                        replicated elements of the trajectories. In this way, the distance
                            <em class="emph">δ</em> assesses the spatial proximity of two points while the
                        Dynamic Time Warping Distance preserves their temporal order within the
                        trajectories. By utilizing Dynamic Programming, the computation time
                        complexity of the Dynamic Time Warping Distance lies in 𝒪(n·m). </div>
                    <div class="counter"><a href="#p40">40</a></div><div class="ptext" id="p40"> Although there exist further approaches for the comparison of trajectories,
                        such as <em class="term">Edit Distance on Real Sequences</em> [<a class="ref" href="#chen2005">Chen 2005</a>],
                            <em class="term">Minimal Variance Matching</em>
                        [<a class="ref" href="#latecki2005">Latecki 2005</a>], and <em class="term">Mutual Nearest Point Distance</em>
                        [<a class="ref" href="#fang2009">Fang 2009</a>], we have decided to utilize the Dynamic Time
                        Warping Distance for the following reasons: (i) The distance value is based
                        on all points of the trajectories with respect to their temporal order and
                        is not attributed to partial characteristics of the trajectories, (ii) it
                        provides the ability of exact indexing by lower bounding [<a class="ref" href="#keogh2002">Keogh 2002</a>], and (iii) it indicates superior quality in terms
                        of accuracy within preliminary investigations. </div>
                    <div class="counter"><a href="#p41">41</a></div><div class="ptext" id="p41"> Given a ground distance in the trajectory space T, we will show in the
                        following section how to lift this ground distance to the gesture space
                            S⊂R<span class="hi superscript">T</span> in order to compare gesture signatures
                        with each other. </div>
                </div>
                <div class="div div1">
                    <h2 class="head">Gesture Signature Distance Functions</h2>
                    <div class="counter"><a href="#p42">42</a></div><div class="ptext" id="p42"> Gesture signatures can differ in size and length, i.e., in the number of
                        relevant trajectories and in the lengths of those trajectories. In order to
                        quantify the distance between differently structured gesture signatures, we
                        apply signature-based distance functions [<a class="ref" href="#beecks2013">Beecks 2013</a>], [<a class="ref" href="#beecks2010">Beecks 2010</a>]. In this paper, we focus on those signature-based
                        distance functions that consider the entire structure of two gesture
                        signatures in order not to favor partial similarity between short and long
                        gesture signatures. For this reason, we investigate the transformation-based
                            <em class="term">Earth Mover's Distance</em>
                        [<a class="ref" href="#rubner2000">Rubner 2000</a>] and the correlation-based <em class="term">Signature
                            Quadratic Form Distance</em>
                        [<a class="ref" href="#beecks2010">Beecks 2010</a>] in the remainder of this section. </div>
                    <div class="counter"><a href="#p43">43</a></div><div class="ptext" id="p43"> The Earth Mover's Distance, whose name was inspired by Stolfi and his vivid
                        description of the transportation problem, which he likened to finding the
                        minimal cost to move a total amount of earth from earth hills into holes
                            [<a class="ref" href="#rubner2000">Rubner 2000</a>], has been originated in the computer vision
                        domain. It defines the distance between two gesture signatures by measuring
                        the cost of transforming one gesture signature into another one. The formal
                        definition of the Earth Mover's Distance is given below. </div>
                    <div class="counter"><a href="#p44">44</a></div><div class="ptext" id="p44">
                        <span class="hi bold">Definition:</span> Earth Mover’s Distance </div>
                    <div class="counter"><a href="#p45">45</a></div><div class="ptext" id="p45"> Let <em class="emph">S</em><span class="hi subscript">1</span>,<em class="emph">S</em><span class="hi subscript">2</span>∈S be two gesture signatures and
                        <em class="emph">δ</em>:T×T→R be a trajectory distance function. The <em class="emph">Earth
                            Mover’s Distance EMD</em><span class="hi subscript">δ</span>:S×S→R between
                            <em class="emph">S</em><span class="hi subscript">1</span> and <em class="emph">S</em><span class="hi subscript">2</span> is defined as a minimum cost flow of all
                        possible flows <em class="emph">F</em>={f| f: T×T→R} as: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img9.png" rel="external"><img src="resources/images/math_img9.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 10. </div></div></div>
                    <div class="counter"><a href="#p46">46</a></div><div class="ptext" id="p46"> subject to the constraints: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img10.png" rel="external"><img src="resources/images/math_img10.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 11. </div></div></div>
                    <div class="counter"><a href="#p47">47</a></div><div class="ptext" id="p47"> As can be seen in the definition above, the Earth Mover's Distance between
                        two gesture signatures is defined as a linear optimization problem subject
                        to non-negative flows which do not exceed the corresponding limitations
                        given by the weights of the trajectories of both gesture signatures. The
                        computation of the Earth Mover's Distance can be restricted to the relevant
                        trajectories of both gesture signatures and follows a specific variant of
                        the simplex algorithm [<a class="ref" href="#hillier1990">Hillier 1990</a>]. </div>
                    <div class="counter"><a href="#p48">48</a></div><div class="ptext" id="p48"> The idea of the Signature Quadratic Form Distance consists in adapting the
                        generic concept of <em class="term">correlation</em> to gesture signatures. In
                        general, correlation is the most basic measure of bivariate relationship
                        between two variables [<a class="ref" href="#rodgers1988">Rodgers 1988</a>] and can be interpreted as
                        the amount of variance these variables share [<a class="ref" href="#rovine1997">Rovine 1997</a>]. In
                        order to apply the concept of correlation to gesture signatures, all
                        trajectories and corresponding weights are related with each other based on
                        a trajectory similarity function <em class="emph">s</em>:T×T→R. The resulting
                            <em class="term">similarity correlation</em> between two gesture signatures
                            <em class="emph">S</em><span class="hi subscript">1</span>,<em class="emph">S</em><span class="hi subscript">2</span>∈S is then defined as: </div>
                                        
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img11.png" rel="external"><img src="resources/images/math_img11.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 12. </div></div></div>
                    <div class="counter"><a href="#p49">49</a></div><div class="ptext" id="p49"> The similarity correlation between two gesture signatures finally leads to
                        the definition of the Signature Quadratic Form Distance, as shown below. </div>
                    <div class="counter"><a href="#p50">50</a></div><div class="ptext" id="p50">
                        <span class="hi bold">Definition:</span> Signature Quadratic Form Distance </div>
                    <div class="counter"><a href="#p51">51</a></div><div class="ptext" id="p51"> Let <em class="emph">S</em><span class="hi subscript">1</span>,<em class="emph">S</em><span class="hi subscript">2</span>∈S be two gesture signatures and
                        <em class="emph">s</em>:T×T→R be a trajectory similarity function. The
                            <em class="emph">Signature Quadratic Form Distance SQFD</em><span class="hi subscript">s</span>:S×S→R between <em class="emph">S</em><span class="hi subscript">1</span> and
                            <em class="emph">S</em><span class="hi subscript">2</span> is defined as: </div>
                    
                    <div class="figure">
                        <div class="ptext"><a href="resources/images/math_img12.png" rel="external"><img src="resources/images/math_img12.png" alt=""/></a></div>
                    <div class="caption-no-label"><div class="label">Figure 13. </div></div></div>
                    <div class="counter"><a href="#p52">52</a></div><div class="ptext" id="p52"> The Signature Quadratic Form Distance is defined by adding the
                        intra-similarity correlations &lt;<em class="emph">S</em><span class="hi subscript">1</span>,<em class="emph">S</em><span class="hi subscript">1</span>&gt;<span class="hi subscript">s</span> and &lt;<em class="emph">S</em><span class="hi subscript">2</span>,<em class="emph">S</em><span class="hi subscript">2</span>&gt;<span class="hi subscript">s</span> of the gesture signatures <em class="emph">S</em><span class="hi subscript">1</span> and <em class="emph">S</em><span class="hi subscript">2</span>
                        and subtracting their inter-similarity correlation &lt;<em class="emph">S</em><span class="hi subscript">1</span>,<em class="emph">S</em><span class="hi subscript">2</span>&gt;<span class="hi subscript">s</span>. The smaller the differences among the
                        intra-similarity and inter-similarity correlations the lower the resulting
                        Signature Quadratic Form Distance, and vice versa. The computation of the
                        Signature Quadratic Form Distance can be restricted to the relevant
                        trajectories of both gesture signatures and has a quadratic computation time
                        complexity with respect to the number of relevant trajectories. </div>
                    <div class="counter"><a href="#p53">53</a></div><div class="ptext" id="p53"> More details regarding the Earth Mover's Distance and the Signature
                        Quadratic Form Distance as well as possible similarity functions can be
                        found for instance in the PhD thesis of Beecks [<a class="ref" href="#beecks2013b">Beecks 2013b</a>].
                        Among other approaches, such as the matching-based Signature Matching
                        Distance [<a class="ref" href="#beecks2013">Beecks 2013</a>], the aforementioned signature-based
                        distance functions have been shown to balance the trade-off between
                        retrieval accuracy and query processing efficiency. Before we investigate
                        their performance in the context of gesture signatures, we devote the next
                        section to a discussion about the properties of the proposed distance-based
                        gesture similarity model. </div>
                </div>
            </div>
            <div class="div div0">
                <h1 class="head">Experimental Evaluation</h1>
                <div class="counter"><a href="#p54">54</a></div><div class="ptext" id="p54"> Evaluating the performance of distance-based similarity models is a highly
                    empirical discipline. It is nearly unforeseeable which approach will provide the
                    best retrieval performance in terms of accuracy. To this end, we qualitatively
                    evaluated the proposed distance-based approaches to gesture similarity by using
                    a natural media corpus of motion capture data collected for this project. This
                    dataset comprises three-dimensional motion capture data streams arising from
                    eight participants during a guided conversation. The participants were equipped
                    with a multitude of reflective markers which were attached to the body and in
                    particular to the hands. The motion of the markers was tracked optically via
                    cameras at a frequency of 100 Hz. In the scope of this work, we used the right
                    wrist marker and two markers attached to the right thumb and right index finger
                    each. The gestures arising within the conversation were classified by domain
                    experts according to the following types of movement: spiral, circle, and
                    straight. Example gestures of these movement types are sketched in Figure 1. A
                    total of 20 gesture signatures containing five trajectories each was obtained
                    from the motion capture data streams. The trajectories of the gesture signatures
                    have been normalized to the interval [0,1]<span class="hi superscript">3</span>∈R<span class="hi superscript">3</span> in order to maintain translation invariance. </div>
                <div class="counter"><a href="#p55">55</a></div><div class="ptext" id="p55"> The resulting distance matrices between all gesture signatures with respect to
                    the Earth Mover's Distance and the Signature Quadratic Form Distance are shown
                    in Figure 2 and Figure 3, respectively. As described in the previous Section, we
                    utilized the Dynamic Time Warping Distance based on Euclidean Distance as
                    trajectory distance for the Earth Mover's Distance and converted this trajectory
                    distance by means of the power kernel [<a class="ref" href="#schoelkopf2001">Schölkopf 2001</a>] with
                    parameter <em class="emph">α</em>=1 into a trajectory similarity function for the
                    Signature Quadratic Form Distance. Since weighting of relevant trajectories by
                    motion distance and motion variance, approximately shows a similar behavior, we
                    include the results regarding motion variance gesture signatures only. We depict
                    small and large distance values by bluish and reddish colors in order to
                    visually indicate the performance of our proposal: gesture signatures from the
                    same movement type should result in bluish colors while gesture signatures from
                    different movement types should result in reddish colors. </div>
                <div class="counter"><a href="#p56">56</a></div><div class="ptext" id="p56"> As can be seen in Figure 2 and Figure 3, both Earth Mover's Distance and
                    Signature Quadratic Form Distance show the same tendency in terms of gestural
                    dissimilarity. Although distance values computed through the aforementioned
                    distance functions have different orders of magnitude, both gesture signature
                    distance functions are generally able to distinguish gesture signatures from
                    different movement types. On average, gesture signatures belonging to the same
                    movement type are less dissimilar to each other than gesture signatures from
                    different movement types. We further observed that the distinction between
                    gesture signatures from the movement types spiral and straight are most
                    challenging. This is caused by a similar sequence of movement of these two
                    gestural types. While gesture signatures belonging to the movement type straight
                    follow a certain direction, e.g., movement on the horizontal axis, gesture
                    signatures from the movement type spiral additionally oscillate with respect to
                    a certain direction. Since this oscillation can be dominated by the movement
                    direction, the underlying trajectory distance functions are often unable to
                    distinguish oscillating from non-oscillating trajectories and thus gesture
                    signature of movement type spiral from those of movement type straight. </div>
                <div class="counter"><a href="#p57">57</a></div><div class="ptext" id="p57"> Apart from the quality of accuracy, efficiency is another important aspect when
                    evaluating the performance of gesture similarity models. For this purpose, we
                    measured the computation times needed to perform single distance computations on
                    a single-core 3.4 GHz machine. We implemented the proposed distance-based
                    approaches in Java 1.7. The Earth Mover's Distance, which needs on average 148.6
                    milliseconds for a single distance computation, is approximately three times
                    faster than the Signature Quadratic Form Distance, which needs on average 479.8
                    milliseconds for a single distance computation. In spite of the theoretically
                    exponential and empirically super-cubic computation time complexity of the Earth
                    Mover's Distance [<a class="ref" href="#uysal2014">Uysal 2014</a>], this distance is able to outperform
                    the Signature Quadratic Form Distance. The reason for this is the high number of
                    computationally expensive trajectory distance computations. While the
                    computation of the Earth Mover's Distance is carried out on the trajectory
                    distances between the two gesture signatures, the computation of the Signature
                    Quadratic Form Distance additionally takes into account the trajectory distances
                    within both gesture signatures. Therefore the number of trajectory distance
                    computations is significantly higher for the Signature Quadratic Form Distance
                    than for the Earth Mover's Distance. </div>
                <div class="counter"><a href="#p58">58</a></div><div class="ptext" id="p58"> To sum up, the experimental evaluation reveals that the proposed distance-based
                    approaches are able to model gesture similarity in a flexible and
                    model-independent way. Without the need for a preceding training phase, the
                    Earth Mover's Distance and the Signature Quadratic Form Distance are able to
                    provide similarity models for searching similar gestures which are formalized
                    through gesture signatures. </div>
                
                <div class="figure">
                    <div class="ptext"><a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" alt=""/></a></div>
                <div class="caption-no-label"><div class="label">Figure 14. </div></div></div>
                
                <div class="figure">
                    <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" alt=""/></a></div>
                <div class="caption-no-label"><div class="label">Figure 15. </div></div></div>
            </div>
            <div class="div div0">
                <h1 class="head">Conclusions and Future Work </h1>
                <div class="counter"><a href="#p59">59</a></div><div class="ptext" id="p59"> In this paper, we have investigated distance-based approaches to measure
                    similarity between gestures arising in three-dimensional motion capture data
                    streams. To this end, we have explicated gesture signatures as a way of
                    aggregating the inherent characteristics of spontaneously produced co-speech
                    gestures and signature-based distance functions such as the Earth Mover's
                    Distance and the Signature Quadratic Form Distance in order to quantify
                    dissimilarity between gesture signatures. The experiments conducted on real data
                    are evidence of the appropriateness in terms of accuracy and efficiency of the
                    proposal. </div>
                <div class="counter"><a href="#p60">60</a></div><div class="ptext" id="p60"> In future work, we intend to extend our research on gesture similarity towards
                    indexing and efficient query processing. While the focus of the present paper
                    lies on dissimilarity between pairs of gestures, we further plan to
                    quantitatively analyze motion capture data streams in a query-driven way in
                    order to support the domain experts' qualitative analyses of gestural patterns
                    within multi-media contexts. The overall goal of this research is to contribute
                    to the advancement of automated methods of pattern recognition in gesture
                    research by enhancing qualitative analyses of complex multimodal data in the
                    humanities and social sciences. While this paper focuses on formal features of
                    the gestural movements, further steps will entail examining the semantic and
                    pragmatic dimensions of these patterns in light of the cultural contexts and
                    embodied semiotic practices they emerge from. </div>
            </div>
            <div class="div div0">
                <h1 class="head">Acknowledgment</h1>
                <div class="counter"><a href="#p61">61</a></div><div class="ptext" id="p61"> This work is partially funded by the Excellence Initiative of the German federal
                    and state governments and DFG grant SE 1039/7-1. This work extends [<a class="ref" href="#beecks2015">Beecks 2015</a>]. </div>
            </div>
        
        
            
        
    </div>
<div id="worksCited"><h2>Works Cited</h2><div class="bibl"><span class="ref" id="bavelas1992"><!-- close -->Bavelas 1992</span> Bavelas, J. B., Chovil, N., Lawrie,
                    D. A. and Wade, A. "Interactive Gestures", <cite class="title italic">Discourse Processes</cite>, 15 (1992):
                    469-489.</div><div class="bibl"><span class="ref" id="beecks2010"><!-- close -->Beecks 2010</span> Beecks, C., M.S. Uysal, and Seidl, T.
                    "A Comparative Study of Similarity Measures for Content-Based Multimedia
                        Retrieval". <cite class="title italic">Proceedings of the IEEE International Conference on Multimedia and
                    Expo</cite>, pp. 1552-1557 (2010).</div><div class="bibl"><span class="ref" id="beecks2010a"><!-- close -->Beecks 2010a</span> Beecks, C., M.S. Uysal, and Seidl,
                    T. " Signature Quadratic Form Distance". <cite class="title italic">Proceedings of the 9th International
                    Conference on Image and Video Retrieval</cite>, pp. 438-445 (2010).</div><div class="bibl"><span class="ref" id="beecks2011"><!-- close -->Beecks 2011</span> Beecks, C., Lokoc, J., Seidl, T., and
                    Skopal, T. "Indexing the signature quadratic form distance for efficient
                        content-based multimedia retrieval". In <cite class="title italic">Proceedings of the ACM International
                    Conference on Multimedia Retrieval</cite>, 2011, pp. 24:1–8.</div><div class="bibl"><span class="ref" id="beecks2013"><!-- close -->Beecks 2013</span> Beecks, C., S. Kirchhoff, and Seidl,
                    T. "On Stability of Signature-Based Similarity Measures for Content-Based Image
                        Retrieval". <cite class="title italic">Multimedia Tools and Applications</cite>, pp. 1-14.</div><div class="bibl"><span class="ref" id="beecks2013a"><!-- close -->Beecks 2013a</span> Beecks, C., S. Kirchhoff, and Seidl,
                    T. "Signature matching distance for content-based image retrieval". <cite class="title italic">Proceedings
                    of the ACM International Conference on Multimedia Retrieval</cite>, pp. 41-48
                    (2013)</div><div class="bibl"><span class="ref" id="beecks2013b"><!-- close -->Beecks 2013b</span> Beecks, C. <cite class="title italic">Distance-Based Similarity
                    Models for Content-Based Multimedia Retrieval</cite>. Ph.D. thesis, RWTH Aachen
                    University (2013). Available online:
                    <a class="ref" href="http://darwin.bth.rwth-aachen.de/opus3/volltexte/2013/4807/" onclick="window.open('http://darwin.bth.rwth-aachen.de/opus3/volltexte/2013/4807/'); return false">http://darwin.bth.rwth-aachen.de/opus3/volltexte/2013/4807/</a></div><div class="bibl"><span class="ref" id="beecks2015"><!-- close -->Beecks 2015</span> Beecks, C., Hassani, M., Hinnell, J.,
                    Schüller, D., Brenger, B., Mittelberg, I. and Seidl, T. "Spatiotemporal
                    Similarity Search in 3D Motion Capture Gesture Streams". <cite class="title italic">Proceedings of the 14th
                    International Symposium on Spatial and Temporal Databases</cite>, pp. 355-372
                    (2015).</div><div class="bibl"><span class="ref" id="beecks2015a"><!-- close -->Beecks 2015a</span> Beecks, C., J. Lokoc, T. Seidl, and
                    Skopal, T. "Indexing the Signature Quadratic Form Distance for Efficient
                    Content-Based Multimedia Retrieval". <cite class="title italic">Proceedings of the ACM International
                    Conference on Multimedia Retrieval</cite>, pp. 24:1-8 (2015).</div><div class="bibl"><span class="ref" id="berndt1994"><!-- close -->Berndt 1994</span> Berndt, D. and Clifford, J. "Using
                    dynamic time warping to find patterns in time series". <cite class="title italic">AAAI94 Workshop on
                    Knowledge Discovery in Databases</cite>, pp. 359-370 (1994)</div><div class="bibl"><span class="ref" id="beskow2011"><!-- close -->Beskow 2011</span> Beskow, J., Alex, S., Al Moubayed, S.,
                    Edlund, J. and House, D. "Kinetic Data for Large-Scale Analysis and Modeling of
                        Face-to-Face Conversation". In <cite class="title italic">Proceedings of the International Conference on
                    Audio-Visual Speech Processing</cite>, Stockholm, pp. 103–106 (2011).</div><div class="bibl"><span class="ref" id="bourdieu1987"><!-- close -->Bourdieu 1987</span> Bourdieu, P. Sozialer Sinn. <cite class="title italic">Kritik
                    der theoretischen Vernunft</cite>. Suhrkamp, Frankfurt am Main (1987).</div><div class="bibl"><span class="ref" id="bressem2013"><!-- close -->Bressem 2013</span> Bressem, J. "A Linguistic
                    Perspective on the Notation of Form Features in Gestures". In Müller, C. et al.
                    (eds): <cite class="title italic">Body – Language – Communication. An International Handbook of
                    Multimodality in Human Interaction</cite>. (HSK 38.1). De Gruyter Mouton,
                    Berlin/Boston,pp. 1079–1098 (2013).</div><div class="bibl"><span class="ref" id="chen2005"><!-- close -->Chen 2005</span> Chen, L., Özsu, M.T. and Oria, V. "Robust
                    and Fast Similarity Search for Moving Object Trajectories". <cite class="title italic">Proceedings of the
                    ACM SIGMOD International Conference on Management of Data</cite>, pp. 491-502
                    (2005).</div><div class="bibl"><span class="ref" id="cienki2005"><!-- close -->Cienki 2005</span> Cienki, A. "Image Schemas and
                    Gesture". In Hampe, B. (ed). From <cite class="title italic">Perception to Meaning: Image Schemas in
                    Cognitive Linguistics</cite>. Mouton de Gruyter, Berlin/New York (2005).</div><div class="bibl"><span class="ref" id="cienki2013"><!-- close -->Cienki 2013</span> Cienki, A. "Cognitive Linguistics:
                    Spoken Language and Gesture as Expressions of Conceptualization". In Müller, C.,
                    Cienki, A., Fricke, E., Ladewig, S., McNeill, D. and Teßendorf, S. (eds), <cite class="title italic">Body–
                    Language–Communication: An International Handbook on Multimodality in Human
                    Interaction</cite>, Vol. 1. Mouton de Gruyter, Berlin/New York, pp. 182–201
                    (2013).</div><div class="bibl"><span class="ref" id="duncan2007"><!-- close -->Duncan 2007</span> Duncan, S., Cassell, J. and Levy E.T.
                    (eds), <cite class="title italic">Gesture and the Dynamic Dimension of Language</cite>. John Benjamins, Amsterdam/
                    Philadelphia, pp. 269–283 (2007).</div><div class="bibl"><span class="ref" id="enfield2009"><!-- close -->Enfield 2009</span> Enfield, N. <cite class="title italic">The Anatomy of Meaning.
                    Speech, Gestures, and Composite Utterances</cite>. Cambridge University Press,
                    Cambridge (2009).</div><div class="bibl"><span class="ref" id="fang2009"><!-- close -->Fang 2009</span> Fang, S. and Chan, H. "Human
                    Identification by Quantifying Similarity and Dissimilarity in Electrocardiogram
                    Phase Space". <cite class="title italic">Pattern Recognition</cite>, vol. 42, 9 (2009): 1824-1831.</div><div class="bibl"><span class="ref" id="fricke2010"><!-- close -->Fricke 2010</span> Fricke, E. "Phonaestheme, Kinaestheme
                    und Multimodale Grammatik". <cite class="title italic">Sprache und Literatur</cite>, 41 (2010): 69–88.</div><div class="bibl"><span class="ref" id="gibbs2006"><!-- close -->Gibbs 2006</span> Gibbs, R. W., Jr. <cite class="title italic">Embodiment and
                    Cognitive Science</cite>. Cambridge University Press, Cambridge (2006).</div><div class="bibl"><span class="ref" id="goldin-meadow2003"><!-- close -->Goldin-Meadow 2003</span> Goldin-Meadow, S.
                    <cite class="title italic">Hearing Gesture: How Our Hands Help Us Think</cite>. Harvard University Press,
                    Cambridge, MA (2003).</div><div class="bibl"><span class="ref" id="goldin-meadow2007"><!-- close -->Goldin-Meadow 2007</span> Goldin-Meadow, S.
                    "Gesture with Speech and Without It". In Duncan, S. (ed), <cite class="title italic">Gesture and the
                    Dynamic Dimension of Language: Essays in Honor of David McNeill</cite>. John Benjamins
                    Publishing Company, Amsterdam/Philadelphia, pp. 31–49 (2007).</div><div class="bibl"><span class="ref" id="gonzalez-marquez2007"><!-- close -->Gonzalez-Marquez 2007</span> Gonzalez-Marquez,
                    M., Mittelberg, I., Coulson, S. and Spivey, M.J. (eds). <cite class="title italic">Methods in Cognitive
                    Linguistics</cite>. John Benjamins, Amsterdam/Philadelphia (2007).</div><div class="bibl"><span class="ref" id="hetland2013"><!-- close -->Hetland 2013</span> Hetland, M.L., Skopal, T., Lokoc, J.
                    and Beecks, C. "Ptolemaic Access Methods: Challenging the Reign of the Metric
                        Space Model". <cite class="title italic">Information Systems</cite>, vol.38, no.7 (2013): 989-1006.</div><div class="bibl"><span class="ref" id="hillier1990"><!-- close -->Hillier 1990</span> Hillier, F. and Lieberman, G..
                    <cite class="title italic">Introduction to Linear Programming</cite>. McGraw-Hill.(1990)</div><div class="bibl"><span class="ref" id="itakura1975"><!-- close -->Itakura 1975</span> Itakura, F. "Minimum Prediction
                    Residual Principle Applied to Speech Recognition". <cite class="title italic">IEEE Transactions on
                    Acoustics, Speech and Signal Processing</cite>, vol.23, no.1 (1975): 67-72.</div><div class="bibl"><span class="ref" id="johnson1987"><!-- close -->Johnson 1987</span> Johnson, M. <cite class="title italic">The Body in the Mind:
                    The Bodily Basis of Meaning, Imagination, and Reason</cite>. University of Chicago
                    Press, Chicago (1987).</div><div class="bibl"><span class="ref" id="jaeger2004"><!-- close -->Jäger 2004</span> Jäger, L. &amp; Linz, E. (eds)
                    <cite class="title italic">Medialität und Mentalität. Theoretische und empirische Studien zum Verhältnis
                    von Sprache, Subjektivität und Kognition</cite>. Fink Verlag, München (2004).</div><div class="bibl"><span class="ref" id="kendon1972"><!-- close -->Kendon 1972</span> Kendon, A. "Some Relationships between
                    Body Motion and Speech. An analysis of an example". In Siegman, A. and Pope, B.
                    (eds), <cite class="title italic">Studies in Dyadic Communication</cite>. Pergamon Press, Elmsford, NY, pp.
                    177–210 (1972).</div><div class="bibl"><span class="ref" id="kendon2004"><!-- close -->Kendon 2004</span> Kendon, A. <cite class="title italic">Gesture: Visible Action as
                    Utterance</cite>. Cambridge University Press, Cambridge (2004).</div><div class="bibl"><span class="ref" id="keogh2002"><!-- close -->Keogh 2002</span> Keogh, E. "Exact Indexing of Dynamic
                    Time Warping". <cite class="title italic">Proceedings of 28th International Conference on Very Large Data
                    Bases</cite>, 406-417 (2002).</div><div class="bibl"><span class="ref" id="ladewig2011"><!-- close -->Ladewig 2011</span> Ladewig, S. "Putting the cyclic
                    gesture on a cognitive basis". <cite class="title italic">CogniTextes</cite>, 6 (2011).</div><div class="bibl"><span class="ref" id="latecki2005"><!-- close -->Latecki 2005</span> Latecki, L.J., Megalooikonomou, V.,
                    Wang, Q., Lakämper, R., Ratanamahatana, C.A. and Keogh, E.J. "Elastic Partial
                        Matching of Time Series.Knowledge Discovery in Databases", <cite class="title italic">9th European
                    Conference on Principles and Practice of Knowledge Discovery in Databases,
                    Lecture Notes in Computer Science</cite>, vol. 3721 (2005), Springer, pp.
                    577-584.</div><div class="bibl"><span class="ref" id="lu2010"><!-- close -->Lu 2010</span> Lu, P. and Huenerfauth, M. "Collecting a
                    Motion-Capture Corpus of American Sign Language for Data-Driven Generation
                    Research". In <cite class="title italic">Proceedings of the NAACL HLT 2010 Workshop on Speech and Language
                    Processing for Assistive Technologies</cite>. Los Angeles, pp. 89–97 (2010).</div><div class="bibl"><span class="ref" id="mcneill1985"><!-- close -->McNeill 1985</span> McNeill, D. "So You Think Gestures
                    are Nonverbal?". <cite class="title italic">Psychological Review</cite>, 92,3 (1985): 350–371.</div><div class="bibl"><span class="ref" id="mcneill1992"><!-- close -->McNeill 1992</span> McNeill, D. <cite class="title italic">Hand and Mind: What
                    Gestures Reveal about Thought</cite>. Chicago University Press, Chicago (1992).</div><div class="bibl"><span class="ref" id="mcneill2000"><!-- close -->McNeill 2000</span> McNeill, D. (ed). <cite class="title italic">Language and
                    Gesture</cite>. Cambridge University Press, Cambridge (2000).</div><div class="bibl"><span class="ref" id="mcneill2001"><!-- close -->McNeill 2001</span> McNeill, D., Quek, F., McCullough,
                    K.-E., Duncan, S., Furuyama, N., Bryll, R., Ma, X.-F. and Ansari, R. 2001.
                    "Catchments, Prosody and Discourse", <cite class="title italic">Gesture</cite> 1, 1 (2001): 9–33.</div><div class="bibl"><span class="ref" id="mcneill2005"><!-- close -->McNeill 2005</span> McNeill, D. Gesture and Thought.
                    Chicago University Press, Chicago (2005).</div><div class="bibl"><span class="ref" id="mcneill2012"><!-- close -->McNeill 2012</span> McNeill, D. <cite class="title italic">How Language Began:
                    Gesture and Speech in Human Evolution</cite>. Cambridge University Press, Cambridge
                    (2012).</div><div class="bibl"><span class="ref" id="mittelberg2014"><!-- close -->Mitteberg 2014</span> Mittelberg, I. and Waugh, L.R.
                    "Gestures and Metonymy". In Müller, C., Cienki, A., Fricke, E., Ladewig, S.H.,
                    McNeill, D. and Bressem, J. (eds), <cite class="title italic">Body – Language – Communication. An
                    International Handbook on Multimodality in Human Interaction</cite>. Vol. 2. Handbooks
                    of Linguistics and Communcation Science. De Gruyter Mouton, Berlin/Boston, pp.
                    1747–1766 (2014).</div><div class="bibl"><span class="ref" id="mittelberg2010"><!-- close -->Mittelberg 2010</span> Mittelberg, I. "Geometric and
                    Image-Schematic Patterns in Gesture Space". In Evans, V. and Chilton, P. (eds),
                    <cite class="title italic">Language, Cognition, and Space: The State of the Art and New Directions</cite>.
                    Equinox, London, pp., 351–385 (2010).</div><div class="bibl"><span class="ref" id="mittelberg2010a"><!-- close -->Mittelberg 2010a</span> Mittelberg, I. "Interne und
                    externe Metonymie: Jakobsonsche Kontiguitätsbeziehungen in redebegleitenden
                    Gesten". <cite class="title italic">Sprache und Literatur</cite> 41,1 (2010): 112–143.</div><div class="bibl"><span class="ref" id="mittelberg2013"><!-- close -->Mittelberg 2013</span> Mittelberg, I. "Balancing
                    Acts: Image Schemas and Force Dynamics as Experiential Essence in Pictures by
                    Paul Klee and their Gestural Enactments". In B. Dancygier, M. Bokrent and J.
                    Hinnell (eds), <cite class="title italic">Language and the Creative Mind</cite>. Stanford: Center for the Study of
                    Language and Information, pp. 325–346.</div><div class="bibl"><span class="ref" id="mittelberg2013a"><!-- close -->Mittelberg 2013a</span> Mittelberg, I. "The Exbodied
                    Mind: Cognitive-Semiotic Principles as Motivating Forces in Gesture". In Müller,
                    C., Cienki, A., Fricke, E., Ladewig, S.H., McNeill, D. and Teßendorf, S. (eds).
                    <cite class="title italic">Body – Language – Communication: An International Handbook on Multimodality in
                    Human Interaction</cite>. Handbooks of Linguistics and Communication Science (38.1).
                    Mouton de Gruyter,Berlin/New York, pp. 750-779 (2013).</div><div class="bibl"><span class="ref" id="mittelberg2016"><!-- close -->Mittelberg 2016</span> Mittelberg, I., Schüller, D.
                    "Kulturwissenschaftliche Orientierung in der Gestenforschung". In Jäger, L.,
                    Holly, W., Krapp, P., Weber, S. (eds.). <cite class="title italic">Language – Culture – Communication. An
                    International Handbook of Linguistics as Cultural Study</cite>. Mouton de Gruyter,
                    Berlin/New York, pp. 879-890 (2016).</div><div class="bibl"><span class="ref" id="mueller1998"><!-- close -->Müller 1998</span> Müller, C. Redebegleitende Gesten.
                    <cite class="title italic">Kulturgeschichte - Theorie - Sprachvergleich</cite>. Berlin Verlag A. Spitz, Berlin
                    (1998).</div><div class="bibl"><span class="ref" id="mueller2008"><!-- close -->Müller 2008</span> Müller, C. <cite class="title italic">Metaphors Dead and Alive,
                    Sleeping and Waking. A Dynamic View</cite>. University of Chicago Press, Chicago
                    (2008).</div><div class="bibl"><span class="ref" id="mueller2010"><!-- close -->Müller 2010</span> Müller, C. "Wie Gesten bedeuten. Eine
                    kognitiv-linguistische und sequenzanalytische Perspektive", <cite class="title italic">Sprache und
                    Literatur</cite> 41 (2010): 37–68.</div><div class="bibl"><span class="ref" id="mueller2013"><!-- close -->Müller 2013</span> Müller, C., Cienki, A., Fricke, E.,
                    Ladewig, S., McNeill, D. and Teßendorf, S. (eds). <cite class="title italic">Body– Language–Communication:
                    An International Handbook on Multimodality in Human Interaction</cite>, Vol. 1. Mouton
                    de Gruyter, Berlin/New York (2013).</div><div class="bibl"><span class="ref" id="mueller2014"><!-- close -->Müller 2014</span> Müller, C., Cienki, A., Fricke, E.,
                    Ladewig, S., McNeill, D. and Bressem, J.(eds). <cite class="title italic">Body– Language–Communication: An
                    International Handbook on Multimodality in Human Interaction</cite>, Vol. 2. Mouton de
                    Gruyter, Berlin/New York (2014).</div><div class="bibl"><span class="ref" id="pfeiffer2013"><!-- close -->Pfeiffer 2013</span> Pfeiffer, T. "Documentation with
                    Motion Capture". In Müller, C., Cienki, A., Fricke, E., Ladewig, S.H., McNeill,
                    D. and Teßendorf, S. (eds). <cite class="title italic">Body- Language-Communication: An International Hand-
                    book on Multimodality in Human Interaction</cite>, Hand- books of Linguistics and
                    Communication Science. Mouton de Gruyter, Berlin, New York (2013).</div><div class="bibl"><span class="ref" id="pfeiffer2013a"><!-- close -->Pfeiffer 2013a</span> Pfeiffer, T., Hofmann, F., Hahn,
                    F., Rieser, H., and Röpke, I. "Gesture Semantics Reconstruction Based on Motion
                        Capturing and Complex Event Processing: A Circular Shape Example". In M.
                    Eskenazi, M. Strube, B. Di Eugenio, and J. D. Williams (eds). <cite class="title italic">Proceedings of the
                    Special Interest Group on Discourse and Dialog (SIGDIAL) 2013 Conference</cite>, pp.
                    270–279 (2013).</div><div class="bibl"><span class="ref" id="quine1980"><!-- close -->Quine 1980</span> Quine, W.V.O. <cite class="title italic">Wort und Gegenstand</cite>.
                    Reclam, Stuttgart (1980).</div><div class="bibl"><span class="ref" id="rieser2012"><!-- close -->Rieser 2012</span> Rieser H., Bergmann K. and Kopp, S.
                    "How do Iconic Gestures Convey Visuo-Spatial Information? Bringing Together
                    Empirical, Theoretical, and Simulation Studies." In Efthimiou, E. and
                    Kouroupetroglou, G. (eds). <cite class="title italic">Gestures in Embodied Communication and Human-Computer
                    Interaction</cite>. Springer, Berlin/Heidelberg (2012).</div><div class="bibl"><span class="ref" id="rodgers1988"><!-- close -->Rodgers 1988</span> Rodgers, J. and Nicewander, W.
                    <cite class="title italic">Thirteen Ways to Look at the Correlation Coefficient</cite>. American Statistician, pp.
                    59-66 (1988).</div><div class="bibl"><span class="ref" id="rovine1997"><!-- close -->Rovine 1997</span> Rovine, M. and Von Eye, A. "A 14th Way
                    to Look at a Correlation Coefficient: Correlation as the Poportion of Matches".
                    <cite class="title italic">American Statistician</cite>, pp. 42-46 (1997)</div><div class="bibl"><span class="ref" id="rubner2000"><!-- close -->Rubner 2000</span> Rubner, Y., C. Tomasi, and Guibas,
                    L.J. "The Earth Mover's Distance as a Metric for Image Retrieval", <cite class="title italic">International
                    Journal of Computer Vision</cite>, vol. 40,2 (2000): 99-121.</div><div class="bibl"><span class="ref" id="sakoe1978"><!-- close -->Sakoe 1978</span> Sakoe, H. and Chiba, S. "Dynamic
                    Programming Algorithm Optimization for Spoken Word Recognition", <cite class="title italic">IEEE
                    Transactions on Acoustics, Speech and Signal Processing</cite>, vol. 26, 1(1978):
                    43-49.</div><div class="bibl"><span class="ref" id="schoelkopf2001"><!-- close -->Schölkopf 2001</span> Schölkopf, B. "The Kernel Trick
                    for Distances". <cite class="title italic">Advances in Neural Information Processing Systems</cite>,
                    301-307.</div><div class="bibl"><span class="ref" id="steen2013"><!-- close -->Steen 2013</span> Steen, L. and Turner, M. "Multimodal
                    Construction Grammar". In Dancygier, B., Bokrent, M. and Hinnell, J. (eds).
                    <cite class="title italic">Language and the Creative Mind, Stanford: Center for the Study of Language and
                    Information</cite> (2013).</div><div class="bibl"><span class="ref" id="streeck2011"><!-- close -->Streeck 2011</span> Streeck, J., Goodwin, C. and
                    LeBaron, C.D. <cite class="title italic">Embodied Interaction: Language and Body in the Material World:
                    Learning in doing: Social, Cognitive and Computational Perspectives</cite>. Cambridge
                    University Press, New York (2011).</div><div class="bibl"><span class="ref" id="sweetser2007"><!-- close -->Sweetser 2007</span> Sweetser, E. "Looking at Space to
                    Study Mental Spaces: Co-speech Gesture as a Crucial Data Source in Cognitive
                    Linguistics". In Gonzalez-Marquez, M., Mittelberg, I., Coulson, S. and Spivey,
                    M. (eds). <cite class="title italic">Methods in Cognitive Linguistics</cite>. John Benjamins,
                    Amsterdam/Philadelphia, pp. 201¬224 (2007).</div><div class="bibl"><span class="ref" id="tomasello1999"><!-- close -->Tomasello 1999</span> Tomasello, M. <cite class="title italic">The Cultural
                    Origins of Human Cognition</cite>. Harvard University Press, Cambridge, MA
                    (1999).</div><div class="bibl"><span class="ref" id="uysal2014"><!-- close -->Uysal 2014</span> Uysal, M.S., Beecks, C., Schmücking, J.,
                    and Seidl, T. "Efficient Filter Approximation using the Earth Mover's Distance
                        in Very Large Multimedia Databases with Feature Signatures". <cite class="title italic">Proceedings of the
                    23rd ACM International Conference on Conference on Information and Knowledge
                    Management</cite>, pp. 979-988 (2014).</div></div><div class="toolbar"><a href="/dhq/vol/11/2/index.html">2017 11.2</a>
             | 
            <a rel="external" href="/dhq/vol/11/2/000309.xml">XML</a>
            | 
            <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div></div><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script><div id="comments"><div id="disqus_thread"/><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    // The following are highly recommended additional parameters. Remove the slashes in front to use.
    var disqus_identifier = '000309';
    var disqus_url = 'http://www.digitalhumanities.org/dhq/vol/11/2/000309/000309.html';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><div id="footer"> 
            URL: http://www.digitalhumanities.org/dhq/vol/11/2/000309/000309.html<br/>Last updated:
            <script type="text/javascript">
                var monthArray = new initArray("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December");
                var lastModifiedDate = new Date(document.lastModified);
                var currentDate = new Date();
                document.write(" ",monthArray[(lastModifiedDate.getMonth()+1)]," ");
                document.write(lastModifiedDate.getDate(),", ",(lastModifiedDate.getFullYear()));
            </script><br/> Comments: <a href="mailto:dhqinfo@digitalhumanities.org" class="footer">dhqinfo@digitalhumanities.org</a><br/> Published by:
            <a href="http://www.digitalhumanities.org" class="footer">The Alliance of Digital Humanities Organizations</a><br/>Affiliated with: <a href="http://llc.oxfordjournals.org/">Literary and Linguistic Computing</a><br/> Copyright 2005 - <script type="text/javascript">
                document.write(currentDate.getFullYear());</script><br/><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png"/></a><br/>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
        </div></div></div></body></html>