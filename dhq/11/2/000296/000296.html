<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><title>DHQ: Digital Humanities Quarterly: Exploratory Search Through Visual Analysis of Topic
                    Models</title><link rel="stylesheet" type="text/css" href="/dhq/common/css/dhq.css"/><link rel="stylesheet" type="text/css" media="screen" href="/dhq/common/css/dhq_screen.css"/><link rel="stylesheet" type="text/css" media="print" href="/dhq/common/css/dhq_print.css"/><link rel="alternate" type="application/atom+xml" href="/dhq/feed/news.xml"/><link rel="shortcut icon" href="/dhq/common/images/favicon.ico"/><script type="text/javascript" src="/dhq/common/js/javascriptLibrary.js">
                &lt;!-- Javascript functions --&gt;
            </script><script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-15812721-1']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type =
'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0];
s.parentNode.insertBefore(ga, s);
 })();

        </script></head><body><div id="top"><div id="backgroundpic"><script type="text/javascript" src="/dhq/common/js/pics.js"><!--displays banner image--></script></div><div id="banner"><div id="dhqlogo"><img src="/dhq/common/images/dhqlogo.png" alt="DHQ Logo"/></div><div id="longdhqlogo"><img src="/dhq/common/images/dhqlogolonger.png" alt="Digital Humanities Quarterly Logo"/></div></div><div id="topNavigation"><div id="topnavlinks"><span><a href="/dhq/" class="topnav">home</a></span><span><a href="/dhq/submissions/index.html" class="topnav">submissions</a></span><span><a href="/dhq/about/about.html" class="topnav">about dhq</a></span><span><a href="/dhq/people/people.html" class="topnav">dhq people</a></span><span id="rightmost"><a href="/dhq/contact/contact.html" class="topnav">contact</a></span></div><div id="search"><form action="/dhq/findIt" method="get" onsubmit="javascript:document.location.href=cleanSearch(this.queryString.value); return false;"><div><input type="text" name="queryString" size="18"/> <input type="submit" value="Search"/></div></form></div></div></div><div id="main"><div id="leftsidebar"><div id="leftsidenav"><span>Current Issue<br/></span><ul><li><a href="/dhq/vol/11/3/index.html">2017: 11.3</a></li></ul><span>Preview Issue<br/></span><ul><li><a href="/dhq/preview/index.html">2017: 11.4</a></li></ul><span>Previous Issues<br/></span><ul><li><a href="/dhq/vol/11/2/index.html">2017: 11.2</a></li><li><a href="/dhq/vol/11/1/index.html">2017: 11.1</a></li><li><a href="/dhq/vol/10/4/index.html">2016: 10.4</a></li><li><a href="/dhq/vol/10/3/index.html">2016: 10.3</a></li><li><a href="/dhq/vol/10/2/index.html">2016: 10.2</a></li><li><a href="/dhq/vol/10/1/index.html">2016: 10.1</a></li><li><a href="/dhq/vol/9/4/index.html">2015: 9.4</a></li><li><a href="/dhq/vol/9/3/index.html">2015: 9.3</a></li><li><a href="/dhq/vol/9/2/index.html">2015: 9.2</a></li><li><a href="/dhq/vol/9/1/index.html">2015: 9.1</a></li><li><a href="/dhq/vol/8/4/index.html">2014: 8.4</a></li><li><a href="/dhq/vol/8/3/index.html">2014: 8.3</a></li><li><a href="/dhq/vol/8/2/index.html">2014: 8.2</a></li><li><a href="/dhq/vol/8/1/index.html">2014: 8.1</a></li><li><a href="/dhq/vol/7/3/index.html">2013: 7.3</a></li><li><a href="/dhq/vol/7/2/index.html">2013: 7.2</a></li><li><a href="/dhq/vol/7/1/index.html">2013: 7.1</a></li><li><a href="/dhq/vol/6/3/index.html">2012: 6.3</a></li><li><a href="/dhq/vol/6/2/index.html">2012: 6.2</a></li><li><a href="/dhq/vol/6/1/index.html">2012: 6.1</a></li><li><a href="/dhq/vol/5/3/index.html">2011: 5.3</a></li><li><a href="/dhq/vol/5/2/index.html">2011: 5.2</a></li><li><a href="/dhq/vol/5/1/index.html">2011: 5.1</a></li><li><a href="/dhq/vol/4/2/index.html">2010: 4.2</a></li><li><a href="/dhq/vol/4/1/index.html">2010: 4.1</a></li><li><a href="/dhq/vol/3/4/index.html">2009: 3.4</a></li><li><a href="/dhq/vol/3/3/index.html">2009: 3.3</a></li><li><a href="/dhq/vol/3/2/index.html">2009: 3.2</a></li><li><a href="/dhq/vol/3/1/index.html">2009: 3.1</a></li><li><a href="/dhq/vol/2/1/index.html">2008: 2.1</a></li><li><a href="/dhq/vol/1/2/index.html">2007: 1.2</a></li><li><a href="/dhq/vol/1/1/index.html">2007: 1.1</a></li></ul><span>Indexes<br/></span><ul><li><a href="/dhq/index/title.html"> Title</a></li><li><a href="/dhq/index/author.html"> Author</a></li></ul></div><img src="/dhq/common/images/lbarrev.png" style="margin-left : 7px;" alt="sidenavbarimg"/><div id="leftsideID"><b>ISSN 1938-4122</b><br/></div><div class="leftsidecontent"><h3>Announcements</h3><ul><li><a href="/dhq/announcements/index.html#reviewers">Call for Reviewers</a></li><li><a href="/dhq/announcements/index.html#submissions">Call for Submissions</a></li></ul></div><div class="leftsidecontent"><script type="text/javascript">addthis_pub  = 'dhq';</script><a href="http://www.addthis.com/bookmark.php" onmouseover="return addthis_open(this, '', '[URL]', '[TITLE]')" onmouseout="addthis_close()" onclick="return addthis_sendto()"><img src="http://s9.addthis.com/button1-addthis.gif" width="125" height="16" alt="button1-addthis.gif"/></a><script type="text/javascript" src="http://s7.addthis.com/js/152/addthis_widget.js">&lt;!-- Javascript functions --&gt;</script></div></div><div id="mainContent"><div id="printSiteTitle">DHQ: Digital Humanities Quarterly</div><div xmlns:dhqBiblio="http://digitalhumanities.org/dhq/ns/biblio" class="DHQarticle"><div id="pubInfo">2017<br/>Volume 11 Number 2</div><div class="toolbar"><form id="taporware" action="get"><div><a href="/dhq/vol/11/2/index.html">2017 11.2</a>
                     | 
                    <a rel="external" href="/dhq/vol/11/2/000296.xml">XML</a>

| 
		   Discuss
			(<a href="/dhq/vol/11/2/000296/000296.html#disqus_thread" data-disqus-identifier="000296">
				Comments
			</a>)
                </div></form></div>
    <div class="DHQheader">
        
            
                
                <h1 class="articleTitle lang en">Exploratory Search Through Visual Analysis of Topic
                    Models</h1>
                <div class="author"><a rel="external" href="../bios.html#jähnichen_patrick">Patrick Jähnichen</a> &lt;<a href="mailto:jaehnichen_at_informatik_dot_uni-leipzig_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('jaehnichen_at_informatik_dot_uni-leipzig_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('jaehnichen_at_informatik_dot_uni-leipzig_dot_de'); return false;">jaehnichen_at_informatik_dot_uni-leipzig_dot_de</a>&gt;, Machine Learning Group, Humboldt-Universität zu Berlin<a class="noteRef" href="#d5517e27">[1]</a></div>

                <div class="author"><a rel="external" href="../bios.html#oesterling_patrick">Patrick Oesterling</a> &lt;<a href="mailto:oesterling_at_informatik_dot_uni-leipzig_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('oesterling_at_informatik_dot_uni-leipzig_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('oesterling_at_informatik_dot_uni-leipzig_dot_de'); return false;">oesterling_at_informatik_dot_uni-leipzig_dot_de</a>&gt;, Image and Signal Processing Group, Leipzig University,
                        Germany</div>

                <div class="author"><a rel="external" href="../bios.html#heyer_gerhard">Gerhard Heyer</a> &lt;<a href="mailto:heyer_at_informatik_dot_uni-leipzig_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('heyer_at_informatik_dot_uni-leipzig_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('heyer_at_informatik_dot_uni-leipzig_dot_de'); return false;">heyer_at_informatik_dot_uni-leipzig_dot_de</a>&gt;, Natural Language Processing Group, Leipzig University,
                        Germany</div>

                <div class="author"><a rel="external" href="../bios.html#liebmann_tom">Tom Liebmann</a> &lt;<a href="mailto:liebmann_at_informatik_dot_uni-leipzig_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('liebmann_at_informatik_dot_uni-leipzig_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('liebmann_at_informatik_dot_uni-leipzig_dot_de'); return false;">liebmann_at_informatik_dot_uni-leipzig_dot_de</a>&gt;, Image and Signal Processing Group, Leipzig University,
                        Germany</div>

                <div class="author"><a rel="external" href="../bios.html#scheuermann_gerik">Gerik Scheuermann</a> &lt;<a href="mailto:scheuermann_at_informatik_dot_uni-leipzig_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('scheuermann_at_informatik_dot_uni-leipzig_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('scheuermann_at_informatik_dot_uni-leipzig_dot_de'); return false;">scheuermann_at_informatik_dot_uni-leipzig_dot_de</a>&gt;, Image and Signal Processing Group, Leipzig University,
                        Germany</div>

                <div class="author"><a rel="external" href="../bios.html#kuras_christoph">Christoph Kuras</a> &lt;<a href="mailto:ckuras_at_informatik_dot_uni-leipzig_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('ckuras_at_informatik_dot_uni-leipzig_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('ckuras_at_informatik_dot_uni-leipzig_dot_de'); return false;">ckuras_at_informatik_dot_uni-leipzig_dot_de</a>&gt;, Natural Language Processing Group, Leipzig University,
                        Germany</div>

            
            

            
        
        
        
        
    <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Exploratory%20Search%20Through%20Visual%20Analysis%20of%20Topic%20Models&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=2017-05-22&amp;rft.volume=011&amp;rft.issue=2&amp;rft.aulast=Jähnichen&amp;rft.aufirst=Patrick&amp;rft.au=Patrick%20Jähnichen&amp;rft.au=Patrick%20Oesterling&amp;rft.au=Gerhard%20Heyer&amp;rft.au=Tom%20Liebmann&amp;rft.au=Gerik%20Scheuermann&amp;rft.au=Christoph%20Kuras"> </span></div>

    <div id="DHQtext">
        
            <div id="abstract"><h2>Abstract</h2>
                
                <p>This paper addresses exploratory search in large collections of historical texts.
                    By way of example, we apply our method to a collection of documents comprising
                    dossiers of the former East-German Ministry for State Security, and classical
                    texts. The bases of our approach are topic models, a class of algorithms that
                    define and infer themes pervading the corpus as probability distributions over
                    the vocabulary. Our topic-centered visual metaphor supports to explore the
                    corpus following an intuitive methodology: First, determine a topic of interest,
                    second, suggest documents that contain the topic with "sufficient" proportion,
                    and third, browse iteratively through related topics and documents. Our main
                    focus lies on providing a suitable bird's eye view onto the data to facilitate
                    an in-depth analysis in terms of the topics contained.</p>
            </div>
            
        
        
            
            

            <div class="div div0">
                <h1 class="head">Introduction</h1>
                <div class="counter"><a href="#p1">1</a></div><div class="ptext" id="p1">When dealing with large collections of digitized historical documents, very often
                    only little is known about the quantity, coverage and relations of its content.
                    In order to get an overview, an interactive way to explore the data is needed
                    that goes beyond simple "lookup" approaches. The notion of exploratory search
                    has been coined by [<a class="ref" href="#marchionini2006">Marchionini 2006</a>] to cover such cases.
                    Consider a large corpus of documents. Typically, we know the source and the
                    broader scope of such corpora, but not necessarily the content of individual
                    documents. One classical option to explore this data is based on key-word
                    search. While this approach is useful when the user "knows" what she is looking
                    for, an iterative <em class="emph">exploration</em> of the corpus is not possible. Our
                    approach is a structured one. We provide the user with a bird's eye view on the
                    data, she then identifies topics of interest and finds the documents related to
                    them. Additionally, these documents may also be related to other topics, a
                    connection that helps to reveal new and interesting insights previously unknown.
                    We are also able to identify different contexts in which specific terms appear,
                    i.e. dissipate semantic ambiguities that may appear. Especially when working
                    with historical texts, this might help to reveal new aspects of known
                    concepts.</div>
                <div class="counter"><a href="#p2">2</a></div><div class="ptext" id="p2">Topic modeling [<a class="ref" href="#blei2009">Blei 2009</a>] has become one of the main tools to
                    analyze text corpora in such a manner. We gain insight into a corpus' contents
                    by identifying semantic classes of words, coined topic, opening up for
                    exploratory search and analysis based on them. An excellent overview of how
                    topic modeling can help humanists in their research is given in [<a class="ref" href="#blei2012">Blei 2012</a>].</div>
                <div class="counter"><a href="#p3">3</a></div><div class="ptext" id="p3">Topic modeling research, however, often focuses on the development of
                    probabilistic models, i.e. incorporating a richer meta-data structure,
                    increasing the speed of inference or using nonparametric models to circumvent
                    model selection problems. Comparatively little effort has been made to develop
                    methods to use the outcome of these models in applications
                    <em class="emph">visually</em>, although recently this task received growing attention
                    (see section 3).</div>
                <div class="counter"><a href="#p4">4</a></div><div class="ptext" id="p4">In this paper, we present a prototypical visual analysis tool to find and display
                    the relations suggested by topic modeling. We derive distinct exploration tasks
                    from the elements of a topic model, and present visual implementations for these
                    tasks to provide the user with interactive means to browse through relations
                    between documents, topics and words. In this way, the user uncovers expected or
                    unexpected facts that eventually lead to interesting documents. More precisely,
                    we represent topics by tag clouds of different size and, by considering
                    pair-wise topic-similarities, we layout these clouds in the plane to provide the
                    user with a topic-centered view on the data. Using smooth level-of-detail
                    transitions and by interacting with topic distribution charts, the user freely
                    navigates through the data by concatenating single exploration tasks – following
                    focus-and-context concepts and an intuitive methodology: overview first, details
                    on demand.</div>
                <div class="counter"><a href="#p5">5</a></div><div class="ptext" id="p5">The rest of the paper is organized as follows: in the next section we briefly
                    discuss the underlying method, topic modeling. We then review related work in
                    section 3 from both the language processing point view and from the direction of
                    presenting topic models (and their alternatives) visually. In section 4, we
                    define elementary exploration tasks applicable to the outcome of topic models,
                    followed by descriptions of their visual implementation in our analysis tool in
                    section 5. We report results from fitting topic models to two different data
                    sets in section 6. We use Stasi records collected from the former East-German
                    Ministry of State Security and the ECCO-TCP<a class="noteRef" href="#d5517e317">[2]</a> data set, a set of
                    classical literature texts, and conclude in section 7.</div>
            </div>

            <div class="div div0">
                <h1 class="head">Topic Models</h1>
                <div class="counter"><a href="#p6">6</a></div><div class="ptext" id="p6">Topic models are a family of algorithms that decompose the content of large
                    collections of documents into a set of topics and then represent each document
                    as a mixture over these topics (based on the document's content). The outcome is
                    thus a list of words for each topic (showing the probability of a term appearing
                    in this topical context) and the proportion of topics for each of the documents.
                    The key ingredients for finding this structure are word co-occurrences, words in
                    a topic tend to co-occur across documents and hence are interpreted to share a
                    common semantic concept (following the assumptions of distributional semantics
                    (e.g., [<a class="ref" href="#desaussure2001">de Saussure 2001</a>]). On a more technical level, a topic
                    model is a Bayesian hierarchical probabilistic (graphical) model. It defines an
                    artificial generative process for document generation, describing how the
                    actually observable data (the words in the documents), get into their place. The
                    most general topic model is Latent Dirichlet Allocation (LDA) introduced by [<a class="ref" href="#blei2003">Blei 2003</a>] which is one probabilistic extension of the well-known
                    Latent Semantic Analysis (LSA) technique (see [<a class="ref" href="#landauer2008">Landauer 2008</a>]. As
                    in LSA, it makes use of the bag-of-words assumption, in which the order of terms
                    in text is neglected and their document-specific frequencies remain the sole
                    basis of analysis. However, instead of factorizing the emerging document-term
                    matrix algebraically, LDA defines a generative process that describes how
                    documents are constructed. Here, latent variables control document generation:
                    (1) the topics (as sets of word proportions), (2) the documents' topic
                    proportions and (3) the probability of a certain term in a specific documents to
                    belong to a specific topic. Using a Bayesian technique, prior distributions are
                    placed on these factors. They interact as follows:</div>
                <div class="ptext"><ol class="list"><li class="item">for all topics <em class="emph">k</em> = 1,...,<em class="emph">K</em>, draw topics
                        <em class="emph">β</em><span class="hi subscript"><em class="emph">k</em></span> ~ Dir<span class="hi subscript"><em class="emph">V</em></span>(<em class="emph">η</em>)</li><li class="item">for all documents
                        <em class="emph">d</em> = 1,...,<em class="emph">D</em><div class="ptext"><ul class="list"><li class="item">draw document <em class="emph">d</em>'s
                    topic proportion <em class="emph">θ</em><span class="hi subscript"><em class="emph">d</em></span> ~ Dir<span class="hi subscript"><em class="emph">K</em></span>(<em class="emph">α</em>)</li><li class="item">for all words n =
                        1,...,<em class="emph">N</em><span class="hi subscript"><em class="emph">d</em></span> in the document<div class="ptext"><ul class="list"><li class="item">draw the topic assignment <em class="emph">z</em><span class="hi subscript"><em class="emph">dn</em></span> ~ Mult(<em class="emph">θ</em><span class="hi subscript"><em class="emph">d</em></span>)</li><li class="item">draw the word <em class="emph">w</em><span class="hi subscript"><em class="emph">dn</em></span> ~ Mult(<em class="emph">β</em><span class="hi subscript"><em class="emph">z</em><span class="hi subscript"><em class="emph">dn</em></span></span>)</li></ul></div></li></ul></div></li></ol></div>
                <div class="counter"><a href="#p7">7</a></div><div class="ptext" id="p7">where <em class="emph">K</em> is the number of topics, <em class="emph">N</em><span class="hi subscript"><em class="emph">d</em></span> are the document lengths, Dir(·) and Mult(·)
                    respectively denote the Dirichlet and multinomial distribution (see [<a class="ref" href="#kotz2000">Kotz 2000</a>]; [<a class="ref" href="#johnson1997">Johnson 1997</a>] and <em class="emph">η</em> and
                        <em class="emph">α</em> are so called hyperparameters (i.e. model parameters) to the
                    Dirichlet distribution. A topic <em class="emph">β</em><span class="hi subscript"><em class="emph">k</em></span> is defined as a probability distribution over the
                    word simplex, i.e., in every topic each word has a certain probability and the
                    probabilities in an individual topic sum to 1. The set of words with highest
                    probability is assumed to be different across different topics and to describe
                    the individual topics thematically. Moreover, the assumption is that only a
                    limited fraction of terms exhibit high probability in each topic. We can ensure
                    this by appropriately setting the topic hyperparameter <em class="emph">η</em>. The
                    document topic proportions <em class="emph">θ</em><span class="hi subscript"><em class="emph">d</em></span> are again probability distributions, defined over the
                    topic simplex, i.e. every topic gets some probability in a document. Each
                    document has its own topic proportions (hence the subscript <em class="emph">d</em>), the
                    probabilities of topics for a single document also summing to 1. Again, we
                    assume that only a small number of topics is active in each document and set the
                    hyperparameter α accordingly. The words <em class="emph">w</em><span class="hi subscript"><em class="emph">dn</em></span> that we see in a document are now generated by
                    first finding a topic <em class="emph">z</em><span class="hi subscript"><em class="emph">dn</em></span>
                    through the document's topic proportions θ<span class="hi subscript"><em class="emph">d</em></span> and then finding a word from the chosen topic β<span class="hi subscript"><em class="emph">z</em><span class="hi subscript"><em class="emph">dn</em></span></span>. Both choices are random draws from their
                    respective multinomial distributions. During inference, we seek to reverse this
                    generative process in order to get approximations for the governing latent
                    factors that best give rise to the observed words, i.e. we want to find the
                    setting of the latent factors for which the observed words are highly likely. We
                    end up with a suitable approximation for these factors that describes the
                    generation of words assuming our generative model would be true. Note that we
                    have skipped the technical details of how this approximation is achieved, the
                    interested reader is referred to [<a class="ref" href="#blei2003">Blei 2003</a>] or [<a class="ref" href="#heinrich2005">Heinrich 2005</a>] for a more thorough technical description.</div>
                <div class="counter"><a href="#p8">8</a></div><div class="ptext" id="p8">Visualizing the results of this model is one solution to unveil knowledge hidden
                    in the data. However, the outcome (i.e. the topics and documents' topic
                    proportions) is obviously inappropriate for direct visualization. Without using
                    thresholds, presenting entire probability distributions as sorted lists of words
                    and values is not very handy and quickly results in information excess and
                    cluttered visualizations. Even working with thresholds does not immediately lead
                    to parameter settings that are independent of the input data, e.g. how many
                    words are actually necessary to obtain a reasonably good impression of a topic
                    found by the model. That is, depending on the semantic quality of words and
                    topics, a flexible level-of-detail is necessary to identify meaningful
                    information in a topic. On the other hand, the amount of information relevant
                    for each element of the topic model is assumed to be rather small. Therefore,
                    the visual implementation of these elements should focus on the pivotal parts of
                    the distributions, while increasingly disregarding irrelevant parts. In the end,
                    the relations between the input documents, the latent topics found by the model
                    and the actual probabilities of a topic's keywords are the key elements
                    containing interesting insights about the data.</div>
                <div class="counter"><a href="#p9">9</a></div><div class="ptext" id="p9">We emphasize that LDA is just one model that subsumes document collection content
                    into topics. There exists a numerous amount of different topic models that can
                    be used alternatively. Besides LDA, others take additional meta-data into
                    account. These include e.g. the Author-Topic model [<a class="ref" href="#rosen-zvi2005">Rosen-Zvi 2005</a>], inferring probability distributions over topics for each author instead of
                    each document, the Hierarchical Dirichlet process topic model [<a class="ref" href="#teh2009">Teh 2009</a>, 887], a nonparametric model that uses Dirichlet
                    Process priors instead of Dirichlet distributions and language models based on
                    probabilistic nonnegative matrix factorization like the LDA interpretation of
                        [<a class="ref" href="#gopalan2013">Gopalan 2013</a>], among others. Our visualization approach takes
                    two matrices as input: a document-topic (in LDA the matrix formed by the
                    individual θ<span class="hi subscript"><em class="emph">d</em></span>s) and a topic-term matrix
                    (formed by the β_<span class="hi subscript"><em class="emph">k</em></span>s). Every model that
                    produces this output (or whose output can be transformed to these structures) is
                    amenable to our visualization. This includes all of the above models (whereas in
                    the Author-Topic model, authors would replace documents conceptually), in fact
                    we could also visualize the outcome of the LSA model which follows completely
                    different approach as topic modeling.</div>
                <div class="counter"><a href="#p10">10</a></div><div class="ptext" id="p10">We also note that we do not go into the analysis of the models themselves but
                    rather restrict our discussion to the outcome that they produce. Assessing the
                    quality of the models' results is a research field on its own (e.g.,[<a class="ref" href="#boyd-graber2009">Boyd-Graber 2009</a>]; [<a class="ref" href="#mimno2011">Mimno 2011</a>]) and we do not make
                    any assumptions on its quality. [<a class="ref" href="#gelman2013">Gelman 2013</a>] argue that for
                    Bayesian methods in general, outcome must undergo thorough inspection and
                    interpretation by domain experts.</div>
            </div>

            <div class="div div0">
                <h1 class="head">Related Work</h1>
                <div class="counter"><a href="#p11">11</a></div><div class="ptext" id="p11">Traditional linguistic approaches such as the vector space model [<a class="ref" href="#salton1988">Salton 1988</a>] translate documents into high-dimensional feature
                    vectors (typically in combination with, e.g., the tf-idf [<a class="ref" href="#sparckjones1972">Sparck Jones 1972</a>] term weighting). Visualizing such
                    high-dimensional structures is a research field on its own and several different
                    methods have been proposed in the literature. Themeriver [<a class="ref" href="#havre2000">Havre 2000</a>] is a chart-based flow illustration that focuses on the
                    change of themes over time based on word frequencies. The visualization has a
                    strong focus on the themes and no support of relations between topics,
                    documents, and words. We currently ignore time information and consider the data
                    set as static although there are plans to enhance our tools to be able to follow
                    topic evolution through time (cf. section 7).</div>
                <div class="counter"><a href="#p12">12</a></div><div class="ptext" id="p12">Closely related to our approach is that of [<a class="ref" href="#chaney2012">Chaney 2012</a>], an
                    attempt to directly visualize the output of topic models. The authors describe
                    the main functionalities of such a system that are quite naturally similar to
                    the definition of our exploration tasks in section 4. However, their approach
                    includes generating a set of static websites that can be browsed to explore the
                    data set, providing a lightweight, largely text-based application to solve the
                    tasks. On the other hand, this method lacks user interaction and also is rather
                    document-centered. There exists an overview over the different topics, but each
                    topic is described by the three most probable words only. There is no
                    possibility to further investigate a topic <em class="emph">and</em> to keep track of the
                    others. Numerous solutions extending this concept exist, e.g. [<a class="ref" href="#snyder2013">Snyder 2013</a>] or [<a class="ref" href="#hinneburg2012">Hinneburg 2012</a>], enriching or
                    refining the resulting presentation with different kinds of metadata. [<a class="ref" href="#cao2010">Cao 2010</a>] propose a visualization technique for entities extracted
                    from texts which they call FaceAtlas; a graph-based network visualization
                    augmented with density maps to visually analyze text corpora with documents
                    having relations based on different facets. This approach is similar to ours in
                    that semantic similarity of entities determines spatial distance in the
                    visualization. However, the method of how we arrive at our data model
                    considerably differs. They use Named Entity Recognition (NER) to extract named
                    entities from texts and visualize their relations whereas we extract latent
                    structures (the topics) from the text that define distributions over the
                    vocabulary. Relations between them are implicitly defined by measuring
                    similarity of those distributions with suitable metrics (see e.g., [<a class="ref" href="#alsumait2009">AlSumait 2009</a>]; [<a class="ref" href="#niekler2012">Niekler 2012</a>]). TopicNets [<a class="ref" href="#gretarsson2012">Gretarsson 2012</a>] is a graph-based, interactive analysis tool that
                    incorporates topic models into the mechanics of graph visualization and
                    facilitates the collapsing of nodes based on semantic association, topic-based
                    deformation of node sets, or real-time topic modeling on graph subsets at
                    various levels. Topic Islands~[<a class="ref" href="#miller1998">Miller 1998</a>] uses stereoscopic
                    depictions of topics using wavelets to describe thematic characteristics.
                    ThemeScapes [<a class="ref" href="#wise1995">Wise 1995</a>] uses a terrain-like landscape metaphor to
                    illustrate topics as hills with documents on top. Less complex linguistic
                    approaches translate documents into high-dimensional feature vectors using the
                    vector space model [<a class="ref" href="#salton1988">Salton 1988</a>] in combination with, e.g., the
                    tf-idf [<a class="ref" href="#sparckjones1972">Sparck Jones 1972</a>] term weighting. In this space, words
                    serve as dimensions and documents are finally represented as a point cloud; with
                    (sub-)clusters of documents for each (sub-)topic. Finding and visualizing this
                    high-dimensional structure is a research field on its own. Established
                    approaches include projective techniques, like the Text Map Explorer [<a class="ref" href="#paulovich2006">Paulovich 2006</a>] Multidimensional Scaling (MDS) [<a class="ref" href="#kruskal2009">Kruskal 2009</a>], e.g. Sammon's mapping [<a class="ref" href="#sammon1969">Sammon 1969</a>];
                    neuro-computational algorithms like Kohonen's Self Organizing Maps (SOM) [<a class="ref" href="#kohonen2001">Kohonen 2001</a>], scatterplot matrices [<a class="ref" href="#elmqvist2008">Elmqvist 2008</a>]
                    or topological analysis based on density functions [<a class="ref" href="#oesterling2010">Oesterling 2010</a>]. However, compared to the modeling approach used in this paper, the insights
                    obtained from the vector space model is rather limited. Our work differs in that
                    we focus on the visual representation of topic model elements to provide more
                    thorough and quicker visual access to the data. We use a more flexible depiction
                    of probability distributions as tag clouds and illustrate topics in an overview
                    image to permit the identification of related topic groups or outliers.
                    Furthermore, we extend the analysis to word-based tasks like finding polysemous
                    and homonymic relations, we use smooth level-of-detail instead of using a fixed
                    number of keywords, and we allow the user to quantify the relative impact of
                    related topics or documents. We also note that there exist a wide variety of
                    different topic models. For example models that impose a network structure on
                    document during model design provide the possibility to interpret the links
                    between documents on a semantic level ([<a class="ref" href="#chang2010">Chang 2010</a>]; [<a class="ref" href="#mei2008">Mei 2008</a>]). However, we want to keep our tool applicable to the
                    widest range of data possible and thus neglect models that make use of other
                    meta data then word frequencies. Further, we do not aim at visualizing links
                    between documents but rather links between topics (which are given by
                    distributional similarity, see below).</div>
            </div>

            <div class="div div0">
                <h1 class="head">Exploration Tasks</h1>
                <div class="counter"><a href="#p13">13</a></div><div class="ptext" id="p13">Using the aforementioned outcome of topic models, we aim to provide the user with
                    exploratory means to analyze a corpus by creating a largely topic-centered view
                    on the data and letting latent topics act as the user's main interface to the
                    documents. In this section, we recall the elements of a topic model and classify
                    them into exploration tasks to relate topics to words and documents, and vice
                    versa. The analysis process then consists of concatenating elementary
                    exploration tasks. That is, motivated by intermediate insights about expected or
                    unexpected relationships, the user interactively browses through the data via
                    linked tasks.</div>

                <div class="div div1">
                    <h2 class="head">Definition of Exploration Tasks</h2>
                    <div class="counter"><a href="#p14">14</a></div><div class="ptext" id="p14">The probability distributions resulting from a topic model relate the whole
                        vocabulary to latent topics, and the latter to all documents. That is, the
                        outcome of this model is very complex in that all words occur in every
                        topic, and all topics appear in every document – both with certain
                        "significance" (in fact probability). Because such complex data is hard to
                        handle as a whole, we split the analysis process into distinct exploration
                        tasks to reveal possible relations between single conceptual entities, e.g.
                        documents, and one or more topics, between topics related to a single
                        documents, or between topics related to single words. Based on a simple
                        input-output scheme, every task requires certain information produced by the
                        topic model or provided by the input data and it discloses potentially
                        existent relationships between them.</div>

                    <div class="div div2">
                        <h3 class="head">Exploration Task 1 - Examining a Topic</h3>
                        <div class="counter"><a href="#p15">15</a></div><div class="ptext" id="p15">Examining a single topic is difficult because it is a probability
                            distribution over potentially thousands of words in the vocabulary.
                            Technically, this task involves the following information: the topic's
                            overall significance in the corpus, a meaningful sorting of the words
                            for appropriate topic description, and actual word significances to
                            provide pivotal keywords and their relative importance. The quantity of
                            a topic's overall significance can easily be computed as a relative
                            measure using the model outcome of topic models: topic-significance<span class="hi subscript"><em class="emph">k</em></span> = (∑<span class="hi subscript"><em class="emph">d</em></span> θ<span class="hi subscript"><em class="emph">dk</em></span>
                            · <em class="emph">N</em><span class="hi subscript"><em class="emph">d</em></span>)/(∑<span class="hi subscript"><em class="emph">d</em></span><em class="emph">N</em><span class="hi subscript"><em class="emph">d</em></span>), where θ<span class="hi subscript"><em class="emph">dk</em></span> is the <em class="emph">d</em>-th document's topic
                            proportion of topic <em class="emph">k</em>, satisfying θ<span class="hi subscript"><em class="emph">dk</em></span> ≤ 0, <em class="emph">k</em>=1,...,<em class="emph">K</em>
                            and ∑<span class="hi subscript"><em class="emph">k</em></span> θ<span class="hi subscript"><em class="emph">dk</em></span> = 1. <em class="emph">N</em><span class="hi subscript"><em class="emph">d</em></span> is the length of document $d$. Relevance
                            determination of a topic's words involves finding a suitable sorting
                            because both tasks are based on the word probabilities provided by the
                            topic model. Since we can assume that the largest part of the vocabulary
                            does not carry topic-specific information, it is reasonable to sort the
                            words by decreasing probability and increasingly disregard their
                            relevance for that topic. Another approach involves determining the
                            words' relevances using a tf-idf [<a class="ref" href="#sparckjones1972">Sparck Jones 1972</a>]
                            flavored procedure; each topic is interpreted as a document and word
                            probabilities are treated as scaled document frequencies. Using a basic
                            tf-idf scheme, we could easily identify words that are highly
                            descriptive exclusively to their respective topics. In the
                            visualization, this information is used to help the user to quickly
                            identify the key words and their relative importance for a topic.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">Exploration Task 2 - Overview over the Topics</h3>
                        <div class="counter"><a href="#p16">16</a></div><div class="ptext" id="p16">The second exploration task is to summarize the set of latent topics
                            found by the topic model. This includes the following information: the
                            number of topics, their overall significance for (or impact on) the
                            corpus, and similarities between topics defined by some measure. While
                            the overall significance is equal to that in Exploration Task 1, for
                            topic similarity, different metrics are possible. One score that is
                            motivated by similar topic distributions was described by [<a class="ref" href="#chaney2012">Chaney 2012</a>]. Another approach is to understand topics as
                            vectors in a high-dimensional space where words of the vocabulary serve
                            as dimensions. Then topic similarities are described by proximities in
                            the vector distribution on the surface of a unit d-simplex. Possible
                            metrics are the Jensen-Shannon divergence [<a class="ref" href="#manning1999">Manning 1999</a>]
                            or distance-based measures like, e.g., simple Euclidean distance. Using
                            any appropriate metric, topic (dis)similarity shall be described by
                            spatial proximity in the overview.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">Exploration Task 3 - Finding Different Polysemous and Homonymic
                            Semantics of Terms</h3>
                        <div class="counter"><a href="#p17">17</a></div><div class="ptext" id="p17">One advantage of topic models is the automatic disambiguation of semantic
                            meanings of words into topics. A word with different meanings<a class="noteRef" href="#d5517e706">[3]</a>
                            automatically appears in different topics that correspond to its
                            different semantic contexts. Note that each word has some probability in
                            every topic. Instead of introducing a significance threshold, i.e. a
                            hard threshold in probability, we resort to the idea that if the
                            significance in another topic falls below a certain level that is not
                            visually presentable anymore, the relation is of no importance. This
                            corresponds to defining an implicit threshold in probability through
                            visual limitations. To highlight the occurrence of polysemous and
                            homonymic terms including their relative importance in other topics,
                            this task consists of the following requirements: selecting a word of
                            interest, the quantification of relevant topics that share this word
                            with "sufficient" probability, and the relevance of the selected word in
                            these topics to evaluate semantic diversity. In the visualization, the
                            user should be able to quickly deduce potential semantical ambiguity by
                            selecting a word in any topic and seeing the impact of this word in
                            other topics provided by the topic model.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">Exploration Task 4 - Identifying Documents Covering a Topic</h3>
                        <div class="counter"><a href="#p18">18</a></div><div class="ptext" id="p18">Having identified one or more interesting topics 𝒦 = {<em class="emph">k</em> :
                                <em class="emph">k</em>∈{1,...,<em class="emph">K</em>}}, the user may want to look at
                            documents that cover these topics. This task is at the core of
                            exploratory analysis. The information required for this are the topics
                            of interest <em class="emph">k</em><span class="hi subscript"><em class="emph">i</em></span> and a
                            list of documents sorted in decreasing order by the combined impact of
                            topics 𝒦 on the documents. Given 𝒦, we can easily read off the
                            probability of these topics in all of the documents θ<span class="hi subscript">·<em class="emph">k</em></span>, <em class="emph">k</em> ∈ 𝒦. After
                            sorting the documents by their proportions of the impact product of all
                            topics 𝒦 (we use Π<span class="hi subscript">k∈ 𝒦<em class="emph">k</em></span>
                            <em class="emph">p</em>(θ<span class="hi subscript"><em class="emph">dk̂</em></span>|k̂ = k) to
                            approximate the combined impact of 𝒦 on each document <em class="emph">d</em>),
                            we obtain a list of documents that exhibit the topics of interest with
                            decreasing significance.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">Exploration Task 5 - Finding Related Topics of a Document</h3>
                        <div class="counter"><a href="#p19">19</a></div><div class="ptext" id="p19">Once an interesting document has been identified, the user may want to
                            inspect other topics related to it, or, in a transitive way, documents
                            related to these other topics. Again, this task aims at giving the user
                            a tool for exploring related documents (and thus the corpus) through
                            picking interesting topics. The following information are involved in
                            this task: a document of interest, the proportions of other topics in
                            this document, and document-related documents. While the related topics
                            of a document simply result from the topic model, the latter information
                            could be obtained from considering the similarity between topic
                            distributions between two documents (an example metric is given in [<a class="ref" href="#chaney2012">Chaney 2012</a>]). Because we focus on a topic-centered
                            navigation through the data, we drop document-document relationships and
                            primarily focus on a document's topic distribution.</div>
                    </div>
                </div>
            </div>

            <div class="div div0">
                <h1 class="head">Visualization Approach</h1>
                <div class="counter"><a href="#p20">20</a></div><div class="ptext" id="p20">In this section, we explain our analysis tool and provide visual implementations
                    for each exploration task defined in section 4. Furthermore, we describe
                    interactive means to navigate through the data by letting the user concatenate
                    individual tasks, stimulated by the feedback of previous insights and following
                    an intuitive analysis methodology: overview first, details on demand.</div>

                <div class="div div1">
                    <h2 class="head">Visual Implementation of Exploration Tasks</h2>
                    <div class="counter"><a href="#p21">21</a></div><div class="ptext" id="p21">The visualization of a topic model should provide quick visual access to the
                        key features and relations in the data. This task is difficult because the
                        broad and complex probability distributions produced by a topic model
                        contain large amounts of irrelevant information. That is, only a minor part
                        of the vocabulary is meaningful to describe a topic, and only some topics
                        have considerable impact on a certain document. Hence, visualizing the topic
                        model as a whole rapidly creates cluttered visualizations. We pursue a
                        visualization approach that illustrates the crucial information of
                        individual exploration tasks, but that also allows the user to refine the
                        level-of-detail in an intuitive and interactive way.</div>
                </div>

                <div class="div div1">
                    <h2 class="head">Visual Implementation of Exploration Task 1 - Examining a Topic</h2>
                    <div class="counter"><a href="#p22">22</a></div><div class="ptext" id="p22">To visualize a single topic we make use of <em class="emph">tag clouds</em>
                        [<a class="ref" href="#steele2010">Steele 2010</a>], a popular visual metaphor for weighted
                        word-lists. Although tag cloud implementations can be highly sophisticated,
                        we keep it simple and only focus on the information required for the
                        exploration task. Taking the sorted words, we create labels with size and
                        opacity proportional to the words' probabilities and arrange them in a
                        spiral layout around the most significant word. That is, for each word, we
                        start a spiral from a center point until sufficient space is found to place
                        this word. As a consequence, small and increasingly transparent irrelevant
                        words are positioned at the cloud's border or in the gaps between relevant
                        words. The user can smoothly change the level-of-detail by zooming in and
                        out to make small words appear and to adjust a word's readability (size and
                        opacity) proportional to the zoom-factor. The minimum level-of-detail shows
                        at least the top keyword per topic at full opacity. Furthermore, the tag
                        cloud's extent is scaled by the topic's overall significance in the corpus
                        and each cloud is assigned a distinguishable color to ease further analysis.
                        Example topic clouds are shown in Figure 2.</div>
                </div>
                <div class="div div1">
                    <h2 class="head">Visual Implementation of Exploration Task 2 - Overview over the
                        Topics</h2>
                    <div class="counter"><a href="#p23">23</a></div><div class="ptext" id="p23">The topic overview visualization is the main view on the data and the
                        starting point of any other exploration task. To visualize the required
                        information for this task, we layout tag clouds in the plane to present them
                        on the screen. Their number reflects the number of latent topics found by
                        the topic model and their pair-wise distances in the layout approximate
                        their similarities; understood either as the difference between probability
                        distributions or distances in the high-dimensional <em class="emph">topic space</em>
                        on the surface of the unit d-simplex. There are plenty of algorithms to
                        create a layout of the clouds that reflects pair-wise (dis)similarities as
                        distances in the plane. For the sake of simplicity, we use either a
                        force-directed approach or Sammon's mapping. Although the probability
                        distributions of the topics are assumed to be sufficiently diverse to
                        minimize cloud occlusions, the user can also scale all pair-wise cloud
                        distances to disperse accumulations. In the overview, the user can quickly
                        distinguish cloud sizes and identify related topics as nearby clouds or
                        cloud accumulation.</div>
                </div>

                <div class="div div1">
                    <h2 class="head">Visual Implementation of Exploration Task 3 - Finding Different Polysemous
                        and Homonymic Terms</h2>
                    <div class="counter"><a href="#p24">24</a></div><div class="ptext" id="p24">To identify polysemous terms and homonyms, the selected word of interest
                        (selection mechanisms will be explained in section 5.2) is highlighted in
                        every topic cloud which provides quick access to this word's significances
                        in other topics via their labels' sizes. Moreover, clouds corresponding to
                        topics in which the selected word is considered insignificant are decolored,
                        i.e. bleached out to facilitate focusing only on those topics with relevant
                        word probability. Because a word's size in another topic could be marginal
                        relative to all other label sizes, or because the current zoom-level is not
                        high enough to identify the word of interest in every cloud, we also provide
                        a chart in a head-up display (HUD) to denote the proportions of this word's
                        probability in each topic. Every part of the chart is colored according to
                        the topic it represents. By inspecting the highlighted labels' size or their
                        corresponding parts in the chart, the user can quickly judge the diversity
                        and partial quantities of a word's different meanings. The topic chart is
                        also a starting point for topic-related exploration tasks (cf. section
                        5.2).</div>
                </div>

                <div class="div div1">
                    <h2 class="head">Visual Implementation of Exploration Task 4 - Documents Covering a
                        Topic</h2>
                    <div class="counter"><a href="#p25">25</a></div><div class="ptext" id="p25">Given one or more selected topics of interest, the sorted list of documents
                        covering these topics is presented in the head-up display. Each of the
                        scrollable list's entries shows the document's name. The list is also a
                        starting point for document-related exploration tasks (cf. section 5.2).</div>
                </div>

                <div class="div div1">
                    <h2 class="head">Visual Implementation of Exploration Task 5 - Finding Related Topics of a
                        Document</h2>
                    <div class="counter"><a href="#p26">26</a></div><div class="ptext" id="p26">Each of the document list's entries additionally shows a small chart of the
                        document's overall topic distribution. From these charts, the user can
                        directly read off and compare the impact of the selected topics on every
                        document; also in contrast to all other topics. Selecting a document
                        activates a magnified version of the topic chart to the right of the list in
                        the HUD and serves as a source for topic-related exploration tasks, like
                        examining its words or updating the document list. </div>

                    <div id="figure01" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 1. </div>Overview over our analysis tool. The visual context of the topic
                            space is always preserved in the background, moving the camera if necessary
                            to center selected topics currently not visible on the screen. Widgets in
                            the head-up display (HUD) summarize relations between user-selected
                            elements. In this example scenario, the user first selects the word
                            "newcastle" in the top green cloud. By doing so, this word gets highlighted
                            (red) in any other cloud and the probabilities of this word in other topics
                            are additionally shown in a pie chart (bottom left). Clouds with
                            insignificant parts in this chart are decolored. Selecting the other topics
                            in which "newcastle" appears (black border) we can create the document list
                            (sorted by the impact of these topics) in the middle of the HUD. Selecting a
                            single document (red line) creates a chart (bottom right) with this
                            document's topic probabilities, revealing other significant topics (colored)
                            in the topic overview that appear in the document. </div></div>
    
                </div>
                <div class="div div1">
                    <h2 class="head">User Interaction Mechanisms</h2>
                    <div class="counter"><a href="#p27">27</a></div><div class="ptext" id="p27">The user browses interactively through the data by concatenating exploration
                        tasks. The visual implementations of these tasks can be linked in order to
                        launch subsequent tasks based on intermediate insights about the data (cf.
                        Figure 1 for the different visualization components).</div>

                    <div class="div div2">
                        <h3 class="head">Selecting a Topic</h3>
                        <div class="counter"><a href="#p28">28</a></div><div class="ptext" id="p28">Topics can be selected in two ways: by right-clicking one or more clouds
                            in the overview visualization, or by selecting the corresponding part of
                            a topic chart (see the next to selection mechanisms). A selection of one
                            or more topics triggers the following actions: an accentuation of the
                            corresponding clouds with an additional border to highlight selected
                            topics, and an update of the document list in the HUD to present those
                            documents that share the selected topics, sorted by decreasing combined
                            impact. In addition, if a topic is currently not visible in the cloud
                            overview, a camera movement centers and magnifies it on the screen so
                            that representative words can be read off to get a quick understanding
                            of the topic. Note that this camera movement is actually also part of
                            exploration task 1 because being able to read words associated to a
                            topic is an inherent part of examining it.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">Selecting a Word</h3>
                        <div class="counter"><a href="#p29">29</a></div><div class="ptext" id="p29">As part of exploration task 3, words are selected by left-clicking them
                            in any of the topic clouds. A word selection triggers three actions: the
                            word's accentuation in every other cloud, the decoloring of those clouds
                            that feature only insignificant probabilities for this word, and the
                            creation of a pie chart showing an aggregation of the word's
                            significance in different topics in the HUD, creating starting point for
                            exploration task~4 or exploration task 1.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">Selecting a Document</h3>
                        <div class="counter"><a href="#p30">30</a></div><div class="ptext" id="p30">Documents are selected by clicking on them in the HUD's document list. A
                            document selection triggers the creation of the topic distribution chart
                            for this document, which is placed to the right of the document and is
                            used to trigger the topic-based exploration tasks 1 and 4. Further,
                            topics not relevant for this document are decolored in the cloud
                            overview to quickly identify those that are.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">Cyclic Analysis Process and Visual Context of Topics</h3>
                        <div class="counter"><a href="#p31">31</a></div><div class="ptext" id="p31">Using these interaction mechanisms, the analysis process is carried out
                            by combining exploration tasks in a transitive or cyclic way. That is,
                            the selection of topics, words, or documents highlights other visual
                            entities and updates widgets which triggers the next exploration task.
                            For example, clicking on a word in one of the clouds (task 1 and 2)
                            creates a topic chart (task 3) in which click-events create the document
                            list for certain selected topics (task 4). Clicking on a document
                            creates a chart for related topics (task 5) whose selection centers a
                            topic cloud (task 1) and updates the document list (task 4)-and so
                            on.</div>
                        <div class="counter"><a href="#p32">32</a></div><div class="ptext" id="p32">Note that camera movements related to topic selections constantly
                            preserve the visual context in the topic space. That is, for selected
                            topics, the user can always read off keywords to evaluate their meaning
                            and importance and related topics and their overall significance can be
                            identified by examining nearby clouds. By bleaching out topics that do
                            not significantly contribute to one of the topic charts, the user can
                            quickly identify the spatial relation between selected (colored) topics
                            in the overview. This can help to reveal interesting words appearing in
                            deselected topics. Note that we understand our framework as a tool to
                            navigate through the data based on relations between topics and both
                            words and documents. Once interesting documents are identified, their
                            content is presented to the user in a linked view or in the head-up
                            display.</div>
                    </div>
                </div>
            </div>
            <div class="div div0">
                <h1 class="head">Experiments</h1>
                <div class="div div1">
                    <h2 class="head">Data Sets</h2>
                    <div class="counter"><a href="#p33">33</a></div><div class="ptext" id="p33">We report use cases of fitting topic models to two different data sets. The
                        first is the series of publications "The GDR through the Eyes of the Stasi.
                        The Secret Reports to the SED Leadership" (German: "<span class="foreign i">Die DDR im Blick der
                        Stasi. Die geheimen Berichte an die SED-Führung</span>") [<a class="ref" href="#muenkel2014">Münkel 2014</a>] that reveal the Stasi's<a class="noteRef" href="#d5517e932">[4]</a> specific view of the
                        GDR, containing references to real and perceived oppositional conduct as
                        well as to economic and supply problems. The second data set is the
                            ECCO-TCP<a class="noteRef" href="#d5517e939">[5]</a>, a set of
                        classical literature and non-fiction texts. We preprocessed the raw text and
                        performed a set of standard preprocessing steps (including stop word
                        removal, minimum frequency pruning and lower casing). In the following we
                        walk through the exploration tasks as described in section 4 and provide
                        screen shots of the respective visual implementations, including how the
                        spatial proximity of topics helps to identify semantic similarities between
                        them. We fitted non-parametric topic models [<a class="ref" href="#teh2009">Teh 2009</a>, 887] on each of the two data sets using the Gibbs sampling approach described
                        there. Our visualization prototype is based on the OpenWalnut visualization
                        engine<a class="noteRef" href="#d5517e947">[6]</a> and is implemented as a
                        plugin for this framework. Our current workflow is pretty rudimentary: given
                        a collection of documents, we learn a topic model. We then take the
                        resulting document-topic and topic-term probability matrices and feed them
                        into our prototype. OpenWalnut runs on a wide variety of platforms, we have
                        used a ready-to-use Linux distribution<a class="noteRef" href="#d5517e951">[7]</a>
                        providing installation packages for OpenWalnut and added our plugin.
                        Experiments could smoothly be carried out using
                        virtualBox<a class="noteRef" href="#d5517e955">[8]</a> on an Apple MacBook
                        Air laptop.</div>
                </div>

                <div class="div div1">
                    <h2 class="head">Examining a topic</h2>
                    <div class="counter"><a href="#p34">34</a></div><div class="ptext" id="p34">Figures 2 and 3 show two topics extracted from the data sources. Words' sizes are
                        determined by their probability in the topic's distribution over the
                        vocabulary. The topics can easily be identified to circle around the
                        literary genre of drama and communist-party propaganda concerning the youth
                        respectively. As the user zooms in, more words become visible that were
                        hidden because of lesser relevance to the topic, uncovering them reveals a
                        semantic refinement of the topic. This shows that not only the most
                        significant terms define a topic, they merely allude to the topic's semantic
                        meaning that is subsequently defined by the other words with considerable
                        probability mass in it.</div>
                    
                    
                    <div id="figure02a" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure02a.png" rel="external"><img src="resources/images/figure02a.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 2. </div>A topic covering several classical drama authors and their work from the
                            ECCO-TCP data set. </div></div>
                    
                    <div id="figure02b" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure02b.png" rel="external"><img src="resources/images/figure02b.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 3. </div>A topic covering communist party propaganda in
                            connection with the youth from the Stasi files data set.</div></div>
                    
                </div>

                <div class="div div1">
                    <h2 class="head">Overview of the Topics</h2>
                    <div class="counter"><a href="#p35">35</a></div><div class="ptext" id="p35">Figures 4 and 5 show an overview over the topics found. As described above, the
                        size of the tag clouds represents the overall topic-significance in the
                        whole data set and spatial proximity indicates closer semantic relatedness
                        of topics. One example (cf. Figure 4) is the cluster of topics that cover
                        different aspects of religion and its role in colonization and history. To
                        determine the differences between them the user can follow the methodology
                        for examining a topic. The objective of this view is to provide an initial
                        starting point for further analysis and to draw the user's attention to
                        interesting parts of the data. Figure 5 shows a cluster of topics that are
                        concerned with taking appropriate measure towards different problems in
                        economy and society in GDR. The user gains a first insight into the corpus
                        and is motivated to continue her exploration of the data by concatenating
                        further tasks.</div>
                    
                    <div id="figure03a" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure03a.png" rel="external"><img src="resources/images/figure03a.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 4. </div>Example topics from the ECCO-TCP data set.</div></div>
                    
                    <div id="figure03b" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure03b.png" rel="external"><img src="resources/images/figure03b.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 5. </div>Examples from the Stasi files
                            data set.</div></div>
                    
                </div>

                <div class="div div1">
                    <h2 class="head">Finding different semantics</h2>
                    <div class="counter"><a href="#p36">36</a></div><div class="ptext" id="p36">To disambiguate the semantics of a given term, the user selects the word of
                        interest leading to other topics that exhibit the word with sufficient
                        significance being highlighted. In Figure 6 the selected word in the
                        visualization of the ECCO-TCP corpus is "greeks". Other topics that include
                        this term with sufficient probability are the topics about medicinal
                        findings about organs, Jesus and religion, and drama. Clearly, "greeks"
                        appears in different semantical contexts here: it relates to the Greek
                        political system and the ancient Greek society in that it had a modern
                        understanding of medicine (for their time) and stands for famous Greek drama
                        authors (most prominently Homer). Figure 7 shows the different semantics
                        of the word "untersuchungen" (investigations) from the Stasi corpus. The
                        term is used in connection with the ever suspicious Stasi (topics dealing
                        with addresses, name, Berlin, "<span class="foreign i">wohnhaft</span>" (lives at) etc. but also with
                        investigations of accidents in factories ("<span class="foreign i">veb</span>" meaning Peoples-owned
                        enterprise) and the state railroad ("<span class="foreign i">reichsbahn</span>").</div>
                    
                    
                    <div id="figure04a" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure04a.png" rel="external"><img src="resources/images/figure04a.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 6. </div>Different usages
                            of the term "greeks" in the ECCO-TCP data set. Usages in connection with
                            religion, literature, science and politics are apparent. </div></div>
                    
                    <div id="figure04b" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure04b.png" rel="external"><img src="resources/images/figure04b.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 7. </div>Usages of the
                            term "<span class="foreign i">untersuchungen</span>" (Engl. investigations) from the Stasi files data set.</div></div>                  
                    
                </div>

                <div class="div div1">
                    <h2 class="head">Identifying documents covering a topic</h2>
                    <div class="counter"><a href="#p37">37</a></div><div class="ptext" id="p37">Akin to example 7, assume that the user is interested in documents that
                        include the topic described by terms like "indian", "cape", "spaniards",
                        "china" or "anchor". Selecting the topic creates a list of documents that
                        cover it (in this case travel narratives and reports from the English
                        colonies) in the middle of the HUD as shown in Figure 8. The user can
                        scroll through this list and finish one goal of exploratory analysis. She
                        has identified a topic of interest, examined it to identify its semantic
                        nature and found a list of documents that cover it. Figure 9 solves this
                        task on the Stasi data set. The selected topic is about problems in medical
                        care in former GDR (indicated by the terms "<span class="foreign i">medizinischen</span>" (medical),
                        "<span class="foreign i">versorgung</span>" (care) and "<span class="foreign i">probleme</span>" (problems). The resulting list includes
                        mainly statistical reports but also documents about the unions' evaluation
                        to the problem and even about the planned departure of a physician. This
                        allows for a content driven access to the data. It is also possible to
                        combine different topics so that we can display documents that cover a
                        combination of topics (see Figure 1 for one such example).</div>
                    
                    <div id="figure05a" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure05a.png" rel="external"><img src="resources/images/figure05a.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 8. </div>Documents
                            from the ECCO-TCP data set that are related to the topic about "indians",
                            "cape", "anchor", "spaniards" etc., the documents are mainly travel
                            narratives and reports from English colonies. </div></div>
                    
                    <div id="figure05b" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure05b.png" rel="external"><img src="resources/images/figure05b.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 9. </div>Documents from the Stasi
                            data set related to the topic about problems in medical care in former GDR.
                            The documents are mainly statistical reports, one about the unions'
                            evaluation to the problem and the planned departure of a physician. </div></div>                             
          
                </div>

                <div class="div div1">
                    <h2 class="head">Finding related topics of a document</h2>
                    <div class="counter"><a href="#p38">38</a></div><div class="ptext" id="p38">While exploring the data set and reading documents that cover a topic of
                        interest, it is often the case that this topic is not the only one covered
                        by a document. By displaying a chart of the portions of other topics covered
                        by this document as in Figure 10 the user is encouraged to continue her
                        exploration by examining these other topics. In our example the user first
                        selected a topic about anatomy (with terms like "organs", "fluids",
                        "sensations", "uterus" etc.) and then a document covering this topic. We
                        find a connection to another topic about religion ("jesus", "gospel",
                            "saviour" etc.). Indeed the selected document's title fully reads "The
                        analyst: or, a discourse addressed to an infidel mathematician. Wherein it
                        is examined whether the object, ... and inferences of the modern analysis
                        are more distinctly conceived, or more evidently deduced, than religious
                        mysteries ... By author of The minute philosopher". (In this case the
                        connection is of course not that surprising given the title.)</div>
                    
                    <div id="figure06" class="figure">
                        
                        <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" alt=""/></a></div>
                    <div class="caption"><div class="label">Figure 10. </div>Solving task 5, finding topics related to a document. The selected
                            document has been found by first selecting the anatomy topic (experiments,
                            velocity, uid, organs, magnitude etc.). A second topic contained by this
                            document has typical words like "Jesus", "savior", "spiritual", "salvation"
                            etc. as would be expected considering the document's title. </div></div>  
                </div>
            </div>

            <div class="div div0">
                <h1 class="head">Discussion and Future Work</h1>
                <div class="counter"><a href="#p39">39</a></div><div class="ptext" id="p39">In this paper we have described a visual tool using a tag clouds based approach
                    to visualize the outcome of topic models. Showing series of word probabilities
                    as tag clouds easily provides quick visual access to a topic's meaningful
                    keywords including their significance and qualitative difference to other words
                    in the topic. That is, compared to a simple list of sorted words, the user can
                    quickly judge topical distinctness by the ratio between words of high and low
                    probability, and pivotal keywords also stand out visually in the clouds.
                    Furthermore, by zooming in and out to change the level-of-detail, the user can
                    quickly adjust a topic's expressiveness in terms of its keywords; while still
                    minimizing unnecessary information by keeping the remaining words small and
                    translucent. We also advanced [<a class="ref" href="#chaney2012">Chaney 2012</a>]'s topic model
                    visualization by introducing additional word-based exploration tasks, providing
                    the user with additional information (absolute values and proportions) to rank
                    related topics and documents, and by the usage of a topic clouds layout to
                    identify related topic groups or outliers easily in a global overview. Camera
                    movements in the cloud space triggered by topic selections also preserve the
                    context of the topic-centered analysis process.</div>
                <div class="counter"><a href="#p40">40</a></div><div class="ptext" id="p40">We understand our tool as a topic-centered navigator to visually disclose and
                    present structure hidden in the outcome of the topic model. That is, reading
                    documents and other document-related exploration tasks are currently not
                    considered in our tool. We leave the investigation of these tasks and their
                    visual implementations, and also the expansion of the visual analysis to
                    time-dependent data for future work. We also omitted further possible
                    improvements in the preprocessing of data, i.e. before learning a topic model.
                    These may include stemming, lemmatization and restricting the vocabulary by
                    confining to certain parts-of-speech. Also, as of now, we are not able to export
                    findings from our approach and are restricted to identifying document titles.
                    However, plans to incorporate our visualization into a larger NLP-toolbox (the
                    Leipzig Corpus
                    Miner<a class="noteRef" href="#d5517e1194">[9]</a>
                    currently under development) would readily solve both these shortcomings.
                    Ultimately, we hope to use the application to restrict an existing corpus to the
                    set of documents the user is interested in, enabling her to perform further
                    analysis steps on the now semantically localized subcorpus.</div>
            </div>
        
        
            

        
    </div>
<div id="notes"><h2>Notes</h2><div class="endnote" id="d5517e27"><span class="noteRef">[1]</span>At the time of writing, the author was affiliated with the Natural Language Processing Group, Leipzig University,
                        Germany</div><div class="endnote" id="d5517e317"><span class="noteRef">[2]</span>The Eighteenth Century
                        Collection Online Text Creation Partnership.</div><div class="endnote" id="d5517e706"><span class="noteRef">[3]</span>A
                                polysemous word has multiple related meanings, e.g. bank as the
                                financial institution and as the building in which the institution
                                resides. A homonym has different meanings that are unrelated, e.g.
                                bank as the financial institution and the river bank.</div><div class="endnote" id="d5517e932"><span class="noteRef">[4]</span>Literally, Stasi abbreviates
                            "<span class="foreign i">STAatsSIcherheit</span>", i.e. State Security.</div><div class="endnote" id="d5517e939"><span class="noteRef">[5]</span>see
                                <a class="ref" href="http://www.textcreationpartnership.org/tcp-ecco/" onclick="window.open('http://www.textcreationpartnership.org/tcp-ecco/'); return false">http://www.textcreationpartnership.org/tcp-ecco/</a></div><div class="endnote" id="d5517e947"><span class="noteRef">[6]</span><a class="ref" href="http://www.openwalnut.org" onclick="window.open('http://www.openwalnut.org'); return false">http://www.openwalnut.org</a></div><div class="endnote" id="d5517e951"><span class="noteRef">[7]</span><a class="ref" href="http://neuro.debian.net/" onclick="window.open('http://neuro.debian.net/'); return false">http://neuro.debian.net/</a></div><div class="endnote" id="d5517e955"><span class="noteRef">[8]</span><a class="ref" href="https://www.virtualbox.org/" onclick="window.open('https://www.virtualbox.org/'); return false">https://www.virtualbox.org/</a></div><div class="endnote" id="d5517e1194"><span class="noteRef">[9]</span><a class="ref" href="http://www.epol-projekt.de/tools-nlp/leipzig-corpus-miner-lcm/" onclick="window.open('http://www.epol-projekt.de/tools-nlp/leipzig-corpus-miner-lcm/'); return false">http://www.epol-projekt.de/tools-nlp/leipzig-corpus-miner-lcm/</a></div></div><div id="worksCited"><h2>Works Cited</h2><div class="bibl"><span class="ref" id="alsumait2009"><!-- close -->AlSumait 2009</span> AlSumait, L., Barbará, D., Gentle,
                    J., Domeniconi, C. "Topic significance ranking of LDA generative models", <cite class="title italic">Machine Learning and Knowledge Discovery in Database</cite>s
                    (2009): 67–82.</div><div class="bibl"><span class="ref" id="blei2003"><!-- close -->Blei 2003</span> Blei, D. M., Ng, A. Y., Jordan, M. I.
                    "Latent dirichlet allocation", <cite class="title italic">The Journal of Machine
                        Learning Research</cite>, 3 (2003):993–1022.</div><div class="bibl"><span class="ref" id="blei2009"><!-- close -->Blei 2009</span> Blei, D. M., Lafferty, J. D. Topic models.
                    In A. N. Srivastava and M. Sahami (eds) <cite class="title italic">Text Mining:
                        Classification, Clustering, and Applications</cite> (2009): 71. CRC
                    Press.</div><div class="bibl"><span class="ref" id="blei2012"><!-- close -->Blei 2012</span> Blei, D. M. "Topic Modeling and Digital
                    Humanities", <cite class="title italic">Journal of Digital Humanities</cite>, 2.1,
                    January 2012.</div><div class="bibl"><span class="ref" id="boyd-graber2009"><!-- close -->Boyd-Graber 2009</span> Boyd-Graber, J., Chang, J.,
                    Gerrish, S., Wang, C., Blei, D. M. "Reading tea leaves: How humans interpret
                    topic models", <cite class="title italic">Advances in Neural Information Processing
                        Systems</cite>, 31 (2009).</div><div class="bibl"><span class="ref" id="cao2010"><!-- close -->Cao 2010</span> Cao, N., Sun, J., Lin, Y. R., Gotz, D., Liu,
                    S., Qu, H. "FacetAtlas: Multifaceted Visualization for Rich Text Corpora",
                        <cite class="title italic">IEEE Transactions on Visualization and Computer
                        Graphics</cite>, 16.6 (2010): 1172–1181.</div><div class="bibl"><span class="ref" id="chaney2012"><!-- close -->Chaney 2012</span> Chaney, A., Blei, D. M. "Visualizing
                    Topic Models", <cite class="title italic">Sixth International AAAI Conference on
                        Weblogs and Social Media</cite> (2012).</div><div class="bibl"><span class="ref" id="chang2010"><!-- close -->Chang 2010</span> Chang, J., Blei, D. M. "Hierarchical
                    relational models for document networks", <cite class="title italic">The Annals of
                        Applied Statistics</cite>, 4.1 (2010): 124–150.</div><div class="bibl"><span class="ref" id="elmqvist2008"><!-- close -->Elmqvist 2008</span> Elmqvist, N., Dragicevic, P.,
                    Fekete, J. D. "Rolling the dice: Multidimensional visual exploration using
                    scatterplot matrix navigation", <cite class="title italic">IEEE Transactions on
                        Visualization and Computer Graphics</cite>, 14.6 (2008): 1539–1148.</div><div class="bibl"><span class="ref" id="gelman2013"><!-- close -->Gelman 2013</span> Gelman, A., Shalizi, C. "Philosophy
                    and the practice of Bayesian statistics", <cite class="title italic">British Journal
                        of Mathematical and Statistical Psychology</cite>, 66.1 (2013):
                    8–38.</div><div class="bibl"><span class="ref" id="gopalan2013"><!-- close -->Gopalan 2013</span> Gopalan, P., Hofman, J. M., Blei, D.
                    M. :Scalable Recommendation with Poisson Factorization", <cite class="title italic">arXiv.org </cite> (2013).</div><div class="bibl"><span class="ref" id="gretarsson2012"><!-- close -->Gretarsson 2012</span> Gretarsson, B., O’donovan, J.,
                    Bostandjiev, S., Höllerer, T., Asuncion, A., Newman, D., Smyth, P. "Topicnets:
                    Visual analysis of large text corpora with topic modeling", <cite class="title italic">ACM Transactions on Intelligent Systems and Technology (TIST)</cite>, 3.2
                    (2012): 23.</div><div class="bibl"><span class="ref" id="havre2000"><!-- close -->Havre 2000</span> Havre, S., Hetzler, B., Nowell, L.
                        <cite class="title italic">ThemeRiver: Visualizing theme changes over time</cite>
                    (2000): 115–123.</div><div class="bibl"><span class="ref" id="heinrich2005"><!-- close -->Heinrich 2005</span> Heinrich, G. <cite class="title italic">Parameter estimation for text analysis</cite> (2005).
                    http://www.arbylon.net/publications/textest</div><div class="bibl"><span class="ref" id="hinneburg2012"><!-- close -->Hinneburg 2012</span> Hinneburg, A., Preiss, R.,
                    Schröder, R. <cite class="title italic">TopicExplorer: Exploring document collections
                        with topic models</cite> (2012): 838–841.</div><div class="bibl"><span class="ref" id="johnson1997"><!-- close -->Johnson 1997</span> Johnson, N. L., Kotz, S.,
                    Balakrishnan, N. <cite class="title italic">Discrete Multivariate
                        Distributions</cite>. John Wiley &amp; Sons (1997).</div><div class="bibl"><span class="ref" id="kohonen2001"><!-- close -->Kohonen 2001</span> Kohonen, T. <cite class="title italic">Self-Organizing Maps</cite>. Springer Science &amp; Business Media
                    (2001).</div><div class="bibl"><span class="ref" id="kotz2000"><!-- close -->Kotz 2000</span> Kotz, S., Balakrishnan, N., Lloyd Johnson,
                    N. <cite class="title italic">Continuous Multivariate Distributions: Models and
                        Applications</cite>, 1. John Wiley &amp; Sons, 2nd edition (2000).</div><div class="bibl"><span class="ref" id="kruskal2009"><!-- close -->Kruskal 2009</span> Kruskal, J. B., Wish, M. <cite class="title italic">Multidimensional Scaling. Quantitative Applications in the
                        Social Sciences</cite>. SAGE Publications (1978).</div><div class="bibl"><span class="ref" id="landauer2008"><!-- close -->Landauer 2008</span> Landauer, T., Dumais, S. "Latent
                    semantic analysis", <cite class="title italic">Scholarpedia</cite>, 3.11 (2008):
                    4356.</div><div class="bibl"><span class="ref" id="manning1999"><!-- close -->Manning 1999</span> Manning, C. D., Schütze, H. <cite class="title italic">Foundations of statistical natural language
                    processing</cite>. MIT Press (1999).</div><div class="bibl"><span class="ref" id="marchionini2006"><!-- close -->Marchionini 2006</span> Marchionini, G. "Exploratory
                    Search", <cite class="title italic">Communications of the ACM</cite>, 49.4 (2006):
                    41–46.</div><div class="bibl"><span class="ref" id="mei2008"><!-- close -->Mei 2008</span> Mei, Q., Cai, D., Zhang, D., Zhai, C. X.
                    "Topic modeling with network regularization." In <cite class="title italic">17th
                        international conference on World Wide Web</cite> (2008): 101–110.</div><div class="bibl"><span class="ref" id="miller1998"><!-- close -->Miller 1998</span> Miller, N. E., Wong, P. C., Brewster,
                    M., Foote, H. <cite class="title italic">TOPIC ISLANDS TM-a wavelet-based text
                        visualization system</cite> (1998): 189–196.</div><div class="bibl"><span class="ref" id="mimno2011"><!-- close -->Mimno 2011</span> Mimno, D., Blei, D. M. "Bayesian
                    checking for topic models." In <cite class="title italic">Proceedings of the
                        Conference on Empirical Methods in Natural Language Processing</cite>
                    (2011): 227–237.</div><div class="bibl"><span class="ref" id="muenkel2014"><!-- close -->Münkel 2014</span> Münkel, D. <cite class="title italic">Die
                        DDR im Blick der Stasi. Für den Bundesbeauftragten für die Unterlagen des
                        Staatssicherheitsdienstes der ehemaligen Deutschen Demokratischen
                        Republik</cite> (BStU) (2014).</div><div class="bibl"><span class="ref" id="niekler2012"><!-- close -->Niekler 2012</span> Niekler, A., Jähnichen, P. <cite class="title italic">Matching Results of Latent Dirichlet Allocation for
                        Text</cite> (2012): 317–322.</div><div class="bibl"><span class="ref" id="oesterling2010"><!-- close -->Oesterling 2010</span> Oesterling, P., Scheuermann,
                    G., Teresniak, S., Heyer, G., Koch, S., Ertl, T., Weber, G. H. "Two-stage
                    framework for a topology-based projection and visualization of classified
                    document collections", <cite class="title italic">Transactions of the IRE Professional
                        Group on Audio</cite>, (2010): 91–98.</div><div class="bibl"><span class="ref" id="paulovich2006"><!-- close -->Paulovich 2006</span> Paulovich, F. V., Minghim, R.
                    "Text Map Explorer: a Tool to Create and Explore Document Maps", <cite class="title italic">Tenth International Conference on Information Visualisation
                        (IV’06)</cite>, (2006): 245–251.</div><div class="bibl"><span class="ref" id="rosen-zvi2005"><!-- close -->Rosen-Zvi 2005</span> Rosen-Zvi, M., Griffiths, T. L.,
                    Steyvers, M. "Learning Author Topic Models from Text Corpora", <cite class="title italic">The Journal of Machine Learning Research</cite>
                    (2005).</div><div class="bibl"><span class="ref" id="salton1988"><!-- close -->Salton 1988</span> Salton, G., Buckley, C.
                    "Term-Weighting Approaches in Automatic Text Retrieval", <cite class="title italic">Information processing &amp; management</cite>, 24.5 (1988):
                    513–523.</div><div class="bibl"><span class="ref" id="sammon1969"><!-- close -->Sammon 1969</span> Sammon, J. W. "A nonlinear mapping for
                    data structure analysis", <cite class="title italic">IEEE Transactions on
                        Computers</cite> (1969).</div><div class="bibl"><span class="ref" id="snyder2013"><!-- close -->Snyder 2013</span> Snyder, J., Knowles, R., Dredze, M.,
                    Gormley, M. R., Wolfe, T. "Topic Models and Metadata for Visualizing Text
                    Corpora." In <cite class="title italic">HLT- NAACL</cite> (2013): 5–9.</div><div class="bibl"><span class="ref" id="sparckjones1972"><!-- close -->Sparck Jones 1972</span> Sparck Jones, K. "A
                    statistical interpretation of term specificity and its application in
                    retrieval", <cite class="title italic">Journal of documentation</cite>, 28.1 (1972):
                    11–21.</div><div class="bibl"><span class="ref" id="steele2010"><!-- close -->Steele 2010</span> Steele, J., Iliinsky, N. <cite class="title italic">Beautiful Visualization. Looking at Data through the Eyes of
                        Experts</cite>. O'Reilly Media, Inc. (2010). </div><div class="bibl"><span class="ref" id="teh2009"><!-- close -->Teh 2009</span> Teh, Y. W., Jordan M. I. "Hierarchical
                    Bayesian nonparametric models with applications", <cite class="title italic">Bayesian
                        Nonparametrics</cite> (2009): 158.</div><div class="bibl"><span class="ref" id="wise1995"><!-- close -->Wise 1995</span> Wise, J. A., Thomas, J. J., Pennock, K.,
                    Lantrip, D., Pottier, M., Schur, A., Crow, V. <cite class="title italic">Visualizing
                        the non-visual: spatial analysis and interaction with information from text
                        documents</cite> (1995): 51-58.</div><div class="bibl"><span class="ref" id="desaussure2001"><!-- close -->de Saussure 2001</span> de Saussure, F. <cite class="title italic">Grundfragen der allgemeinen Sprachwissenschaft</cite>. de
                    Gruyter, 3rd edition (2001).</div></div><div class="toolbar"><a href="/dhq/vol/11/2/index.html">2017 11.2</a>
             | 
            <a rel="external" href="/dhq/vol/11/2/000296.xml">XML</a>
            | 
            <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div></div><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script><div id="comments"><div id="disqus_thread"/><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    // The following are highly recommended additional parameters. Remove the slashes in front to use.
    var disqus_identifier = '000296';
    var disqus_url = 'http://www.digitalhumanities.org/dhq/vol/11/2/000296/000296.html';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><div id="footer"> 
            URL: http://www.digitalhumanities.org/dhq/vol/11/2/000296/000296.html<br/>Last updated:
            <script type="text/javascript">
                var monthArray = new initArray("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December");
                var lastModifiedDate = new Date(document.lastModified);
                var currentDate = new Date();
                document.write(" ",monthArray[(lastModifiedDate.getMonth()+1)]," ");
                document.write(lastModifiedDate.getDate(),", ",(lastModifiedDate.getFullYear()));
            </script><br/> Comments: <a href="mailto:dhqinfo@digitalhumanities.org" class="footer">dhqinfo@digitalhumanities.org</a><br/> Published by:
            <a href="http://www.digitalhumanities.org" class="footer">The Alliance of Digital Humanities Organizations</a><br/>Affiliated with: <a href="http://llc.oxfordjournals.org/">Literary and Linguistic Computing</a><br/> Copyright 2005 - <script type="text/javascript">
                document.write(currentDate.getFullYear());</script><br/><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png"/></a><br/>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
        </div></div></div></body></html>