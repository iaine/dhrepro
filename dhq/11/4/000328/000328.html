<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><title>DHQ: Digital Humanities Quarterly: Semantic Enrichment of a Multilingual Archive with Linked Open
                    Data</title><link rel="stylesheet" type="text/css" href="/dhq/common/css/dhq.css"/><link rel="stylesheet" type="text/css" media="screen" href="/dhq/common/css/dhq_screen.css"/><link rel="stylesheet" type="text/css" media="print" href="/dhq/common/css/dhq_print.css"/><link rel="alternate" type="application/atom+xml" href="/dhq/feed/news.xml"/><link rel="shortcut icon" href="/dhq/common/images/favicon.ico"/><script type="text/javascript" src="/dhq/common/js/javascriptLibrary.js">
                &lt;!-- Javascript functions --&gt;
            </script><script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-15812721-1']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type =
'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0];
s.parentNode.insertBefore(ga, s);
 })();

        </script></head><body><div id="top"><div id="backgroundpic"><script type="text/javascript" src="/dhq/common/js/pics.js"><!--displays banner image--></script></div><div id="banner"><div id="dhqlogo"><img src="/dhq/common/images/dhqlogo.png" alt="DHQ Logo"/></div><div id="longdhqlogo"><img src="/dhq/common/images/dhqlogolonger.png" alt="Digital Humanities Quarterly Logo"/></div></div><div id="topNavigation"><div id="topnavlinks"><span><a href="/dhq/" class="topnav">home</a></span><span><a href="/dhq/submissions/index.html" class="topnav">submissions</a></span><span><a href="/dhq/about/about.html" class="topnav">about dhq</a></span><span><a href="/dhq/people/people.html" class="topnav">dhq people</a></span><span id="rightmost"><a href="/dhq/contact/contact.html" class="topnav">contact</a></span></div><div id="search"><form action="/dhq/findIt" method="get" onsubmit="javascript:document.location.href=cleanSearch(this.queryString.value); return false;"><div><input type="text" name="queryString" size="18"/> <input type="submit" value="Search"/></div></form></div></div></div><div id="main"><div id="leftsidebar"><div id="leftsidenav"><span>Current Issue<br/></span><ul><li><a href="/dhq/vol/11/3/index.html">2017: 11.3</a></li></ul><span>Preview Issue<br/></span><ul><li><a href="/dhq/preview/index.html">2017: 11.4</a></li></ul><span>Previous Issues<br/></span><ul><li><a href="/dhq/vol/11/2/index.html">2017: 11.2</a></li><li><a href="/dhq/vol/11/1/index.html">2017: 11.1</a></li><li><a href="/dhq/vol/10/4/index.html">2016: 10.4</a></li><li><a href="/dhq/vol/10/3/index.html">2016: 10.3</a></li><li><a href="/dhq/vol/10/2/index.html">2016: 10.2</a></li><li><a href="/dhq/vol/10/1/index.html">2016: 10.1</a></li><li><a href="/dhq/vol/9/4/index.html">2015: 9.4</a></li><li><a href="/dhq/vol/9/3/index.html">2015: 9.3</a></li><li><a href="/dhq/vol/9/2/index.html">2015: 9.2</a></li><li><a href="/dhq/vol/9/1/index.html">2015: 9.1</a></li><li><a href="/dhq/vol/8/4/index.html">2014: 8.4</a></li><li><a href="/dhq/vol/8/3/index.html">2014: 8.3</a></li><li><a href="/dhq/vol/8/2/index.html">2014: 8.2</a></li><li><a href="/dhq/vol/8/1/index.html">2014: 8.1</a></li><li><a href="/dhq/vol/7/3/index.html">2013: 7.3</a></li><li><a href="/dhq/vol/7/2/index.html">2013: 7.2</a></li><li><a href="/dhq/vol/7/1/index.html">2013: 7.1</a></li><li><a href="/dhq/vol/6/3/index.html">2012: 6.3</a></li><li><a href="/dhq/vol/6/2/index.html">2012: 6.2</a></li><li><a href="/dhq/vol/6/1/index.html">2012: 6.1</a></li><li><a href="/dhq/vol/5/3/index.html">2011: 5.3</a></li><li><a href="/dhq/vol/5/2/index.html">2011: 5.2</a></li><li><a href="/dhq/vol/5/1/index.html">2011: 5.1</a></li><li><a href="/dhq/vol/4/2/index.html">2010: 4.2</a></li><li><a href="/dhq/vol/4/1/index.html">2010: 4.1</a></li><li><a href="/dhq/vol/3/4/index.html">2009: 3.4</a></li><li><a href="/dhq/vol/3/3/index.html">2009: 3.3</a></li><li><a href="/dhq/vol/3/2/index.html">2009: 3.2</a></li><li><a href="/dhq/vol/3/1/index.html">2009: 3.1</a></li><li><a href="/dhq/vol/2/1/index.html">2008: 2.1</a></li><li><a href="/dhq/vol/1/2/index.html">2007: 1.2</a></li><li><a href="/dhq/vol/1/1/index.html">2007: 1.1</a></li></ul><span>Indexes<br/></span><ul><li><a href="/dhq/index/title.html"> Title</a></li><li><a href="/dhq/index/author.html"> Author</a></li></ul></div><img src="/dhq/common/images/lbarrev.png" style="margin-left : 7px;" alt="sidenavbarimg"/><div id="leftsideID"><b>ISSN 1938-4122</b><br/></div><div class="leftsidecontent"><h3>Announcements</h3><ul><li><a href="/dhq/announcements/index.html#reviewers">Call for Reviewers</a></li><li><a href="/dhq/announcements/index.html#submissions">Call for Submissions</a></li></ul></div><div class="leftsidecontent"><script type="text/javascript">addthis_pub  = 'dhq';</script><a href="http://www.addthis.com/bookmark.php" onmouseover="return addthis_open(this, '', '[URL]', '[TITLE]')" onmouseout="addthis_close()" onclick="return addthis_sendto()"><img src="http://s9.addthis.com/button1-addthis.gif" width="125" height="16" alt="button1-addthis.gif"/></a><script type="text/javascript" src="http://s7.addthis.com/js/152/addthis_widget.js">&lt;!-- Javascript functions --&gt;</script></div></div><div id="mainContent"><div id="printSiteTitle">DHQ: Digital Humanities Quarterly</div><div xmlns:dhqBiblio="http://digitalhumanities.org/dhq/ns/biblio" class="DHQarticle"><div id="pubInfo">Preview<br/>2017<br/>Volume 11 Number 4</div><div class="toolbar"><form id="taporware" action="get"><div><a href="/dhq/preview/index.html">Preview</a>
                     | 
                    <a rel="external" href="/dhq/vol/11/4/000328.xml">XML</a>

| 
		   Discuss
			(<a href="/dhq/vol/11/4/000328/000328.html#disqus_thread" data-disqus-identifier="000328">
				Comments
			</a>)
                </div></form></div>
    <div class="DHQheader">
        
            
                
                <h1 class="articleTitle lang en">Semantic Enrichment of a Multilingual Archive with Linked Open
                    Data</h1>
                <div class="author"><a rel="external" href="/dhq/preview/bios.html#dewilde_max">Max De Wilde</a> &lt;<a href="mailto:madewild_at_ulb_dot_ac_dot_be" onclick="javascript:window.location.href='mailto:'+deobfuscate('madewild_at_ulb_dot_ac_dot_be'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('madewild_at_ulb_dot_ac_dot_be'); return false;">madewild_at_ulb_dot_ac_dot_be</a>&gt;, Université libre de Bruxelles(ULB), Information Science
                        Department</div>
                <div class="author"><a rel="external" href="/dhq/preview/bios.html#hengchen_simon">Simon Hengchen</a> &lt;<a href="mailto:simon_dot_hengchen_at_helsinki_dot_fi" onclick="javascript:window.location.href='mailto:'+deobfuscate('simon_dot_hengchen_at_helsinki_dot_fi'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('simon_dot_hengchen_at_helsinki_dot_fi'); return false;">simon_dot_hengchen_at_helsinki_dot_fi</a>&gt;, University of Helsinki (UH)</div>
            
            

            
        
        
        
        
    <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Semantic%20Enrichment%20of%20a%20Multilingual%20Archive%20with%20Linked%20Open%20Data&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=2018-01-08&amp;rft.volume=011&amp;rft.issue=4&amp;rft.aulast=De Wilde&amp;rft.aufirst=Max&amp;rft.au=Max%20De Wilde&amp;rft.au=Simon%20Hengchen"> </span></div>

    <div id="DHQtext">
        
            <div id="abstract"><h2>Abstract</h2>
                
                <p>This paper introduces MERCKX, a Multilingual Entity/Resource Combiner &amp;
                    Knowledge eXtractor. A case study involving the semantic enrichment of a
                    multilingual archive is presented with the aim of assessing the relevance of
                    natural language processing techniques such as named-entity recognition and
                    entity linking for cultural heritage material. In order to improve the indexing
                    of historical collections, we map entities to the Linked Open Data cloud using a
                    language-independent method. Our evaluation shows that MERCKX outperforms
                    similar tools on the task of place disambiguation and linking, achieving over
                    80% precision despite lower recall scores. These results are encouraging for
                    small and medium-size cultural institutions since they demonstrate that semantic
                    enrichment can be achieved with limited resources.</p>
            </div>
            
        
        
            <div class="div div0">
                <h1 class="head">1. Introduction</h1>
                <div class="counter"><a href="#p1">1</a></div><div class="ptext" id="p1">Libraries, Archives and Museums (LAM) are increasingly faced with budget cuts,
                    compelling them to review their traditional ways to exploit their collections.
                    Short-term results are often expected by funding bodies, and cultural heritage
                    institutions are therefore encouraged to gain more value out of their data by
                    linking them to existing knowledge bases. In this context, semantic enrichment
                    techniques such as named-entity recognition (NER) and entity linking (EL) have
                    attracted attention since they allow these institutions to enrich their
                    collections semantically with few resources. For LAM, the perspective of freely
                    reusing existing knowledge to map their collections to the Web represents a
                    great opportunity.</div>
                <div class="counter"><a href="#p2">2</a></div><div class="ptext" id="p2">Using digital methods also allows researchers to tackle larger datasets. While
                    larger corpora do not induce better research <em class="term">per se</em>, they allow to
                    have a bird’s eye view on the context of a particular project. In this paper, we
                    evaluate the relevance of NER and EL for an archive of OCRised Belgian
                    periodicals. We focus on historical locations because our analysis, detailed in
                    Section 3, shows that users are mainly interested in them. However, the
                    methodology described in this paper could be extended to other types of entities
                    present in knowledge bases.</div>
                <div class="counter"><a href="#p3">3</a></div><div class="ptext" id="p3">The remainder of this paper is structured as follows: Section 2 discusses related
                    work, sections 3 and 4 present our case-study and workflow respectively, Section
                    5 discusses the results obtained and Section 6 concludes the paper whilst
                    providing new research tracks.</div>
            </div>
            <div class="div div0">
                <h1 class="head">2. Related Work</h1>
                <div class="counter"><a href="#p4">4</a></div><div class="ptext" id="p4">Named-entity recognition and other information extraction techniques such as
                    entity linking have been increasingly adopted by DH practitioners, since they
                    help small institutions to enrich their collections with semantic
                        information<a class="noteRef" href="#d20668e217">[1]</a>. According to Blanke and
                    Kristel, "semantically enriched library and archive federations
                            have recently become an important part of research in digital libraries
                            and archives"
                         [<a class="ref" href="#blanke2013">Blanke and Kristel 2013</a>]. This is illustrated by such projects as EHRI<a class="noteRef" href="#d20668e228">[2]</a>, an 8-million euros EU project
                    focusing on Holocaust research, and CENDARI<a class="noteRef" href="#d20668e233">[3]</a>, a
                    European Commission-funded project aiming to integrate digital resources for
                    medieval and WWI history. Similarily, the Europeana Newspapers project<a class="noteRef" href="#d20668e238">[4]</a> has been developing
                    NER tools<a class="noteRef" href="#d20668e244">[5]</a>
                    specifically to process historical newspaper collections.</div>
                <div class="counter"><a href="#p5">5</a></div><div class="ptext" id="p5">The growing of the Linked Open Data (LOD) cloud and the availability of free
                    online tools have facilitated the access to information extraction for
                    librarians, archivists and collections managers that are not IT experts but are
                    eager to experiment with new technologies. The LOD Around The Clock project of
                    the European Commission, for instance, was started to "help institutions and
                        individuals in publishing and consuming quality Linked Data on the Web".<a class="noteRef" href="#d20668e255">[6]</a> Its main
                    declared goal is to "continuously monitor and improve the quality of data
                        links within the Linking Open Data cloud". The existence of such
                    large-scale incentives demonstrate the potential of LOD for the semantic
                    enrichment of collections maintained in LAM.</div>
                <div class="counter"><a href="#p6">6</a></div><div class="ptext" id="p6">A number of cultural institutions have experimented with NER and EL over the last
                    decade. The Powerhouse Museum in Sydney has implemented OpenCalais within its
                    collection management database, although no evaluation of the entities has been
                            performed.<a class="noteRef" href="#d20668e266">[7]</a>
                    Lin et al. also explore NER in order to create a faceted browsing interface for
                    users of large museum collections [<a class="ref" href="#lin2010">Lin et al. 2010</a>], while Segers et al.
                    offer an interesting evaluation of the extraction of people, locations and
                    events from unstructured text in the collection management database of the
                    Rijksmuseum in Amsterdam [<a class="ref" href="#segers2011">Segers et al. 2011</a>]. Maturana et al. showed how
                    LOD could be successfully integrated in a museum platform to enhance the
                    experience of end users [<a class="ref" href="#maturana2013">Maturana et al. 2013</a>]. Their innovative semantic
                    platform MisMuseos, a meta-museum aggregating 17 000 works from seven Spanish
                    cultural institutions, offers users a facet-based search module, semantic
                    content creation and graph navigation.</div>
                <div class="counter"><a href="#p7">7</a></div><div class="ptext" id="p7">In the specific domain of archives, Rodriquez et al. compared the results of
                    several NER services on a corpus of mid-20th-century typewritten documents [<a class="ref" href="#rodriquez2012">Rodriquez et al. 2012</a>]. A set of test data, consisting of raw and
                    corrected OCR output, was manually annotated with people, locations, and
                    organisations. This approach allows an evaluation of the different NER services
                    against the manually annotated data. Their methodology was generalised for LAM
                    by van Hooland et al. in the context of the <cite class="title italic">Free Your
                        Metadata</cite> project<a class="noteRef" href="#d20668e284">[8]</a>
                    [<a class="ref" href="#vanhooland2015">van Hooland et al. 2015</a>], and extended to other languages such as French,
                    by Hengchen et al. [<a class="ref" href="#hengchen2015">Hengchen et al. 2015</a>]. The BBC also set up a system
                    to connect its vast archive with current material through Semantic Web
                    technologies [<a class="ref" href="#raimond2013">Raimond et al. 2013</a>].</div>
                <div class="counter"><a href="#p8">8</a></div><div class="ptext" id="p8">Bingel and Haider compared the performance of various entity classifiers on the
                    DeReKo corpus of contemporary German [<a class="ref" href="#bingel2014">Bingel and Haider 2014</a>]
                    [<a class="ref" href="#kupietz2010">Kupietz et al. 2010</a>], which they say exhibits a "strong dispersion
                        [with regard to] genre, register and time". However, the authors concede
                    that newspaper documents are largely prevailing and that "relatively few texts
                        reach back to the mid-20th century". This casts doubt over the actual
                    strong temporal dispersion of this corpus. Moreover, although the study of NER
                    in German is particularly challenging due to its use of capital letters for all
                    common nouns, their evaluation remains monolingual and does not offer any
                    insights as to how the classifiers would perform on a linguistically diverse
                    corpus. Agirre et al. and Fernando and Stevenson considered how to adapt entity
                    linking to cultural heritage content, but both focus exclusively on English data
                    and did not take advantage of the multilingual structure of the Semantic Web
                        [<a class="ref" href="#agirre2012">Agirre et al. 2012</a>]
                    [<a class="ref" href="#fernando2012">Fernando and Stevenson 2012</a>]. Frontini et al. exploited the French DBpedia and
                    combined it with the BnF Linked Data<a class="noteRef" href="#d20668e315">[9]</a> in order to extract
                    mentions of less known authors, but their graph-based approach also remained
                    monolingual [<a class="ref" href="#frontini2015">Frontini et al. 2015</a>].</div>
                <div class="counter"><a href="#p9">9</a></div><div class="ptext" id="p9">On the more focused task of extracting place names, Speriosu and Baldridge used a
                    corpus of 20th century newswire articles and 19th century American Civil War
                    texts to demonstrate that relying on information available in the text being
                    processed is more effective than using external data [<a class="ref" href="#speriosu2013">Speriosu and Baldridge 2013</a>]. DeLozier et al. tackled the task of annotating a historical text corpus with
                    geographic references [<a class="ref" href="#delozier2016">DeLozier et al. 2016</a>], while Leidner proposed an
                    evaluation method for different systems [<a class="ref" href="#leidner2007">Leidner 2007</a>].</div>
                <div class="counter"><a href="#p10">10</a></div><div class="ptext" id="p10">Finally, the periodical Aggregation and Indexing Plan for Europeana periodicals,
                    was launched in 2005. It produced metadata for 18 million pages of news and
                    full-text from OCR for around 10 million pages, also including a NER component
                    performed by the National Library of the Netherlands.<a class="noteRef" href="#d20668e334">[10]</a> A new website<a class="noteRef" href="#d20668e340">[11]</a> was introduced in 2014,
                    allowing users to cross-search and reuse over 25 million digital items and over
                    165 million bibliographic records. However, this European Library does not use
                    LOD resources to enrich documents, using instead its own ontology developed
                    specifically for the project, a methodology that few institutions could afford
                    to follow.</div>
            </div>
            <div class="div div0">
                <h1 class="head">3. Case study</h1>
                <div class="counter"><a href="#p11">11</a></div><div class="ptext" id="p11">The <cite class="title italic">Historische Kranten</cite><a class="noteRef" href="#d20668e356">[12]</a> project involved the
                    digitization, OCR processing and online publication of over a million articles
                    compiled from 41 Belgian newspapers published between 1818 and 1972. Such a
                    large scope allows researchers to gather information on day-to-day activities in
                    the Ypres region during WWI and WWII, thus offering a great potential in the
                    context of CENDARI and other war-related Digital Humanities projects. The
                    project has been launched under the impulse of Erfgoedcel CO7<a class="noteRef" href="#d20668e361">[13]</a>, a Flemish organisation aiming to shed
                    light on the cultural features of the Ypres region. The target audience of such
                    a project is thus broad: it includes scientists, WWI historians, and also simply
                    individuals interested in the history of their region. Analysing the needs of
                    one’s target audience is central to digitisation projects, as illustrated by
                    recent initiatives such as a Europeana user requirements group or, more
                    recently, the Belgian Science Policy (BELSPO) funded project MADDLAIN<a class="noteRef" href="#d20668e366">[14]</a>.</div>
                <div class="counter"><a href="#p12">12</a></div><div class="ptext" id="p12">Articles in the <cite class="title italic">Historische Kranten</cite> corpus are
                    written in Dutch, French, and English, and focus mainly on the city of Ypres and
                    its neighbourhood. Currently, the full texts of the <cite class="title italic">Historische Kranten</cite> corpus have been indexed, which means that
                    searches for particular mentions in the periodicals suffer from both noise and
                    silence. For instance, a query on the string "Huygens"
                    returns correct results about Christiaan Huygens:</div>
                <div class="counter"><a href="#p13">13</a></div><div class="ptext" id="p13"><span class="hi bold">Example 1.</span> Links zien wij Christiaan Huygens die met zijn
                    slingeruurwerk de oplossing bracht voor het meten van de tijd </div>
                <div class="counter"><a href="#p14">14</a></div><div class="ptext" id="p14">But one also gets results that are not relevant in this context (noise):</div>
                <div class="counter"><a href="#p15">15</a></div><div class="ptext" id="p15"><span class="hi bold">Example 2.</span> La reconnaissance du cadavre de la veuve Huygens,
                    faite par les hommes de l’art, a fait constater l’existence de neuf blessures
                    sur la tête </div>
                <div class="counter"><a href="#p16">16</a></div><div class="ptext" id="p16">Moreover, interesting results are lost due to variations in spelling
                    (silence):</div>
                <div class="counter"><a href="#p17">17</a></div><div class="ptext" id="p17"><span class="hi bold">Example 3.</span> [...] en op het uurwerk toegepast door den
                    Hollander Huyghens (1629-1695). </div>
                <div class="counter"><a href="#p18">18</a></div><div class="ptext" id="p18">In order to get a clearer picture of the interests of users, we tracked
                    individual queries on <a class="ref" href="http://www.historischekranten.be/" onclick="window.open('http://www.historischekranten.be/'); return false">http://www.historischekranten.be/</a> with Google Analytics over a 4-year
                    period, yielding 124 510 results. About 4 200 unique keywords were used at least
                    three times, of which the ten most popular are shown in Table 1. We can see that
                    locations are especially favored by the users, which prompted us to focus
                    preliminary work on this type of entities. Most are related to the First World
                    War, since Ypres was the scene of four major battles during that conflict.</div>
                <div class="counter"><a href="#p19">19</a></div><div class="ptext" id="p19">Interestingly, we notice that the list also contains the term <span class="foreign i">oorlog</span> ("war" in Dutch) which
                    does not constitute a valid named entity in the sense of Kripke: "a rigid designator designates the same object in all
                            possible worlds in which that object exists and never designates
                            anything else"
                         [<a class="ref" href="#kripke1982">Kripke 1982</a>, 77]. This makes the case for a more integrated approach to information
                    extraction able to tackle proper nouns and common nouns in a single workflow,
                    which is precisely one of the advantages of working with Linked Open Data
                    resources, as will be shown in Section 4.</div>
                <div class="table"><table class="table"><tr class="row label">
                        <td valign="top" class="cell">#</td>
                        <td valign="top" class="cell">Term</td>
                        <td valign="top" class="cell">Hits</td>
                        <td valign="top" class="cell">Category</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">1.</td>
                        <td valign="top" class="cell">Zillebeke</td>
                        <td valign="top" class="cell">398</td>
                        <td valign="top" class="cell">Location</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">2.</td>
                        <td valign="top" class="cell">Passendale</td>
                        <td valign="top" class="cell">351</td>
                        <td valign="top" class="cell">Location</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">3.</td>
                        <td valign="top" class="cell">Westouter</td>
                        <td valign="top" class="cell">259</td>
                        <td valign="top" class="cell">Location</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">4.</td>
                        <td valign="top" class="cell">leper</td>
                        <td valign="top" class="cell">197</td>
                        <td valign="top" class="cell">Location</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">5.</td>
                        <td valign="top" class="cell">oorlog</td>
                        <td valign="top" class="cell">178</td>
                        <td valign="top" class="cell">Concept</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">6.</td>
                        <td valign="top" class="cell">Reninghelst</td>
                        <td valign="top" class="cell">163</td>
                        <td valign="top" class="cell">Location</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">7.</td>
                        <td valign="top" class="cell">Bikschote</td>
                        <td valign="top" class="cell">149</td>
                        <td valign="top" class="cell">Location</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">8.</td>
                        <td valign="top" class="cell">Merkem</td>
                        <td valign="top" class="cell">127</td>
                        <td valign="top" class="cell">Location</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">9.</td>
                        <td valign="top" class="cell">Geluveld</td>
                        <td valign="top" class="cell">125</td>
                        <td valign="top" class="cell">Location</td>
                    </tr><tr class="row">
                        <td valign="top" class="cell">10.</td>
                        <td valign="top" class="cell">Wijtschate</td>
                        <td valign="top" class="cell">121</td>
                        <td valign="top" class="cell">Location</td>
                    </tr></table><div class="caption"><div class="label">Table 1. </div>Top 10 search terms on Historische Kranten</div></div>
            </div>
            <div class="div div0">
                <h1 class="head">4. MERCKX: A Knowledge Extractor</h1>
                <div class="counter"><a href="#p20">20</a></div><div class="ptext" id="p20">The idea behind entity linking is that knowledge bases can be leveraged to
                    perform a full disambiguation of entities through URIs. A correct disambiguation
                    of a mention of Huygens in a text with DBpedia URI dbr:Christiaan_Huygens would
                    encompass mentions of "Christian Huyghens" (French spelling)
                    while excluding information about the Belgian painter Léon Huygens (which has
                    his own unique URI: dbr:Léon_Huygens) or the crater on Mars named after the
                    Dutch astronomer, dbr:Huygens_(crater). In this section, we present MERCKX
                    (Multilingual Entity/Resource Combiner &amp; Knowledge eXtractor), a tool that
                    we designed in order to extract entity mentions from documents and to link them
                    to DBpedia [<a class="ref" href="#bizer2009">Bizer et al. 2009</a>]. As the basis of most semantic web
                    projects and an extremely diverse, multilingual, and extensive ontology, DBpedia
                    was the obvious choice for providing the URIs.</div>
                <div class="counter"><a href="#p21">21</a></div><div class="ptext" id="p21">The workflow of MERCKX for the extraction and disambiguation of entities consists
                    of three phases: downloading resources, building the dictionary, and annotating
                    mentions with positions and URIs. The first two steps can be time-consuming
                    depending on the chosen entity type and additional languages, but they need to
                    be performed only once.</div>
                <div class="div div1">
                    <h2 class="head">4.1. Downloading resources</h2>
                    <div class="counter"><a href="#p22">22</a></div><div class="ptext" id="p22">In order to simplify the download and decompression of the DBpedia dump, we
                        provide a shell script doing this automatically.<a class="noteRef" href="#d20668e626">[15]</a> This script
                        invokes another one written in Python which extracts all the URIs matching a
                        given type in the DBpedia ontology. Instances (or resources) are linked to
                        corresponding types in the form of RDF triples (subject – predicate –
                        object):</div>
                    <span class="monospace">
                        <div class="counter"><a href="#p23">23</a></div><div class="ptext" id="p23">&lt;http://dbpedia.org/resource/Autism&gt; <br/>
                            &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; <br/>
                            &lt;http://dbpedia.org/ontology/Disease&gt;</div>
                        <div class="counter"><a href="#p24">24</a></div><div class="ptext" id="p24">&lt;http://dbpedia.org/resource/Aristotle&gt; <br/>
                            &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; <br/>
                            &lt;http://dbpedia.org/ontology/Philosopher&gt;</div>
                        <div class="counter"><a href="#p25">25</a></div><div class="ptext" id="p25">&lt;http://dbpedia.org/resource/Alabama&gt; <br/>
                            &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; <br/>
                            &lt;http://dbpedia.org/ontology/AdministrativeRegion&gt;</div>
                        <div class="counter"><a href="#p26">26</a></div><div class="ptext" id="p26">&lt;http://dbpedia.org/resource/Alabama&gt; <br/>
                            &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;<br/>
                            &lt;http://dbpedia.org/ontology/Place&gt; </div>
                    </span>
                    <div class="counter"><a href="#p27">27</a></div><div class="ptext" id="p27">A concept can be categorised by several types, as illustrated by
                            "Alabama" in the sample above, which is both an
                            "Administrative Region" and a
                            "Place".</div>
                </div>
                <div class="div div1">
                    <h2 class="head">4.2. Mapping labels to URIs</h2>
                    <div class="counter"><a href="#p28">28</a></div><div class="ptext" id="p28">In the second phase, MERCKX maps all the labels to their corresponding URIs
                        in the selected languages. The relationship between URIs and labels is also
                        expressed by triples:</div>
                    <span class="monospace">
                        <div class="counter"><a href="#p29">29</a></div><div class="ptext" id="p29">&lt;http://dbpedia.org/resource/South_Africa&gt; <br/>
                            &lt;http://www.w3.org/2000/01/rdf-schema#label&gt; <br/> "Afrique du
                            Sud"@fr</div>
                        <div class="counter"><a href="#p30">30</a></div><div class="ptext" id="p30">&lt;http://dbpedia.org/resource/Andorra&gt; <br/>
                            &lt;http://www.w3.org/2000/01/rdf-schema#label&gt; <br/>
                            "Andorre"@fr</div>
                        <div class="counter"><a href="#p31">31</a></div><div class="ptext" id="p31">&lt;http://dbpedia.org/resource/Angola&gt; <br/>
                            &lt;http://www.w3.org/2000/01/rdf-schema#label&gt; <br/> "Angola"@fr</div>
                        <div class="counter"><a href="#p32">32</a></div><div class="ptext" id="p32">&lt;http://dbpedia.org/resource/Saudi_Arabia&gt; <br/>
                            &lt;http://www.w3.org/2000/01/rdf-schema#label&gt; <br/> "Arabie
                            saoudite"@fr </div>
                    </span>
                    <div class="counter"><a href="#p33">33</a></div><div class="ptext" id="p33">To make this more legible and reduce the size of the file, the initialisation
                        script converts this to a cleaner format, using the dbr: prefix instead of
                        the full URI starting with <a class="ref" href="http://dbpedia.org/resource/" onclick="window.open('http://dbpedia.org/resource/'); return false">http://dbpedia.org/resource/</a>, removing the recurrent
                            "label" predicate, and inverting the order of the
                        original triples:</div>
                    <span class="monospace">
                        <div class="counter"><a href="#p34">34</a></div><div class="ptext" id="p34">Afrique du Sud    dbr:South_Africa <br/> Andorre
                                      dbr:Andorra <br/>
                            Angola           
                            dbr:Angola <br/> Arabie saoudite   dbr:Saudi_Arabia </div>
                    </span>
                    <div class="counter"><a href="#p35">35</a></div><div class="ptext" id="p35">When using more than one language, the order in which they are loaded in this
                        lookup table is important because a label can only point to a single URI.
                        For instance, the French label "Liège"@fr predictably corresponds to the
                        city of dbr:Liège, but the Dutch label "Liège"@nl redirects to the homonymy
                        page dbr:Liège_(disambiguation) which is not a valid place: it contains
                        references to the French municipality of Le Liège and to the Liège metro
                        station in Paris for instance. If the languages are combined in that order,
                        the conflict between URIs will result in a decrease in recall. To reduce
                        problems due to conflicting labels, MERCKX applies the following strategy
                        (text in parentheses provides a concrete example for every step):</div>
                    <div class="ptext"><ol class="list"><li class="item">Load the label files for each language, one by one (EN &gt; NL &gt;
                            FR).</li><li class="item">Check for each label if it corresponds to the chosen type
                            (dbo:Place).</li><li class="item">If the label already exists, check if the type remains the same
                            ("Avant"@nl is already listed as a place, but is "Avant"@fr also a
                            place?).</li><li class="item">If the type is the same, update the URI (yes &gt; URI FR replaces URI
                            NL).</li><li class="item"> If the type is different – i.e. multilingually ambiguous – remove the
                            label (no &gt; suppress “Avant” from the file).<a class="noteRef" href="#d20668e757">[16]</a></li></ol></div>
                    <div class="counter"><a href="#p36">36</a></div><div class="ptext" id="p36">Table 2 shows a summary of the number of places extracted (URIs, labels by
                        language, and combined labels).</div>
                    <div class="table"><table class="table"><tr class="row label">
                            <td valign="top" class="cell">URIs</td>
                            <td valign="top" class="cell">EN</td>
                            <td valign="top" class="cell">NL</td>
                            <td valign="top" class="cell">FR</td>
                            <td valign="top" class="cell">ALL</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">735,062</td>
                            <td valign="top" class="cell">709,357</td>
                            <td valign="top" class="cell">194,208</td>
                            <td valign="top" class="cell">186,483</td>
                            <td valign="top" class="cell">857,911</td>
                        </tr></table><div class="caption"><div class="label">Table 2. </div>Summary of the extracted places</div></div>
                    <div class="counter"><a href="#p37">37</a></div><div class="ptext" id="p37">In total, 735,062 unique locations were found in the DBpedia dump of August
                            2014.<a class="noteRef" href="#d20668e808">[17]</a> Only 709,357 of them have a corresponding English
                        label, leaving over 25,000 without a proper lexicalised form in this
                        language. This can be explained by the fact that English speakers do not
                        always find it useful to mention explicitly in a Wikipedia infobox (from
                        which DBpedia extracts structured information) that the term to refer to the
                        city of Ypres is “Ypres” for instance. In other words, a mapping from the
                        label "Ypres"@en to the URI dbr:Ypres may seem redundant but makes sense in
                        a multilingual perspective, taking non-native speakers into account.</div>
                    <div class="counter"><a href="#p38">38</a></div><div class="ptext" id="p38">The numbers of labels for Dutch and French are dramatically lower, 194,208
                        and 186,483 respectively. The explanation is similar: users of the English
                        Wikipedia/DBpedia seldom take time to encode labels in alternative
                        languages, while speakers from these other languages are often more keen to
                        fill information on their "own" language chapters (<a class="ref" href="http://nl.dbpedia.org" onclick="window.open('http://nl.dbpedia.org'); return false">http://nl.dbpedia.org</a> or <a class="ref" href="http://fr.dbpedia.org" onclick="window.open('http://fr.dbpedia.org'); return false">http://fr.dbpedia.org</a> for instance)
                        rather than perform this tedious work for the benefit of the English central
                        version. This state of affairs constitutes one of the major downside of the
                        current structure of DBpedia, which is simply replicated from Wikipedia
                        rather than organised in a language-independent manner. The overall number
                        of labels (857,911) is not equal to the sum of the individual languages but
                        a much lower number, since several labels were either replaced or suppressed
                        during the steps 4 and 5 described above.</div>
                    <div class="counter"><a href="#p39">39</a></div><div class="ptext" id="p39">At initialisation time, all the labels and URIs are loaded into a Python dict
                        (json-like dictionary) data structure, allowing instant lookup during the
                        spotting phase. After this last transformation, the data in memory look like
                        this:</div>
                    <div class="counter"><a href="#p40">40</a></div><div class="ptext" id="p40"><span class="monospace"> { </span></div>
                    <div class="counter"><a href="#p41">41</a></div><div class="ptext" id="p41"><span class="monospace">   "Afrique de Sud" :
                         "dbr:South_Africa",</span></div>
                    <div class="counter"><a href="#p42">42</a></div><div class="ptext" id="p42">
                        <span class="monospace">   "Andorre" :
                                   "dbr:Andorra",</span>
                    </div>
                    <div class="counter"><a href="#p43">43</a></div><div class="ptext" id="p43"><span class="monospace">   "Angola" :
                                    "dbr:Angola",</span>
                    </div>
                    <div class="counter"><a href="#p44">44</a></div><div class="ptext" id="p44"><span class="monospace">   "Arabie saoudite" : "dbr:Saudi_Arabia",</span></div>
                    <div class="counter"><a href="#p45">45</a></div><div class="ptext" id="p45"><span class="monospace"> } </span></div>
                    <div class="counter"><a href="#p46">46</a></div><div class="ptext" id="p46">At this stage, everything is in place to process textual content with
                        MERCKX.</div>
                </div>
                <div class="div div1">
                    <h2 class="head">4.3. Tokenizing, spotting, and annotating</h2>
                    <div class="counter"><a href="#p47">47</a></div><div class="ptext" id="p47">The next step is to tokenize the documents we want to enrich with the NLTK
                            WordPunctTokenizer<a class="noteRef" href="#d20668e868">[18]</a> and to perform a simple greedy lookup<a class="noteRef" href="#d20668e874">[19]</a> of entities up to three tokens in length. Tokens
                        shorter than three characters are ignored in order to reduce the noise they
                        are likely to induce, although this comes at the price of losing locations
                        like the municipality of Y in the Somme department.</div>
                    <div class="counter"><a href="#p48">48</a></div><div class="ptext" id="p48">For the entities present in the dictionary, the longest match is chosen and
                        annotated with its first and last characters, in addition to the
                        corresponding URI, thereby disambiguating these entities completely. For
                        instance, the expression "East Yorkshire" has a match in
                        DBpedia, and is therefore preferred to the shorter
                            "Yorkshire". It appears in the sample from character
                        626 to 640, and links to the URI <a class="ref" href="http://dbpedia.org/resource/East_Riding_of_Yorkshire" onclick="window.open('http://dbpedia.org/resource/East_Riding_of_Yorkshire'); return false">http://dbpedia.org/resource/East_Riding_of_Yorkshire</a>. This
                        corresponds to the format of the Entity Discovery and Linking track<a class="noteRef" href="#d20668e889">[20]</a> at the Text Analysis Conference.<a class="noteRef" href="#d20668e894">[21]</a> Once the URI is known, contextual
                        knowledge about the entities (such as the date of birth of people and the
                        geographic coordinates of a place, for instance) can be retrieved seamlessly
                        from the Linked Open Data cloud, enriching the original content.</div>
                </div>
            </div>
            <div class="div div0">
                <h1 class="head">5. Evaluation</h1>
                <div class="counter"><a href="#p49">49</a></div><div class="ptext" id="p49">In this section, we describe the methodology used in order to design the
                    reference corpus used for evaluation purposes (Section 5.1), before performing a
                    benchmarking of some related tools (Section 5.2) and providing the results
                    obtained along with a quantitative and qualitative analysis of errors (Section
                    5.3).</div>
                <div class="div div1">
                    <h2 class="head">5.1. Gold-standard corpus</h2>
                    <div class="counter"><a href="#p50">50</a></div><div class="ptext" id="p50">In order to compute the precision, recall and F-score of MERCKX, we needed a
                        manually constructed gold-standard corpus (GSC). In information science,
                        precision, recall and F-score are metrics intended to measure the
                        performance of a retrieval system. Precision is how <em class="emph">correct</em> a
                        retrieval system is: it determines whether results retrieved are relevant.
                        Recall measures the thoroughness of the system – by comparing how many
                        relevant items are in the corpus and how many are retrieved by the system,
                        it becomes possible to assess the performance of the system. The F-score is
                        the harmonic mean of precision and recall: it balances out the two
                        above-mentioned metrics into a single one – it measures a system’s
                        reliability. Although some GSC are available online for the evaluation of
                        entity linking, none of them is centred on digitised newspapers or the
                        cultural heritage sector. Making the same observation, Rodriquez et al.
                        built their own GSC for the evaluation of NER on raw OCR text, but using
                        very different data: testimonies and newsletters, which do not compare to
                        newspapers archives [<a class="ref" href="#rodriquez2012">Rodriquez et al. 2012</a>]. We therefore used a
                        sample from our own archival corpus and asked trained annotators to indicate
                        all valid places.</div>
                    <div class="div div2">
                        <h3 class="head">5.1.1. Sample selection</h3>
                        <div class="counter"><a href="#p51">51</a></div><div class="ptext" id="p51">Since the <cite class="title italic">Historische Kranten</cite> corpus
                            contains 1,028,555 articles, we calculated with the help of an online tool<a class="noteRef" href="#d20668e933">[22]</a> that a
                            sample of at least 96 articles was needed to reach a 95% confidence
                            level with a 10% confidence interval. This means that with 96 articles,
                            we are 95% certain that our sample is representative of the overall
                            corpus with a deviance of maximum 10%. The confidence interval is
                            actually much smaller (about 5%), since the probability of a word being
                            a location is not 50% but rather 2–3%. We therefore generated a random
                            sample of 100 documents, divided over the three languages proportionally
                            to the overall distribution: 49 French documents, 49 Dutch ones and 2
                            English ones.<a class="noteRef" href="#d20668e938">[23]</a> The documents range from 1831 to
                            1970, every decade being covered by at least two documents. We then
                            annotated all mentions of places manually with their positions in the
                            text (first and last character) and manually disambiguated them with
                            their corresponding DBpedia URIs, yielding a total of 662 locations in
                            the following format:</div>
                        <div class="counter"><a href="#p52">52</a></div><div class="ptext" id="p52"><span class="monospace">187   198   Bouvancourt</span></div>
                        <div class="counter"><a href="#p53">53</a></div><div class="ptext" id="p53"><span class="monospace">199   205   Fismes</span></div>
                        <div class="counter"><a href="#p54">54</a></div><div class="ptext" id="p54"><span class="monospace">561   565   Pévy</span></div>
                        <div class="counter"><a href="#p55">55</a></div><div class="ptext" id="p55"><span class="monospace">626   640   East Yorkshire</span></div>
                        <div class="counter"><a href="#p56">56</a></div><div class="ptext" id="p56"><span class="monospace">1076  1082  Trigny </span></div>
                        <div class="counter"><a href="#p57">57</a></div><div class="ptext" id="p57"><span class="monospace">1145  1151  Muizon</span></div>
                        <div class="counter"><a href="#p58">58</a></div><div class="ptext" id="p58"><span class="monospace">1200  1205  Vesle </span></div>
                        <div class="counter"><a href="#p59">59</a></div><div class="ptext" id="p59">The median number of locations by document is 4.5, ranging from 1 to 62.
                            Most places comprise only one word, but 38 of them contain two and 9
                            have three words or more. The annotation is partly subjective: one could
                            judge that the correct place is "Yorkshire" instead
                            of "East Yorkshire" for instance, every location
                            having five matching candidates in the dictionary on average. We thus
                            had to validate the list with extra annotators before using it as a
                            GSC.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">5.1.2. Cohen’s kappa</h3>
                        <div class="counter"><a href="#p60">60</a></div><div class="ptext" id="p60">The Cohen’s kappa coefficient measures inter-rater agreement on a scale
                            between 0 and 1, 0 being zero agreement and 1 total agreement [<a class="ref" href="#cohen1960">Cohen 1960</a>]. A value of K greater than .8 is generally
                            considered sufficiently reliable to draw sound conclusions based on the
                            annotation [<a class="ref" href="#carletta1996">Carletta 1996</a>]. The kappa is computed as
                            follows:</div>
                        <div class="figure">
                            
                            <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" alt=""/></a></div>
                        <div class="caption"><div class="label">Figure 1. </div>Pr(<em class="emph">a</em>) stands for the relative agreement between two
                                raters and Pr(<em class="emph">e</em>) for the probability of random
                                agreement.</div></div>
                        
                        <div class="counter"><a href="#p61">61</a></div><div class="ptext" id="p61">Our sample of 100 documents contained 30 186 tokens in total, spread over
                            the three languages. For each language, in addition to our own
                            annotation (A), an external annotator (B) was asked for every token to
                            decide whether it was part of a place name or not. Locations containing
                            OCR errors were accepted as long as the annotator could be reasonably
                            sure that it was a place name. Table 2 presents the raw annotation
                            counts, along with the kappa by language.</div>
                        <div class="table"><table class="table"><tr class="row label">
                                <td valign="top" class="cell">Lang.</td>
                                <td valign="top" class="cell">Both</td>
                                <td valign="top" class="cell">A</td>
                                <td valign="top" class="cell">B</td>
                                <td valign="top" class="cell">None</td>
                                <td valign="top" class="cell">Tot</td>
                                <td valign="top" class="cell">Pr(a)</td>
                                <td valign="top" class="cell">Pr(e)</td>
                                <td valign="top" class="cell"><em class="emph">K</em></td>
                            </tr><tr class="row">
                                <td valign="top" class="cell">EN</td>
                                <td valign="top" class="cell">20</td>
                                <td valign="top" class="cell">2</td>
                                <td valign="top" class="cell">2</td>
                                <td valign="top" class="cell">678</td>
                                <td valign="top" class="cell">702</td>
                                <td valign="top" class="cell">.994</td>
                                <td valign="top" class="cell">.939</td>
                                <td valign="top" class="cell">.906</td>
                            </tr><tr class="row">
                                <td valign="top" class="cell">FR</td>
                                <td valign="top" class="cell">197</td>
                                <td valign="top" class="cell">46</td>
                                <td valign="top" class="cell">8</td>
                                <td valign="top" class="cell">13422</td>
                                <td valign="top" class="cell">13673</td>
                                <td valign="top" class="cell">.996</td>
                                <td valign="top" class="cell">.968</td>
                                <td valign="top" class="cell">.877</td>
                            </tr><tr class="row">
                                <td valign="top" class="cell">NL</td>
                                <td valign="top" class="cell">384</td>
                                <td valign="top" class="cell">13</td>
                                <td valign="top" class="cell">27</td>
                                <td valign="top" class="cell">15387</td>
                                <td valign="top" class="cell">15811</td>
                                <td valign="top" class="cell">.997</td>
                                <td valign="top" class="cell">.950</td>
                                <td valign="top" class="cell">.949</td>
                            </tr></table><div class="caption"><div class="label">Table 3. </div>Cohen’s kappa for our GSC</div></div>
                        <div class="counter"><a href="#p62">62</a></div><div class="ptext" id="p62">The average kappa of .91 shows a high agreement that is largely
                            sufficient to consider the GSC reliable. This good score can be
                            explained in part by the relative straightforwardness of the annotation
                            task (LOC versus NON-LOC) compared to more complex ones involving
                            several types of entities, and in part by the detailed instructions
                            provided to the external annotators prior to the task. After some
                            corrections, insertions and deletions, we were left with 654 locations
                            that we mapped to their corresponding DBpedia resources, producing our
                            GSC in the TAC KBP/EDL format, which is slightly different from the one
                            used to annotate the sample:</div>
                        <div class="counter"><a href="#p63">63</a></div><div class="ptext" id="p63"><span class="monospace">gsc3.txt   187   198   Bouvancourt</span></div>
                        <div class="counter"><a href="#p64">64</a></div><div class="ptext" id="p64"><span class="monospace">gsc3.txt   199   205   Fismes</span></div>
                        <div class="counter"><a href="#p65">65</a></div><div class="ptext" id="p65"><span class="monospace">gsc3.txt   561   565   Pévy</span></div>
                        <div class="counter"><a href="#p66">66</a></div><div class="ptext" id="p66"><span class="monospace">gsc3.txt   626   640   East
                                Yorkshire</span></div>
                        <div class="counter"><a href="#p67">67</a></div><div class="ptext" id="p67"><span class="monospace">gsc3.txt   1076  1082  Trigny</span></div>
                        <div class="counter"><a href="#p68">68</a></div><div class="ptext" id="p68"><span class="monospace">gsc3.txt   1145  1151  Muizon</span></div>
                        <div class="counter"><a href="#p69">69</a></div><div class="ptext" id="p69"><span class="monospace">gsc3.txt   1200  1205  Vesle</span></div>
                        <div class="counter"><a href="#p70">70</a></div><div class="ptext" id="p70"> The impact of OCR quality on entity linking also needed to be evaluated.
                            To do so, we manually corrected the 120 places (out of 654) containing
                            OCR errors and produced a second reference. The original sample and both
                            GSC are also available on the GitHub page of the project.</div>
                    </div>
                </div>
                <div class="div div1">
                    <h2 class="head">5.2. Benchmarking</h2>
                    <div class="counter"><a href="#p71">71</a></div><div class="ptext" id="p71">To compare MERCKX to related systems presented in this section, we used the
                            <cite class="title italic">neleval</cite> tool<a class="noteRef" href="#d20668e1193">[24]</a> which is a
                        collection of Python evaluation scripts for the TAC<a class="noteRef" href="#d20668e1198">[25]</a> entity linking task and related
                        Wikification, named-entity disambiguation, and cross-document coreference
                        tasks. This utility allowed us to specify different GSC (raw OCR versus
                        corrected), systems (DBpedia Spotlight, Zemanta, Babelfy &amp; MERCKX), and
                        measures (simple entity match versus strong annotation match) to compare. </div>
                    <div class="counter"><a href="#p72">72</a></div><div class="ptext" id="p72">According to Ruiz and Poibeau "the E[ntity] L[inking] literature has stressed the
                                importance of evaluating systems on more than one measure"
                             [<a class="ref" href="#ruiz2015">Ruiz and Poibeau 2015</a>]. Following the evaluation framework made available by the authors,<a class="noteRef" href="#d20668e1214">[26]</a> we used
                        the distinction between simple entity match (ENT), i.e. without alignment,
                        and strong annotation match (SAM) which is stricter on entity boundaries : <blockquote><p>
                            SAM requires an annotation’s position to exactly
                                match the reference, besides requiring the entity annotated to match
                                the reference entity. ENT ignores positions and only evaluates
                                whether the entity proposed by the system matches the
                                reference.
                             [<a class="ref" href="#cornolti2013">Cornolti et al. 2013</a>]
                        </p></blockquote></div>
                    <div class="counter"><a href="#p73">73</a></div><div class="ptext" id="p73">Both measures will be used to evaluate our results in Section 5.3 in order to
                        get a more nuanced picture of what can be achieved by entity linking
                        tools.</div>
                    <div class="div div2">
                        <h3 class="head">5.2.1. DBpedia Spotlight</h3>
                        <div class="counter"><a href="#p74">74</a></div><div class="ptext" id="p74">DBpedia Spotlight<a class="noteRef" href="#d20668e1237">[27]</a> allows to find
                            entities in text and link them to DBpedia URIs. Interestingly, the
                            authors pay special attention to quality issues and fitness for use:
                                "DBpedia Spotlight allows users to configure the annotations to
                                their specific needs through the DBpedia Ontology and quality
                                measures such as prominence, topical pertinence, contextual
                                ambiguity and disambiguation confidence". The four stages of its
                            workflow are spotting, candidate selection, disambiguation, and
                            configuration (emphasis preseverd): <blockquote><p>
                                The <em class="emph">spotting</em> stage recognizes in a
                                    sentence the phrases that may indicate a mention of a DBpedia
                                    resource. <span class="hi italic">Candidate selection</span> is
                                    subsequently employed to map the spotted phrase to resources
                                    that are candidate disambiguations for that phrase. The <span class="hi italic">disambiguation</span> stage, in turn, uses the
                                    context around the spotted phrase to decide for the best choice
                                    amongst the candidates. The annotation can be customized by
                                    users to their specific needs through <span class="hi italic">configuration</span> parameters [ ...].
                                 [<a class="ref" href="#mendes2011">Mendes et al. 2011</a>]
                            </p></blockquote>
                        </div>
                        <div class="counter"><a href="#p75">75</a></div><div class="ptext" id="p75">However, the original Spotlight was designed for English only, as is the
                            case for many tools. To counterbalance this limitation, Daiber et al.
                            developed a new multilingual version of DBpedia Spotlight, which they
                            claim is faster, more accurate, and easier to configure [<a class="ref" href="#daiber2013">Daiber et al. 2013</a>]. This statistical version has been adopted
                            for the online demo.<a class="noteRef" href="#d20668e1270">[28]</a> In
                            addition to English, their language-independent model was tested on
                            seven other languages: Danish, French, German, Hungarian, Italian,
                            Russian, and Spanish. The authors reported accuracy scores for the
                            disambiguation task ranging from 68% to 83%.</div>
                        <div class="counter"><a href="#p76">76</a></div><div class="ptext" id="p76">For the spotting phase, the authors experiment with two methods: a
                            language-independent (data-driven) one based on gazetteers and a
                            language-dependent (rule-based) one relying on more heavy linguistic
                            processing using Apache OpenNLP models.<a class="noteRef" href="#d20668e1278">[29]</a> Surprisingly, the
                            language-dependent implementation does not improve the results
                            significantly: it only outperforms the language-independent
                            implementation by less than a percentage point.</div>
                        <div class="counter"><a href="#p77">77</a></div><div class="ptext" id="p77">The subsequent steps are also fully language-independent: candidate
                            selection is done by computing a score for each spot candidate as a
                            linear combination of features with an automated estimation of the
                            optimal cut-off threshold; disambiguation is performed by using the
                            probabilistic model proposed by Han and Sun [<a class="ref" href="#han2011">Han and Sun 2011</a>];
                            finally, configuration allows users to refine the results obtained by
                            setting their own confidence and relevance thresholds, these scores
                            being computed independently of the language.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">5.2.2. Zemanta</h3>
                        <div class="counter"><a href="#p78">78</a></div><div class="ptext" id="p78">Developed as a Web content enrichment platform, Zemanta<a class="noteRef" href="#d20668e1298">[30]</a> offers a NER API among other
                            services for bloggers. It gained worldwide attention in 2010 when an
                            evaluation campaign showed that it outperformed other state-of-the-art
                            systems for entity disambiguation,<a class="noteRef" href="#d20668e1303">[31]</a>
                            an assessment confirmed by later studies [<a class="ref" href="#vanhooland2015">van Hooland et al. 2015</a>]
                            [<a class="ref" href="#hengchen2015">Hengchen et al. 2015</a>]. Zemanta was subsequently integrated into
                            the NERD framework [<a class="ref" href="#rizzo2012">Rizzo and Troncy 2012</a>] and into the OpenRefine
                            NER extension.<a class="noteRef" href="#d20668e1316">[32]</a>
                            Zemanta requires an API key in order to use its services,<a class="noteRef" href="#d20668e1321">[33]</a> but the
                            webpage to apply for one seems to have been down for a long time,
                            preventing new users from registering (although older keys still work).
                            Despite the fact that it officially only supports English text, Zemanta
                            has proved in our own experience to work reasonably well on French and
                            Dutch.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">5.2.3. Babelfy</h3>
                        <div class="counter"><a href="#p79">79</a></div><div class="ptext" id="p79">Moro et al. introduce Babelfy,<a class="noteRef" href="#d20668e1335">[34]</a> a
                            system bridging entity linking and word sense disambiguation and based
                            on the BabelNet<a class="noteRef" href="#d20668e1340">[35]</a>
                            multilingual encyclopaedic dictionary and semantic network which is
                            constructed as a mash-up of Wikipedia and WordNet [<a class="ref" href="#moro2014">Moro et al. 2014</a>]. Aiming to bring together "the best of two
                                worlds", Babelfy also uses a graph-based approach but relies on
                            semantic signatures to select and disambiguate candidates. The use of
                            these dense subgraphs is very effective to collectively disambiguate
                            entities that would have proven almost impossible to identify
                            separately. Relying on a large-scale multilingual network, Babelfy
                            officially supports 267 languages, in addition to a language-agnostic
                            option.</div>
                    </div>
                </div>
                <div class="div div1">
                    <h2 class="head">5.3. Results</h2>
                    <div class="counter"><a href="#p80">80</a></div><div class="ptext" id="p80">Tables 3 and 4 present the results for simple entity match (ENT) and strong
                        annotation match (SAM) respectively, with the best figures indicated in
                        bold. MERCKX outperforms the three other systems evaluated, except for
                        precision where Zemanta scores best.<a class="noteRef" href="#d20668e1360">[36]</a> The columns marked
                            "Raw" show the results obtained on the original GSC,
                        while those marked "Corr" indicate scores obtained on
                        corrected OCR. Preliminary results of this experiment are presented in [<a class="ref" href="#dewilde2014">DeWilde 2015</a>].</div>
                    <div class="table"><table class="table"><tr class="row label">
                            <td valign="top" class="cell">System </td>
                            <td valign="top" class="cell">Precision</td>
                            <td valign="top" class="cell"/>
                            <td valign="top" class="cell">Recall</td>
                            <td valign="top" class="cell"/>
                            <td valign="top" class="cell">F-score</td>
                            <td valign="top" class="cell"/>
                        </tr><tr class="row">
                            <td valign="top" class="cell"/>
                            <td valign="top" class="cell">Raw</td>
                            <td valign="top" class="cell">Corr</td>
                            <td valign="top" class="cell">Raw</td>
                            <td valign="top" class="cell">Corr</td>
                            <td valign="top" class="cell">Raw</td>
                            <td valign="top" class="cell">Corr</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">Spotlight</td>
                            <td valign="top" class="cell">.466</td>
                            <td valign="top" class="cell">.468</td>
                            <td valign="top" class="cell">.192</td>
                            <td valign="top" class="cell">.207</td>
                            <td valign="top" class="cell">.272</td>
                            <td valign="top" class="cell">.287</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">Zemanta</td>
                            <td valign="top" class="cell label">.887</td>
                            <td valign="top" class="cell label">.898</td>
                            <td valign="top" class="cell">.333</td>
                            <td valign="top" class="cell">.371</td>
                            <td valign="top" class="cell">.485</td>
                            <td valign="top" class="cell">.525</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">Babelfy</td>
                            <td valign="top" class="cell">.656</td>
                            <td valign="top" class="cell">.688</td>
                            <td valign="top" class="cell">.376</td>
                            <td valign="top" class="cell">.446</td>
                            <td valign="top" class="cell">.478</td>
                            <td valign="top" class="cell">.541</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">MERCKX</td>
                            <td valign="top" class="cell">.712</td>
                            <td valign="top" class="cell">.744</td>
                            <td valign="top" class="cell label">.488</td>
                            <td valign="top" class="cell label">.559</td>
                            <td valign="top" class="cell label">.579</td>
                            <td valign="top" class="cell label">.638</td>
                        </tr></table><div class="caption"><div class="label">Table 4. </div>Simple entity match (ENT)</div></div>
                    <div class="table"><table class="table"><tr class="row label">
                            <td valign="top" class="cell">System</td>
                            <td valign="top" class="cell">Precision</td>
                            <td valign="top" class="cell"/>
                            <td valign="top" class="cell">Recall</td>
                            <td valign="top" class="cell"/>
                            <td valign="top" class="cell">F-score</td>
                            <td valign="top" class="cell"/>
                        </tr><tr class="row">
                            <td valign="top" class="cell"/>
                            <td valign="top" class="cell">Raw</td>
                            <td valign="top" class="cell">Corr</td>
                            <td valign="top" class="cell">Raw</td>
                            <td valign="top" class="cell">Corr</td>
                            <td valign="top" class="cell">Raw</td>
                            <td valign="top" class="cell">Corr</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">Spotlight</td>
                            <td valign="top" class="cell">.235</td>
                            <td valign="top" class="cell">.287</td>
                            <td valign="top" class="cell">.190</td>
                            <td valign="top" class="cell">.251</td>
                            <td valign="top" class="cell">.210</td>
                            <td valign="top" class="cell">.268</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">Zemanta</td>
                            <td valign="top" class="cell label">.867</td>
                            <td valign="top" class="cell label">.888</td>
                            <td valign="top" class="cell">.278</td>
                            <td valign="top" class="cell">.362</td>
                            <td valign="top" class="cell">.421</td>
                            <td valign="top" class="cell">.515</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">Babelfy</td>
                            <td valign="top" class="cell">.662</td>
                            <td valign="top" class="cell">.711</td>
                            <td valign="top" class="cell">.321</td>
                            <td valign="top" class="cell">.399</td>
                            <td valign="top" class="cell">.433</td>
                            <td valign="top" class="cell">.511</td>
                        </tr><tr class="row">
                            <td valign="top" class="cell">MERCKX</td>
                            <td valign="top" class="cell">.782</td>
                            <td valign="top" class="cell">.805</td>
                            <td valign="top" class="cell label">.443</td>
                            <td valign="top" class="cell label">.517</td>
                            <td valign="top" class="cell label">.566</td>
                            <td valign="top" class="cell label">.629</td>
                        </tr></table><div class="caption"><div class="label">Table 5. </div>Strong annotation match (SAM)</div></div>
                    <div class="div div2">
                        <h3 class="head">5.3.1 Quantitative analysis</h3>
                        <div class="counter"><a href="#p81">81</a></div><div class="ptext" id="p81">Precision is consistently ahead of recall, with Zemanta reaching scores
                            between 85% and 90%. The harder task of strong annotation match (taking
                            into account the exact position of each entity in the text) does not
                            affect precision: Babelfy and MERCKX actually improve on their scores,
                            although Spotlight’s precision is cut by a factor of 2. This can be
                            explained by recurrent entities that are correctly identified in all
                            cases. In contrast, all recall scores decrease when considered from the
                            SAM perspective. MERCKX outperforms other systems on recall, but it
                            peaks at 49% (ENT) and 44% (SAM) only.</div>
                        <div class="counter"><a href="#p82">82</a></div><div class="ptext" id="p82">Low recall scores under 50% can be explained by the multilingual context
                            and by the lack of coverage of DBpedia for some types of locations.
                            Whereas these would be unacceptable in a medical context where failing
                            to retrieve a document can have dramatic consequences, a better
                            precision is generally preferred in less critical applications. MERCKX
                            reaches a F-score just under 60%, a ten-point improvement on both
                            Zemanta and Babelfy which have F-scores under 50%. Spotlight fares
                            disappointingly, with F-scores around the 25% mark. Results on corrected
                            OCR (columns marked "Corr") will be discussed
                            separately in Section 5.3.3.</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">5.3.2. Qualitative analysis</h3>
                        <div class="counter"><a href="#p83">83</a></div><div class="ptext" id="p83">MERCKX is heavily dependent on the quality of DBpedia, on which it relies
                            for the disambiguation of entities. The errors of our system can be
                            grouped into three categories, following the typology of Makhoul et al.:
                            insertions, deletions, and substitutions [<a class="ref" href="#makhoul1999">Makhoul et al. 1999</a>].<a class="noteRef" href="#d20668e1705">[37]</a></div>
                        <div class="counter"><a href="#p84">84</a></div><div class="ptext" id="p84"><span class="hi bold">Insertions</span> (spurious entities or false acceptances)
                            are entities in the system output that do not align with any entity in
                            the reference. A common factor causing this is multilingual ambiguity.
                            The French adjective "tous", for instance, when
                            written with a capital "T", can be incorrectly mapped
                            to the town of dbr:Tous,_Valencia. The type check performed during the
                            construction of the dictionary normally avoids such cases, but some
                            problems can remain when a disambiguation page is missing: in this case,
                            the French resource dbpedia-fr:Tous also points to the Spanish city,
                            with no reference to the adjective. Another frequent mistake occurs when
                            places are mentioned in the name of streets. For instance, the
                                "rue de Lille" in Ypres does not really refer to
                            the French city of Lille, and should therefore not be disambiguated with
                            dbr:Lille. A more elaborate algorithm could try to detect such cases in
                            order to exclude them, but it would be difficult to implement it in a
                            language-independent manner without explicitly blacklisting words such
                            as "rue", "straat",
                                "street", etc.</div>
                        <div class="counter"><a href="#p85">85</a></div><div class="ptext" id="p85"><span class="hi bold">Deletions</span> (missing entities or false rejections) are
                            entities in the reference that do not align with any entity in the
                            system output. One of the main causes for this is the absence of the
                            dbo:Place RDF type in the resource of a location. For instance,
                            dbr:East_Riding_of_Yorkshire is described as a owl:Thing which is very
                            general and therefore not helpful. However, it is also tagged as a
                            yago:YagoGeoEntity which is more precise. Using multiple types instead
                            of just dbo:Place could improve the recall. Another cause is the absence
                            of a particular label (e.g. when an old spelling is used). The resource
                            dbr:Reims, for instance, does not include a label
                                "Rheims" in any of the three languages used.
                            However, the resource dbr:Rheims does exist and redirects to dbr:Reims.
                            Including redirections in addition to labels could also help to limit
                            the number of missing entities.<a class="noteRef" href="#d20668e1739">[38]</a></div>
                        <div class="counter"><a href="#p86">86</a></div><div class="ptext" id="p86"><span class="hi bold">Substitutions</span> (incorrect entities) are entities in
                            the system output that do align with entities in the reference but are
                            scored as incorrect. These cases are far more rare than insertions and
                            deletions. Substitutions can be due to the wrong detection of entity
                            boundaries: "Jette" instead of
                                "Jette-Saint-Pierre",
                                "Flanders" instead of
                                "West-Flanders". The greedy lookup mechanism of
                            MERCKX normally prevents that, but extra spaces ("West-
                                Flanders") or long entities
                                ("Jette-Saint-Pierre" contains five tokens
                            because hyphens are tokenized separately) can prove tricky. Another
                            possibility is the attribution of a wrong URI when two places have the
                            same name. No case was detected in our system, but the output of DBpedia
                            Spotlight contains an occurrence of this type of mistake: mapping
                                "Vitry" to "Vitry-le-François"
                            instead of "Vitry-sur-Seine".</div>
                    </div>
                    <div class="div div2">
                        <h3 class="head">5.3.3. Impact of OCR</h3>
                        <div class="counter"><a href="#p87">87</a></div><div class="ptext" id="p87">In similar work on Holocaust testimonies, Rodriquez et al. found that "manual correction of OCR output does not
                                    significantly improve the performance of named-entity
                                    extraction"
                                 [<a class="ref" href="#rodriquez2012">Rodriquez et al. 2012</a>]. In other words, even poorly digitized material with OCR mistakes
                            could be successfully enriched to meet the needs of users. The
                            confirmation of these findings would mean a lot to institutions that
                            lack the funding to perform first-rate OCR on their collections or the
                            manpower to curate them manually.</div>
                        <div class="counter"><a href="#p88">88</a></div><div class="ptext" id="p88">However, contrary to this study, we see that OCR correction improves the
                            results of all systems. Precision goes up by 1 to 3% on ENT and 5% on
                            SAM in the case of Babelfy. Recall improvement reaches 7% on ENT and
                            over 8% on SAM for Zemanta. Accordingly, F-scores get improved by up to
                            6% on the corrected version, with MERCKX crossing the 60% mark on both
                            ENT and SAM. This state of affairs can be explained by a number of
                            factors. First, the quality of the OCR seems to be much worse in the
                            case of the <cite class="title italic">Historische Kranten</cite> corpus than
                            in the testimonies used for their study: the authors report a word
                            accuracy of 88.6% and a character accuracy of 93.0%, whereas in the case
                            of our sample these scores were somewhat lower: 81.7% (word accuracy on
                            places only) and 85.2% (character accuracy). The overall word accuracy,
                            tested on a subset of the sample, was much lower still: a mere 68.3%.
                            Secondly, the entity linking task is harder than simple named-entity
                            recognition: full disambiguation with an URI is more prone to suffer
                            from OCR mistakes. Using a fuzzy matching algorithm such as the
                            Levenshtein distance could help increase the results without needing
                            manual correction of the OCR. Preliminary experiments with this
                            algorithm indicate that it could lead to an improvement of about 5%
                            F-score, bringing MERCKX close enough to the performance achieved on the
                            corrected version of the sample, although this would come at the expense
                            of efficiency since the Levenshtein distance has an exponential time
                            complexity.</div>
                    </div>
                </div>
            </div>
            <div class="div div0">
                <h1 class="head">6. Conclusion and Future Work</h1>
                <div class="counter"><a href="#p89">89</a></div><div class="ptext" id="p89">In this paper, we have shown how named-entity recognition and entity linking
                    could easily be adopted by LAM in order to semantically enrich their
                    collections. Compared to existing NER tools, the approach of MERCKX is to take
                    advantage of available Linked Data resources to perform a full disambiguation of
                    entities. This also allows to handle multiple languages seamlessly, although it
                    comes at the price of losing some precisions in case of multilingual
                    ambiguity.</div>
                <div class="counter"><a href="#p90">90</a></div><div class="ptext" id="p90">A fully language-independent system would obviously need to incorporate labels
                    from multiple knowledge bases, relying on a fallback mechanism when an entity
                    does not exist in a specific language. To address the issue of low recall, one
                    could experiment will the combination of several knowledge bases instead of
                    DBpedia only. For place names, the aggregation of GeoNames<a class="noteRef" href="#d20668e1815">[39]</a>
                    and GeoVocab<a class="noteRef" href="#d20668e1820">[40]</a> looks
                    promising. By linking historical locations to their corresponding URIs (and, by
                    extent, coordinates), we allow the semi-automatic creation of maps and other
                    visualisations of the dataset – other entry points to the data than the
                    traditional close reading approach. The system should also be tested with other
                    corpora, and steps have been taken forward in collaborating with other cultural
                    heritage institutions, namely the AMSAB-ISG<a class="noteRef" href="#d20668e1825">[41]</a> – which holds a vast collection of Flemish
                    socialist newspapers.</div>
                <div class="counter"><a href="#p91">91</a></div><div class="ptext" id="p91">We are also experimenting with topic modelling [<a class="ref" href="#blei2003">Blei et al. 2003</a>] – a task
                    which has gained momentum in the last years, [<a class="ref" href="#newman2007">Newman et al. 2007</a>]– to
                    further optimise the end users’ search experience. By discovering latent topics
                    in the dataset, disambiguating the topics with DBpedia concepts and grouping
                    related news articles together – thus allowing faceted search capabilities –, we
                    intend to suggest users with results related to their original queries. Using
                    topic modelling to extract keywords and then harvesting the multilingual
                    characteristics of Linked Data [<a class="ref" href="#hengchen2016">Hengchen et al. 2016</a>] we hope to further
                    improve cross-lingual search capabilities. We will then apply this methodology
                    to other collections in order to further demonstrate the added value of natural
                    language processing techniques in the context of Digital Humanities projects
                    undertaken by cultural heritage institutions.</div>
            </div>
            <div class="div div0">
                <h1 class="head">Acknowledgements</h1>
                <div class="counter"><a href="#p92">92</a></div><div class="ptext" id="p92">Simon Hengchen was supported by CENDARI<a class="noteRef" href="#d20668e1849">[42]</a>
                    during the course of this research (funded by the European Union’s Seventh
                    Framework Programme [FP72007-2013] under grant agreement n° 284432). The
                    material used was provided courtesy of the Ypres City Archive and the CO7
                    Heritage Cell.</div>
            </div>
        
        
            
        
    </div>
<div id="notes"><h2>Notes</h2><div class="endnote" id="d20668e217"><span class="noteRef">[1]</span> Semantic enrichment is the process of adding an extra
                        layer of metadata to existing collections.</div><div class="endnote" id="d20668e228"><span class="noteRef">[2]</span>
                        <a class="ref" href="http://www.ehri-project.eu/" onclick="window.open('http://www.ehri-project.eu/'); return false">http://www.ehri-project.eu/</a></div><div class="endnote" id="d20668e233"><span class="noteRef">[3]</span>
                        <a class="ref" href="http://www.cendari.eu/" onclick="window.open('http://www.cendari.eu/'); return false">http://www.cendari.eu/</a></div><div class="endnote" id="d20668e238"><span class="noteRef">[4]</span>
                        <a class="ref" href="http://www.europeana-newspapers.eu/" onclick="window.open('http://www.europeana-newspapers.eu/'); return false">http://www.europeana-newspapers.eu/</a></div><div class="endnote" id="d20668e244"><span class="noteRef">[5]</span>
                        <a class="ref" href="https://github.com/europeananewspapers/ner-corpora" onclick="window.open('https://github.com/europeananewspapers/ner-corpora'); return false">https://github.com/europeananewspapers/ner-corpora</a></div><div class="endnote" id="d20668e255"><span class="noteRef">[6]</span>
                        <a class="ref" href="http://cordis.europa.eu/project/rcn/95552_en.html" onclick="window.open('http://cordis.europa.eu/project/rcn/95552_en.html'); return false">
                            http://cordis.europa.eu/project/rcn/95552_en.html</a></div><div class="endnote" id="d20668e266"><span class="noteRef">[7]</span><a class="ref" href="http://www.freshandnew.org/2008/03/opac20-opencalais-meets-our-museum-collection-auto-tagging-and-semantic-parsing-of-collection-data/" onclick="window.open('http://www.freshandnew.org/2008/03/opac20-opencalais-meets-our-museum-collection-auto-tagging-and-semantic-parsing-of-collection-data/'); return false">http://www.freshandnew.org/2008/03/opac20-opencalais-meets-our-museum-collection-auto-tagging-and-semantic-parsing-of-collection-data/</a></div><div class="endnote" id="d20668e284"><span class="noteRef">[8]</span>
                        <a class="ref" href="http://freeyourmetadata.org/" onclick="window.open('http://freeyourmetadata.org/'); return false">http://freeyourmetadata.org/</a></div><div class="endnote" id="d20668e315"><span class="noteRef">[9]</span>
                        <a class="ref" href="http://data.bnf.fr/semanticweb" onclick="window.open('http://data.bnf.fr/semanticweb'); return false">http://data.bnf.fr/semanticweb</a></div><div class="endnote" id="d20668e334"><span class="noteRef">[10]</span>
                        <a class="ref" href="http://blog.kbresearch.nl/2014/03/03/ner-newspapers/" onclick="window.open('http://blog.kbresearch.nl/2014/03/03/ner-newspapers/'); return false">http://blog.kbresearch.nl/2014/03/03/ner-newspapers/</a> reported on
                        a preliminary experiment on Dutch, French and German.</div><div class="endnote" id="d20668e340"><span class="noteRef">[11]</span>
                        <a class="ref" href="http://www.theeuropeanlibrary.org/" onclick="window.open('http://www.theeuropeanlibrary.org/'); return false">http://www.theeuropeanlibrary.org/</a></div><div class="endnote" id="d20668e356"><span class="noteRef">[12]</span>
                        <a class="ref" href="http://www.historischekranten.be/" onclick="window.open('http://www.historischekranten.be/'); return false">http://www.historischekranten.be/</a></div><div class="endnote" id="d20668e361"><span class="noteRef">[13]</span>
                        <a class="ref" href="http://erfgoedcelco7.be/" onclick="window.open('http://erfgoedcelco7.be/'); return false">http://erfgoedcelco7.be/</a></div><div class="endnote" id="d20668e366"><span class="noteRef">[14]</span>
                        <a class="ref" href="https://www.maddlain.iminds.be" onclick="window.open('https://www.maddlain.iminds.be'); return false">https://www.maddlain.iminds.be</a></div><div class="endnote" id="d20668e626"><span class="noteRef">[15]</span> The source code of
                            MERCKX and related material used in this paper is available at <a class="ref" href="https://github.com/madewild/MERCKX" onclick="window.open('https://github.com/madewild/MERCKX'); return false">https://github.com/madewild/MERCKX</a>.</div><div class="endnote" id="d20668e757"><span class="noteRef">[16]</span> Another option
                                would have been to keep multiple meanings in parallel, but this is
                                currently incompatible with the design of MERCKX.</div><div class="endnote" id="d20668e808"><span class="noteRef">[17]</span> Note that this number is constantly fluctuating: as of
                            August 2015, the figure has decreased to 725,546, which means that
                            almost 10,000 places have been suppressed from DBpedia over the course
                            of a year.</div><div class="endnote" id="d20668e868"><span class="noteRef">[18]</span> See <a class="ref" href="http://www.nltk.org/api/nltk.tokenize.html" onclick="window.open('http://www.nltk.org/api/nltk.tokenize.html'); return false">http://www.nltk.org/api/nltk.tokenize.html</a> for details about
                            how it works.</div><div class="endnote" id="d20668e874"><span class="noteRef">[19]</span> A
                            greedy algorithm always takes the best immediate solution available at
                            each stage.</div><div class="endnote" id="d20668e889"><span class="noteRef">[20]</span>
                            <a class="ref" href="http://nlp.cs.rpi.edu/kbp/2015/" onclick="window.open('http://nlp.cs.rpi.edu/kbp/2015/'); return false">http://nlp.cs.rpi.edu/kbp/2015/</a></div><div class="endnote" id="d20668e894"><span class="noteRef">[21]</span>
                            <a class="ref" href="http://www.nist.gov/tac/" onclick="window.open('http://www.nist.gov/tac/'); return false">http://www.nist.gov/tac/</a></div><div class="endnote" id="d20668e933"><span class="noteRef">[22]</span>
                                <a class="ref" href="http://www.surveysystem.com/sscalc.htm" onclick="window.open('http://www.surveysystem.com/sscalc.htm'); return false">http://www.surveysystem.com/sscalc.htm</a></div><div class="endnote" id="d20668e938"><span class="noteRef">[23]</span> Documents in the sample contain 1430 characters on
                                average, which is comparable to the corpus used by [<a class="ref" href="#milne2008">Milne and Witten 2008</a>].</div><div class="endnote" id="d20668e1193"><span class="noteRef">[24]</span>
                            <a class="ref" href="http://www.surveysystem.com/sscalc.htm" onclick="window.open('http://www.surveysystem.com/sscalc.htm'); return false">https://github.com/wikilinks/neleval</a></div><div class="endnote" id="d20668e1198"><span class="noteRef">[25]</span>
                            <a class="ref" href="http://www.nist.gov/tac/" onclick="window.open('http://www.nist.gov/tac/'); return false">http://www.nist.gov/tac/</a></div><div class="endnote" id="d20668e1214"><span class="noteRef">[26]</span>
                            <a class="ref" href="https://sites.google.com/site/entitylinking1" onclick="window.open('https://sites.google.com/site/entitylinking1'); return false">https://sites.google.com/site/entitylinking1</a></div><div class="endnote" id="d20668e1237"><span class="noteRef">[27]</span>
                                <a class="ref" href="http://spotlight.dbpedia.org/" onclick="window.open('http://spotlight.dbpedia.org/'); return false">http://spotlight.dbpedia.org/</a></div><div class="endnote" id="d20668e1270"><span class="noteRef">[28]</span>
                                <a class="ref" href="http://dbpedia-spotlight.github.io/demo/" onclick="window.open('http://dbpedia-spotlight.github.io/demo/'); return false">http://dbpedia-spotlight.github.io/demo/</a></div><div class="endnote" id="d20668e1278"><span class="noteRef">[29]</span>
                                <a class="ref" href="https://opennlp.apache.org/" onclick="window.open('https://opennlp.apache.org/'); return false">https://opennlp.apache.org/</a></div><div class="endnote" id="d20668e1298"><span class="noteRef">[30]</span>
                                <a class="ref" href="http://www.zemanta.com/" onclick="window.open('http://www.zemanta.com/'); return false">http://www.zemanta.com/</a></div><div class="endnote" id="d20668e1303"><span class="noteRef">[31]</span> See this blog post for a
                                detailed report on the Entity Extraction &amp; Content API
                                Evaluation: <a class="ref" href="http://blog.viewchange.org/2010/05/entity-extraction-content-api-evaluation/" onclick="window.open('http://blog.viewchange.org/2010/05/entity-extraction-content-api-evaluation/'); return false">http://blog.viewchange.org/2010/05/entity-extraction-content-api-evaluation/</a>.</div><div class="endnote" id="d20668e1316"><span class="noteRef">[32]</span>
                                <a class="ref" href="http://freeyourmetadata.org/named-entity-extraction/" onclick="window.open('http://freeyourmetadata.org/named-entity-extraction/'); return false">http://freeyourmetadata.org/named-entity-extraction/</a></div><div class="endnote" id="d20668e1321"><span class="noteRef">[33]</span>
                                <a class="ref" href="http://papi.zemanta.com/services/rest/0.0/" onclick="window.open('http://papi.zemanta.com/services/rest/0.0/'); return false">http://papi.zemanta.com/services/rest/0.0/</a></div><div class="endnote" id="d20668e1335"><span class="noteRef">[34]</span>
                                <a class="ref" href="http://babelfy.org/" onclick="window.open('http://babelfy.org/'); return false">http://babelfy.org/</a></div><div class="endnote" id="d20668e1340"><span class="noteRef">[35]</span>
                                <a class="ref" href="http://babelnet.org/" onclick="window.open('http://babelnet.org/'); return false">http://babelnet.org/</a></div><div class="endnote" id="d20668e1360"><span class="noteRef">[36]</span> Consistently with results
                            reported by Rizzo and Troncy [<a class="ref" href="#rizzo2011">Rizzo and Troncy 2011</a>]. Since Zemanta
                            operates as a black box, it is difficult to learn from it in order to
                            improve precision. Our guess is that it simply uses a higher confidence
                            threshold, at the expense of recall.</div><div class="endnote" id="d20668e1705"><span class="noteRef">[37]</span> To give a rough idea, Zemanta suffers from 9 insertions
                                and 134 deletions on the corrected GSC with ENT measure; in
                                comparison, MERCKX has 41 insertions but only 94 deletions
                                (substitutions being close to nil).</div><div class="endnote" id="d20668e1739"><span class="noteRef">[38]</span> Although the risk is then to
                                introduce more noise: dbr:Cette, for instance, redirects to dbr:Sète
                                because the spelling of the French town changed in 1927. The danger
                                of confusion with the French determiner <em class="term">cette</em> is
                                obvious.</div><div class="endnote" id="d20668e1815"><span class="noteRef">[39]</span>
                        <a class="ref" href="http://www.geonames.org/" onclick="window.open('http://www.geonames.org/'); return false">http://www.geonames.org/</a></div><div class="endnote" id="d20668e1820"><span class="noteRef">[40]</span>
                        <a class="ref" href="http://geovocab.org/" onclick="window.open('http://geovocab.org/'); return false">http://geovocab.org/</a></div><div class="endnote" id="d20668e1825"><span class="noteRef">[41]</span>
                        <a class="ref" href="http://www.amsab-isg.be" onclick="window.open('http://www.amsab-isg.be'); return false">http://www.amsab-isg.be</a>. The
                        AMSAB-ISG is a cultural heritage centre that focuses on social, humanitarian
                        and ecological movements.</div><div class="endnote" id="d20668e1849"><span class="noteRef">[42]</span>
                        <a class="ref" href="http://www.cendari.eu/" onclick="window.open('http://www.cendari.eu/'); return false">http://www.cendari.eu/</a></div></div><div id="worksCited"><h2>Works Cited</h2><div class="bibl"><span class="ref" id="agirre2012"><!-- close -->Agirre et al. 2012</span> Agirre, E., Barrena, A., De
                    Lacalle, O. L., Soroa, A., Fernando, S., and Stevenson, M. (2012). "Matching Cultural Heritage Items to Wikipedia." In
                        <cite class="title italic">Proceedings of the 8th International Conference on
                        Language Resources and Evaluation (LREC)</cite>, pages 1729–1735.</div><div class="bibl"><span class="ref" id="bingel2014"><!-- close -->Bingel and Haider 2014</span> Bingel, J. and Haider, T.
                    (2014). "Named Entity Tagging a Very Large Unbalanced
                        Corpus: Training and Evaluating NE Classifiers." In <cite class="title italic">Proceedings of the 9th International Conference on Language
                        Resources and Evaluation (LREC)</cite>, Reykjavik, Iceland.</div><div class="bibl"><span class="ref" id="bizer2009"><!-- close -->Bizer et al. 2009</span> Bizer, C., Lehmann, J.,
                    Kobilarov, G., Auer, S., Becker, C., Cyganiak, R., and Hellmann, S. (2009).
                        "DBpedia – A Crystallization Point for the Web of
                        Data."
                    <cite class="title italic">Web Semantics: Science, Services and Agents on the World
                        Wide Web</cite>, 7(3):154–165.</div><div class="bibl"><span class="ref" id="blanke2013"><!-- close -->Blanke and Kristel 2013</span> Blanke, T. and Kristel, C.
                    (2013). "Integrating Holocaust Research."
                    <cite class="title italic">International Journal of Humanities and Arts
                        Computing</cite>, 7(1–2):41–57.</div><div class="bibl"><span class="ref" id="blei2003"><!-- close -->Blei et al. 2003</span> Blei, D.M., Ng, A. Y., and Jordan,
                    M. I. (2003). "Latent dirichlet allocation."
                    <cite class="title italic">The Journal of Machine Learning Research</cite>,
                    3:993–1022.</div><div class="bibl"><span class="ref" id="carletta1996"><!-- close -->Carletta 1996</span> Carletta, J. (1996). "Assessing Agreement on Classification Tasks: The Kappa
                        Statistic."
                    <cite class="title italic">Computational Linguistics</cite>, 22(2):249–254.</div><div class="bibl"><span class="ref" id="cohen1960"><!-- close -->Cohen 1960</span> Cohen, J. (1960). "A
                        Coefficient of Agreement for Nominal Scales."
                    <cite class="title italic">Educational and Psychological Measurement</cite>,
                    20(1):37–46.</div><div class="bibl"><span class="ref" id="cornolti2013"><!-- close -->Cornolti et al. 2013</span> Cornolti, M., Ferragina,
                    P., and Ciaramita, M. (2013). "A Framework for Benchmarking
                        Entity-Annotation Systems." In <cite class="title italic">Proceedings of
                        the 22nd International Conference on theWorldWideWeb</cite>, pages
                    249–260.</div><div class="bibl"><span class="ref" id="daiber2013"><!-- close -->Daiber et al. 2013</span> Daiber, J., Jakob, M., Hokamp,
                    C., and Mendes, P. N. (2013). "Improving Efficiency and
                        Accuracy in Multilingual Entity Extraction." In <cite class="title italic">Proceedings of the 9th International Conference on Semantic
                    Systems</cite>, pages 121–124. ACM.</div><div class="bibl"><span class="ref" id="delozier2016"><!-- close -->DeLozier et al. 2016</span> DeLozier, G., Wing, B.,
                    Baldridge, J., and Nesbit, S. (2016). "Creating a novel
                        geolocation corpus from historical texts." In <cite class="title italic">Proceedings of LAW X-The 10th Linguistic Annotation Workshop</cite>, pages
                    188–198.</div><div class="bibl"><span class="ref" id="dewilde2014"><!-- close -->DeWilde 2015</span> De Wilde, M. (2015). "Improving Retrieval of Historical Content with Entity
                        Linking." In <cite class="title italic">New Trends in Databases and
                        Information Systems</cite>, Volume 539 of <span class="hi italic">Communications
                        in Computer and Information Science</span>, pages 498–504. Springer.</div><div class="bibl"><span class="ref" id="fernando2012"><!-- close -->Fernando and Stevenson 2012</span> Fernando, S. and
                    Stevenson, M. (2012). "Adapting wikification to cultural
                        heritage." In <cite class="title italic">Proceedings of the 6th Workshop on
                        Language Technology for Cultural Heritage, Social Sciences, and
                        Humanities</cite>, pages 101–106. ACL.</div><div class="bibl"><span class="ref" id="frontini2015"><!-- close -->Frontini et al. 2015</span> Frontini, F., Brando, C.,
                    andGanascia, J.-G. (2015). "SemanticWeb BasedNamed Entity
                        Linking for Digital Humanities and Heritage Texts." In <cite class="title italic">Proceedings of the 1st International Workshop on Semantic Web
                        for Scientific Heritage at the 12th ESWC 2015 Conference</cite>, pages
                    77–88, Portorož, Slovenia.</div><div class="bibl"><span class="ref" id="han2011"><!-- close -->Han and Sun 2011</span> Han, X. and Sun, L. (2011). "A Generative Entity-Mention Model for Linking Entities with
                        Knowledge Base." In <cite class="title italic">Proceedings of the 49th
                        Annual Meeting of the ACL: Human Language Technologies</cite>, volume 1,
                    pages 945–954, Portland, OR, USA.</div><div class="bibl"><span class="ref" id="hengchen2015"><!-- close -->Hengchen et al. 2015</span> Hengchen, S., van Hooland,
                    S., Verborgh, R., and De Wilde, M. (2015). <span class="foreign i">"L’extraction d’entités nommées: une opportunité pour le
                            secteur culturel?"</span>
                    <cite class="title italic">Information, données &amp; documents</cite>,
                    52(2):70–79.</div><div class="bibl"><span class="ref" id="hengchen2016"><!-- close -->Hengchen et al. 2016</span> Hengchen, S., Coeckelbergs,
                    M., van Hooland, S., Verborgh, R., and Steiner, T. (2016). "Exploring archives with probabilistic models: Topic modelling for the
                        valorisation of digitised archives of the European Commission." In
                        <cite class="title italic">First Workshop “Computational Archival Science: digital
                        records in the age of big data”,Washington DC</cite>, volume 1.</div><div class="bibl"><span class="ref" id="kripke1982"><!-- close -->Kripke 1982</span> Kripke, S. (1982). <cite class="title italic">Naming and Necessity</cite>. Harvard University Press,
                    Cambridge, MA, USA.</div><div class="bibl"><span class="ref" id="kupietz2010"><!-- close -->Kupietz et al. 2010</span> Kupietz, M., Belica, C.,
                    Keibel, H., and Witt, A. (2010). "The German Reference
                        Corpus DeReKo: A Primordial Sample for Linguistic Research." In
                        <cite class="title italic">Proceedings of the 7th International Conference on
                        Language Resources and Evaluation (LREC)</cite>, Valletta, Malta.</div><div class="bibl"><span class="ref" id="leidner2007"><!-- close -->Leidner 2007</span> Leidner, J. L. (2007). "Toponym resolution in text: annotation, evaluation and
                        applications of spatial grounding." In <cite class="title italic">ACM SIGIR
                        Forum</cite>, volume 41, pages 124–126. ACM.</div><div class="bibl"><span class="ref" id="lin2010"><!-- close -->Lin et al. 2010</span> Lin, Y., Ahn, J.-W., Brusilovsky, P.,
                    He, D., and Real, W. (2010). "ImageSieve: Exploratory Search
                        of Museum Archives with Named Entity-Based Faceted Browsing."
                    <cite class="title italic">Proceedings of the American Society for Information Science
                        and Technology</cite>, 47(1):1–10.</div><div class="bibl"><span class="ref" id="makhoul1999"><!-- close -->Makhoul et al. 1999</span> Makhoul, J., Kubala, F.,
                    Schwartz, R., and Weischedel, R. (1999). "Performance
                        Measures for Information Extraction." In <cite class="title italic">Proceedings of the DARPA Broadcast News Transcription and Understanding
                        Workshop</cite>, pages 249–252.</div><div class="bibl"><span class="ref" id="maturana2013"><!-- close -->Maturana et al. 2013</span> Maturana, R. A., Ortega,
                    M., Alvarado, M. E., López-Sola, S., and Ibáñez, M. J. (2013). "Mismuseos.net: Art After Technology. Putting Cultural Data
                        toWork in a Linked Data Platform." LinkedUp Veni Challenge.</div><div class="bibl"><span class="ref" id="mendes2011"><!-- close -->Mendes et al. 2011</span> Mendes, P. N., Jakob, M.,
                    García-Silva, A., and Bizer, C. (2011). "DBpedia Spotlight:
                        Shedding Light on theWeb of Documents." In <cite class="title italic">Proceedings of the 7th International Conference on Semantic
                    Systems</cite>, pages 1–8, Graz, Austria.</div><div class="bibl"><span class="ref" id="milne2008"><!-- close -->Milne and Witten 2008</span> Milne, D. and Witten, I. H.
                    (2008). "Learning to Link with Wikipedia." In <cite class="title italic">Proceedings of the 17th ACM Conference on Information and
                        Knowledge Management</cite>, pages 509–518, Napa Valley, CA, USA.</div><div class="bibl"><span class="ref" id="moro2014"><!-- close -->Moro et al. 2014</span> Moro, A., Raganato, A., and
                    Navigli, R. (2014). "Entity Linking Meets Word Sense
                        Disambiguation: A Unified Approach."
                    <cite class="title italic">Transactions of the ACL</cite>, 2.</div><div class="bibl"><span class="ref" id="newman2007"><!-- close -->Newman et al. 2007</span>  Newman, D., Hagedorn, K.,
                    Chemudugunta, C., and Smyth, P. (2007). "Subject metadata
                        enrichment using statistical topic models." In <cite class="title italic">Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital
                        Libraries</cite>, JCDL ’07, pages 366–375, New York, NY, USA. ACM.</div><div class="bibl"><span class="ref" id="raimond2013"><!-- close -->Raimond et al. 2013</span> Raimond, Y., Smethurst,
                    M.,McParland, A., and Lowis, C. (2013). "Using the Past to
                        Explain the Present: Interlinking Current Affairs with Archives via the
                        Semantic Web." In <cite class="title italic">The Semantic Web – ISWC
                        2013</cite>, pages 146–161. Springer.</div><div class="bibl"><span class="ref" id="rizzo2011"><!-- close -->Rizzo and Troncy 2011</span> Rizzo, G. and Troncy, R.
                    (2011). "NERD: Evaluating Named Entity Recognition Tools in
                        the Web of Data." In <cite class="title italic">Proceedings of the 1st
                        Workshop on Web Scale Knowledge Extraction (WEKEX)</cite>, Bonn,
                    Germany.</div><div class="bibl"><span class="ref" id="rizzo2012"><!-- close -->Rizzo and Troncy 2012</span> Rizzo, G. and Troncy, R.
                    (2012). "NERD: a Framework for Unifying Named Entity
                        Recognition and Disambiguation Extraction Tools." In <cite class="title italic">Proceedings of the Demonstrations at the 13th Conference of
                        the European Chapter of the ACL</cite>, pages 73–76. ACL.</div><div class="bibl"><span class="ref" id="rodriquez2012"><!-- close -->Rodriquez et al. 2012</span> Rodriquez, K. J., Bryant,
                    M., Blanke, T., and Luszczynska, M. (2012). "Comparison of
                        Named Entity Recognition Tools for Raw OCR Text." In <cite class="title italic">Proceedings of KONVENS 2012</cite>, pages 410–414.
                    Vienna.</div><div class="bibl"><span class="ref" id="ruiz2015"><!-- close -->Ruiz and Poibeau 2015</span> Ruiz, P. and Poibeau, T.
                    (2015). "Combining Open Source Annotators for Entity Linking
                        through Weighted Voting." In <cite class="title italic">Proceedings of the
                        4th Joint Conference on Lexical and Computational Semantics (*SEM)</cite>,
                    Denver, CO, USA.</div><div class="bibl"><span class="ref" id="segers2011"><!-- close -->Segers et al. 2011</span> Segers, R., van Erp, M., van
                    der Meij, L., Aroyo, L., Schreiber, G., Wielinga, B., van Ossenbruggen, J.,
                    Oomen, J., and Jacobs, G. (2011). "Hacking History:
                        Automatic Historical Event Extraction for Enriching Cultural Heritage
                        Multimedia Collections." In <cite class="title italic">Proceedings of the
                        6th International Conference on Knowledge Capture (K-CAP)</cite>, Banff,
                    Alberta, Canada.</div><div class="bibl"><span class="ref" id="speriosu2013"><!-- close -->Speriosu and Baldridge 2013</span> Speriosu, M. and
                    Baldridge, J. (2013). "Text-driven toponym resolution using
                        indirect supervision." In <cite class="title italic">ACL (1)</cite>, pages
                    1466–1476.</div><div class="bibl"><span class="ref" id="vanhooland2015"><!-- close -->van Hooland et al. 2015</span> [van Hooland et al.,
                    2015] van Hooland, S., De Wilde, M., Verborgh, R., Steiner, T., and Van de
                    Walle, R. (2015). "Exploring Entity Recognition and
                        Disambiguation for Cultural Heritage Collections."
                    <cite class="title italic">Digital Scholarship in the Humanities</cite>,
                    30(2):262–279.</div></div><div class="toolbar"><a href="/dhq/preview/index.html">Preview</a>
             | 
            <a rel="external" href="/dhq/vol/11/4/000328.xml">XML</a>
            | 
            <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div></div><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script><div id="comments"><div id="disqus_thread"/><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    // The following are highly recommended additional parameters. Remove the slashes in front to use.
    var disqus_identifier = '000328';
    var disqus_url = 'http://www.digitalhumanities.org/dhq/vol/11/4/000328/000328.html';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><div id="footer"> 
            URL: http://www.digitalhumanities.org/dhq/vol/11/4/000328/000328.html<br/>Last updated:
            <script type="text/javascript">
                var monthArray = new initArray("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December");
                var lastModifiedDate = new Date(document.lastModified);
                var currentDate = new Date();
                document.write(" ",monthArray[(lastModifiedDate.getMonth()+1)]," ");
                document.write(lastModifiedDate.getDate(),", ",(lastModifiedDate.getFullYear()));
            </script><br/> Comments: <a href="mailto:dhqinfo@digitalhumanities.org" class="footer">dhqinfo@digitalhumanities.org</a><br/> Published by:
            <a href="http://www.digitalhumanities.org" class="footer">The Alliance of Digital Humanities Organizations</a><br/>Affiliated with: <a href="http://llc.oxfordjournals.org/">Literary and Linguistic Computing</a><br/> Copyright 2005 - <script type="text/javascript">
                document.write(currentDate.getFullYear());</script><br/><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png"/></a><br/>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
        </div></div></div></body></html>